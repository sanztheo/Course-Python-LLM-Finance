{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö SOLUTIONS - Calcul Diff√©rentiel\n",
    "\n",
    "**Solutions compl√®tes pour Sections 4 et 5** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Solution 3.6\nprint('\\n\\nSOLUTION 3.6: D√©riv√©es de Fonctions d\\'Activation')\nprint('='*70)\n\n# 1. Sigmoid\nprint('1. SIGMOID: œÉ(x) = 1 / (1 + e^(-x))')\nsigmoid = 1 / (1 + sp.exp(-x))\nsigmoid_prime = sp.diff(sigmoid, x)\nsigmoid_prime_simplified = sp.simplify(sigmoid_prime)\nprint(f'   œÉ\\'(x) = {sigmoid_prime_simplified}')\n\n# V√©rifier: œÉ'(x) = œÉ(x)(1 - œÉ(x))\nsigma = sigmoid\nexpected = sigma * (1 - sigma)\nexpected_simplified = sp.simplify(expected)\nprint(f'   œÉ(x)(1-œÉ(x)) = {expected_simplified}')\nprint(f'   V√©rification: {sp.simplify(sigmoid_prime_simplified - expected_simplified) == 0} ‚úì\\n')\n\n# 2. Tanh\nprint('2. TANH: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))')\ntanh = sp.tanh(x)\ntanh_prime = sp.diff(tanh, x)\ntanh_prime_simplified = sp.simplify(tanh_prime)\nprint(f'   tanh\\'(x) = {tanh_prime_simplified}')\n\n# V√©rifier: tanh'(x) = 1 - tanh¬≤(x)\nexpected_tanh = 1 - tanh**2\nexpected_tanh_simplified = sp.simplify(expected_tanh)\nprint(f'   1 - tanh¬≤(x) = {expected_tanh_simplified}')\nprint(f'   V√©rification: {sp.simplify(tanh_prime_simplified - expected_tanh_simplified) == 0} ‚úì\\n')\n\n# 3. Softplus\nprint('3. SOFTPLUS: f(x) = ln(1 + e^x)')\nsoftplus = sp.ln(1 + sp.exp(x))\nsoftplus_prime = sp.diff(softplus, x)\nsoftplus_prime_simplified = sp.simplify(softplus_prime)\nprint(f'   f\\'(x) = {softplus_prime_simplified}')\nprint(f'   (C\\'est la sigmoid!)')\nprint(f'   V√©rifi√©: {sp.simplify(softplus_prime_simplified - sigmoid) == 0} ‚úì')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Solution 6.1\nprint('\\n\\nSOLUTION 6.1: Chain Rule Simple')\nprint('='*70)\n\nprint('R√®gle de la Cha√Æne: (f ‚àò g)\\'(x) = f\\'(g(x)) ¬∑ g\\'(x)\\n')\n\nchain_functions = [\n    ('h(x) = (x¬≤ + 1)^5', (x**2 + 1)**5, 'u = x¬≤ + 1, f(u) = u^5'),\n    ('h(x) = e^(x¬≤)', sp.exp(x**2), 'u = x¬≤, f(u) = e^u'),\n    ('h(x) = sin(3x)', sp.sin(3*x), 'u = 3x, f(u) = sin(u)'),\n    ('h(x) = ln(x¬≤ + 1)', sp.ln(x**2 + 1), 'u = x¬≤ + 1, f(u) = ln(u)')\n]\n\nfor name, func, composition in chain_functions:\n    deriv = sp.diff(func, x)\n    deriv_simplified = sp.simplify(deriv)\n    print(f'{name}')\n    print(f'  Composition: {composition}')\n    print(f'  ‚Üí h\\'(x) = {deriv_simplified}\\n')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 6.1 : Chain Rule Simple",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 3.6 : D√©riv√©es de Fonctions d'Activation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 3.5\nprint('\\n\\nSOLUTION 3.5: Combiner les R√®gles')\nprint('='*70)\n\nprint('Combinaisons de r√®gles (quotient + produit + puissance):\\n')\n\nfunctions_combined = [\n    ('f(x) = (x¬≤ + 1) / (x - 1)', (x**2 + 1) / (x - 1)),\n    ('g(x) = (x¬≤ + 3x)(2x - 5)', (x**2 + 3*x) * (2*x - 5)),\n    ('h(x) = (x¬∑e^x) / (x¬≤ + 1)', (x * sp.exp(x)) / (x**2 + 1)),\n    ('k(x) = (x + 1)¬≤(x - 1)¬≥', (x + 1)**2 * (x - 1)**3)\n]\n\nfor name, func in functions_combined:\n    deriv = sp.diff(func, x)\n    deriv_simplified = sp.simplify(deriv)\n    print(f'{name}')\n    print(f'  ‚Üí f\\'(x) = {deriv_simplified}\\n')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 3.5 : Combiner les R√®gles",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 3.4\nprint('\\n\\nSOLUTION 3.4: R√®gle de la Puissance')\nprint('='*70)\n\nprint('R√®gle: (x^n)\\' = n¬∑x^(n-1)\\n')\n\npi = sp.pi\n\nfunctions_power = [\n    ('f(x) = x^10', x**10),\n    ('g(x) = x^(-3)', x**(-3)),\n    ('h(x) = ‚àöx = x^(1/2)', x**(sp.Rational(1,2))),\n    ('k(x) = 1/‚àöx = x^(-1/2)', x**(sp.Rational(-1,2))),\n    ('m(x) = x^œÄ', x**pi)\n]\n\nfor name, func in functions_power:\n    deriv = sp.diff(func, x)\n    deriv_simplified = sp.simplify(deriv)\n    print(f'{name}')\n    print(f'  ‚Üí f\\'(x) = {deriv_simplified}\\n')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 3.4 : R√®gle de la Puissance",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 3.3\nprint('\\n\\nSOLUTION 3.3: R√®gle du Quotient')\nprint('='*70)\n\nprint('R√®gle: (f/g)\\' = (f\\'¬∑g - f¬∑g\\') / g¬≤\\n')\n\nfunctions_quot = [\n    ('f(x) = x¬≤ / (x + 1)', x**2 / (x + 1)),\n    ('g(x) = 1 / (x¬≤ + 1)', 1 / (x**2 + 1)),\n    ('h(x) = e^x / x', sp.exp(x) / x),\n    ('k(x) = sin(x) / x', sp.sin(x) / x)\n]\n\nfor name, func in functions_quot:\n    deriv = sp.diff(func, x)\n    deriv_simplified = sp.simplify(deriv)\n    print(f'{name}')\n    print(f'  ‚Üí f\\'(x) = {deriv_simplified}\\n')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 3.3 : R√®gle du Quotient",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 3.2\nprint('\\n\\nSOLUTION 3.2: R√®gle du Produit')\nprint('='*70)\n\nprint('R√®gle: (f¬∑g)\\' = f\\'¬∑g + f¬∑g\\'\\n')\n\nfunctions_prod = [\n    ('f(x) = x¬≤ ¬∑ e^x', x**2 * sp.exp(x)),\n    ('g(x) = (x + 1)(x¬≤ - 3)', (x + 1) * (x**2 - 3)),\n    ('h(x) = x ¬∑ ln(x)', x * sp.ln(x)),\n    ('k(x) = sin(x) ¬∑ cos(x)', sp.sin(x) * sp.cos(x))\n]\n\nfor name, func in functions_prod:\n    deriv = sp.diff(func, x)\n    deriv_simplified = sp.simplify(deriv)\n    print(f'{name}')\n    print(f'  ‚Üí f\\'(x) = {deriv_simplified}\\n')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 3.2 : R√®gle du Produit",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 3.1\nprint('\\n\\nSOLUTION 3.1: R√®gle de la Somme')\nprint('='*70)\n\nprint('R√®gle: (f + g)\\' = f\\' + g\\'\\n')\n\nfunctions_sum = [\n    ('f(x) = x¬≥ + x¬≤ + x + 1', x**3 + x**2 + x + 1),\n    ('g(x) = e^x + ln(x)', sp.exp(x) + sp.ln(x)),\n    ('h(x) = sin(x) + cos(x)', sp.sin(x) + sp.cos(x)),\n    ('k(x) = 5x‚Å¥ - 3x¬≤ + 7', 5*x**4 - 3*x**2 + 7)\n]\n\nfor name, func in functions_sum:\n    deriv = sp.diff(func, x)\n    print(f'{name}')\n    print(f'  ‚Üí f\\'(x) = {deriv}\\n')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 3.1 : R√®gle de la Somme",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ‚ö° Section 3 : R√®gles de D√©rivation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 2.6\nprint('\\n\\nSOLUTION 2.6: Gradient Descent Simple')\nprint('='*70)\n\ndef f_gd(x_val):\n    return (x_val - 5)**2\n\ndef f_prime_gd(x_val):\n    return 2 * (x_val - 5)\n\nx = 0.0\nalpha = 0.1\niterations = 20\ntrajectory = [x]\nlosses = [f_gd(x)]\n\nprint('Minimiser: f(x) = (x - 5)¬≤')\nprint(f'f\\'(x) = 2(x - 5)')\nprint(f'D√©part: x = 0, Œ± = {alpha}, it√©rations = {iterations}\\n')\n\nfor i in range(iterations):\n    gradient = f_prime_gd(x)\n    x = x - alpha * gradient\n    trajectory.append(x)\n    losses.append(f_gd(x))\n    \n    if i % 5 == 0:\n        print(f'It√©ration {i:2d}: x = {x:8.5f}, f(x) = {f_gd(x):10.6f}, gradient = {gradient:8.4f}')\n\nprint(f'\\nR√©sultat final: x = {x:.5f} (vs optimum x=5)')\nprint(f'Loss final: {losses[-1]:.10f}')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Graphique 1: Loss convergence\nax1.plot(losses, 'b-o', linewidth=2, markersize=4)\nax1.set_title('Convergence de Loss')\nax1.set_ylabel('f(x) = (x - 5)¬≤')\nax1.set_xlabel('It√©ration')\nax1.grid(True, alpha=0.3)\n\n# Graphique 2: Trajectory\nx_range = np.linspace(-1, 7, 300)\ny_range = (x_range - 5)**2\nax2.plot(x_range, y_range, 'b-', linewidth=2, label='f(x)')\nax2.plot(trajectory, losses, 'r.-', linewidth=2, markersize=8, label='Trajectoire GD')\nax2.plot(5, 0, 'g*', markersize=20, label='Optimum')\nax2.set_title('Gradient Descent sur f(x) = (x-5)¬≤')\nax2.set_ylabel('f(x)')\nax2.set_xlabel('x')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint('\\n‚úì Graphiques visualis√©s')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 2.6 : Gradient Descent Simple",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 2.5\nprint('\\n\\nSOLUTION 2.5: D√©riv√©e Num√©rique')\nprint('='*70)\n\ndef f(x_val):\n    return np.exp(x_val)\n\ndef derivee_numerique(func, x_val, h=1e-5):\n    return (func(x_val + h) - func(x_val)) / h\n\nx_test = 1.0\ntrue_derivative = np.exp(1)  # Vraie d√©riv√©e de e^x en x=1\n\nprint(f'f(x) = e^x, calculez f\\'(1)')\nprint(f'Vraie d√©riv√©e: e¬π = {true_derivative:.10f}\\n')\n\nprint(f'{\"h\":>12} | {\"f\\'(x) approx\":>15} | {\"Erreur\":>12}')\nprint('-'*45)\n\nh_values = [0.1, 0.01, 0.001, 0.0001, 0.00001]\nfor h_val in h_values:\n    approx = derivee_numerique(f, x_test, h_val)\n    error = abs(approx - true_derivative)\n    print(f'{h_val:12.5f} | {approx:15.10f} | {error:12.2e}')\n\nprint(f'\\nConclusion: Plus h est petit, meilleure est l\\'approximation!')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 2.5 : D√©riv√©e Num√©rique",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 2.4\nprint('\\n\\nSOLUTION 2.4: Visualisation de D√©riv√©e')\nprint('='*70)\n\nx_vals = np.linspace(-2*np.pi, 2*np.pi, 500)\ny_vals = np.sin(x_vals)\ny_prime_vals = np.cos(x_vals)\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x) = sin(x)')\nax.plot(x_vals, y_prime_vals, 'r-', linewidth=2, label=\"f'(x) = cos(x)\")\nax.axhline(0, color='k', linestyle='-', linewidth=0.5)\nax.grid(True, alpha=0.3)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_title('Fonction sin(x) et sa D√©riv√©e cos(x)')\nax.legend(fontsize=12)\n\n# Annotations\nprint(\"\\nObservations:\")\nprint(\"1. O√π f'(x) = 0 (cos=0): f(x) atteint max/min (sin=¬±1)\")\nprint(\"2. O√π f'(x) > 0 (cos>0): f(x) cro√Æt\")\nprint(\"3. O√π f'(x) < 0 (cos<0): f(x) d√©cro√Æt\")\n\nplt.tight_layout()\nplt.show()\n\nprint('‚úì Graphique visualis√©')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 2.4 : Visualisation de D√©riv√©e",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 2.3\nprint('\\n\\nSOLUTION 2.3: Pente de la Tangente')\nprint('='*70)\n\nf = x**3 - 3*x\nf_prime = sp.diff(f, x)\n\nprint(f'f(x) = x¬≥ - 3x')\nprint(f'f\\'(x) = {f_prime}\\n')\n\n# Pente en x = 1\npente_1 = f_prime.subs(x, 1)\nf_1 = f.subs(x, 1)\nprint(f'En x = 1:')\nprint(f'  Pente f\\'(1) = {pente_1}')\nprint(f'  f(1) = {f_1}')\n\n# √âquation tangente en x = 1\n# y - f(a) = f'(a)(x - a)  ‚Üí  y = f'(a)(x - a) + f(a)\nprint(f'  √âquation tangente: y = {pente_1}(x - 1) + {f_1}')\nprint(f'                     y = {pente_1}x + {f_1 - pente_1}\\n')\n\n# Pente en x = -2\npente_2 = f_prime.subs(x, -2)\nf_2 = f.subs(x, -2)\nprint(f'En x = -2:')\nprint(f'  Pente f\\'(-2) = {pente_2}')\nprint(f'  f(-2) = {f_2}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 2.3 : Pente de la Tangente",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 2.2\nprint('\\n\\nSOLUTION 2.2: D√©riv√©es de Polyn√¥mes')\nprint('='*70)\n\nfunctions = [\n    ('f(x) = 3x‚Å¥ - 2x¬≥ + 5x - 7', 3*x**4 - 2*x**3 + 5*x - 7),\n    ('g(x) = x‚Åµ + 6x¬≤ - 9', x**5 + 6*x**2 - 9),\n    ('h(x) = (2x + 1)¬≥', (2*x + 1)**3),\n    ('k(x) = x¬≥/3 - x¬≤/2 + x', x**3/3 - x**2/2 + x)\n]\n\nfor name, func in functions:\n    deriv = sp.diff(func, x)\n    print(f'{name}')\n    print(f'  ‚Üí f\\'(x) = {deriv}')\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 2.2 : D√©riv√©es de Polyn√¥mes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 2.1\nprint('\\n\\nSOLUTION 2.1: D√©riv√©e avec la D√©finition')\nprint('='*70)\n\nh = sp.Symbol('h')\nf = x**2\n\nprint(\"f(x) = x¬≤\")\nprint(\"f'(x) = lim_(h‚Üí0) [f(x+h) - f(x)] / h\\n\")\n\n# √âtape 1: D√©velopper (x+h)¬≤\nf_xh = (x + h)**2\nprint(f'1. f(x+h) = (x+h)¬≤ = {sp.expand(f_xh)}')\n\n# √âtape 2: f(x+h) - f(x)\ndiff = f_xh - f\nprint(f'2. f(x+h) - f(x) = {sp.expand(diff)}')\n\n# √âtape 3: Diviser par h\ndiff_quotient = diff / h\nprint(f'3. [f(x+h) - f(x)] / h = {sp.simplify(diff_quotient)}')\n\n# √âtape 4: Limite\nderivee = sp.limit(diff_quotient, h, 0)\nprint(f'4. lim_(h‚Üí0) ... = {derivee}')\n\n# V√©rification avec SymPy\nderivee_sympy = sp.diff(x**2, x)\nprint(f'\\nV√©rification SymPy: d/dx[x¬≤] = {derivee_sympy}')\nprint(f'‚úì R√©sultats identiques: {derivee == derivee_sympy}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 2.1 : D√©riv√©e avec la D√©finition",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## üìê Section 2 : D√©riv√©es Basiques",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 1.5\nprint('\\n\\nSOLUTION 1.5: Limite et Forme Ind√©termin√©e')\nprint('='*70)\n\nprint('Calculez: lim_(x‚Üí0) (x¬≤ - 4x) / x\\n')\n\n# M√©thode 1: Direct\nprint('M√âTHODE 1 - Directement:')\nexpr = (x**2 - 4*x) / x\nprint(f'(x¬≤ - 4x) / x en x=0: 0/0 (ind√©termin√©)')\n\n# M√©thode 2: Alg√©brique\nprint('\\nM√âTHODE 2 - Simplification:')\nnumerator = x**2 - 4*x\nsimplified = sp.simplify(numerator / x)\nprint(f'(x¬≤ - 4x) / x = x(x - 4) / x = x - 4')\nlim_algebraic = -4\nprint(f'lim_(x‚Üí0) (x - 4) = {lim_algebraic}')\n\n# M√©thode 3: SymPy\nprint('\\nM√âTHODE 3 - SymPy (automatique):')\nlim_sympy = sp.limit(expr, x, 0)\nprint(f'sp.limit((x¬≤ - 4x)/x, x, 0) = {lim_sympy}')\n\nprint(f'\\n‚úì Toutes les m√©thodes donnent: {lim_sympy}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 1.5 : Limite et Forme Ind√©termin√©e",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 1.4\nprint('\\n\\nSOLUTION 1.4: Approximation Num√©rique de Limite')\nprint('='*70)\n\ndef f_approx(x_val):\n    return (np.exp(x_val) - 1) / x_val\n\nx_vals = [0.1, 0.01, 0.001, 0.0001]\nprint(f'Pour f(x) = (e^x - 1) / x, lim_(x‚Üí0) f(x) = ?')\nprint('\\nApproximation num√©rique:')\nprint(f'{\"x\":>10} | {\"f(x)\":>15} | {\"Erreur vs lim\":>15}')\nprint('-'*45)\n\n# Limite symbolique\nlim_limit = sp.limit((sp.exp(x) - 1) / x, x, 0)\nlim_val = float(lim_limit)\n\nfor x_val in x_vals:\n    f_val = f_approx(x_val)\n    error = abs(f_val - lim_val)\n    print(f'{x_val:10.4f} | {f_val:15.10f} | {error:15.2e}')\n\nprint(f'\\nLimite symbolique SymPy: {lim_limit} ‚âà {lim_val:.10f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 1.4 : Approximation Num√©rique de Limite",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 1.3\nprint('\\n\\nSOLUTION 1.3: Continuit√©')\nprint('='*70)\n\n# 1. f(x) = x¬≤\nf1 = x**2\nlim_f1 = sp.limit(f1, x, 0)\nf1_val = f1.subs(x, 0)\ncont1 = lim_f1 == f1_val\nprint(f'1. f(x) = x¬≤')\nprint(f'   lim_(x‚Üí0) f(x) = {lim_f1}, f(0) = {f1_val}')\nprint(f'   Continue en x=0? {cont1} ‚úì\\n')\n\n# 2. g(x) = 1/x\ng = 1/x\nlim_g = sp.limit(g, x, 0)\nprint(f'2. g(x) = 1/x')\nprint(f'   lim_(x‚Üí0) g(x) = {lim_g} (asymptote verticale)')\nprint(f'   Continue en x=0? False (non d√©finie) ‚úó\\n')\n\n# 3. h(x) = piecewise\nprint(f'3. h(x) = x si x‚â†0, 1 si x=0')\nlim_h = sp.limit(x, x, 0)\nh_val = 1\nprint(f'   lim_(x‚Üí0) h(x) = {lim_h}, h(0) = {h_val}')\nprint(f'   Continue en x=0? {lim_h == h_val} (saut de 1-0=1) ‚úó')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 1.3 : Continuit√©",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 1.2\nprint('\\n\\nSOLUTION 1.2: Limites Lat√©rales')\nprint('='*70)\n\n# f(x) = |x|/x\nf = sp.Abs(x) / x\n\n# Limite √† droite (x ‚Üí 0+)\nlim_droite = sp.limit(f, x, 0, '+')\nprint(f'f(x) = |x|/x')\nprint(f'lim_(x‚Üí0+) f(x) = {lim_droite}')\n\n# Limite √† gauche (x ‚Üí 0-)\nlim_gauche = sp.limit(f, x, 0, '-')\nprint(f'lim_(x‚Üí0-) f(x) = {lim_gauche}')\n\n# Continuit√©\nis_continuous = lim_droite == lim_gauche\nprint(f'\\nLes limites lat√©rales sont-elles √©gales ? {is_continuous}')\nprint(f'La fonction est-elle continue en x=0 ? {is_continuous}')\nprint(f'‚Üí NON CONTINUE (discontinuit√© de saut)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 1.2 : Limites Lat√©rales",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 1.1\nimport sympy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = sp.Symbol('x')\nprint('SOLUTION 1.1: Limites Simples')\nprint('='*70)\n\n# 1. lim(2x + 5) quand x ‚Üí 3\nlim1 = sp.limit(2*x + 5, x, 3)\nprint(f'1. lim_(x‚Üí3) (2x + 5) = {lim1}')\n\n# 2. lim(x¬≤ - 4) quand x ‚Üí 2\nlim2 = sp.limit(x**2 - 4, x, 2)\nprint(f'2. lim_(x‚Üí2) (x¬≤ - 4) = {lim2}')\n\n# 3. lim(sin(x)/x) quand x ‚Üí 0\nlim3 = sp.limit(sp.sin(x)/x, x, 0)\nprint(f'3. lim_(x‚Üí0) sin(x)/x = {lim3}')\n\n# 4. lim(1/x) quand x ‚Üí ‚àû\nlim4 = sp.limit(1/x, x, sp.oo)\nprint(f'4. lim_(x‚Üí‚àû) 1/x = {lim4}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 1.1 : Limites Simples",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : D√©riv√©es Partielles - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.1 : D√©riv√©es Partielles Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x, y, z = sp.symbols('x y z')\n",
    "print('SOLUTION 4.1: D√©riv√©es Partielles Simples')\n",
    "print('='*70)\n",
    "\n",
    "f = x**2 * y + 3*x*y**2\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "\n",
    "print(f'f(x,y) = {f}')\n",
    "print(f'‚àÇf/‚àÇx = {df_dx}')\n",
    "print(f'‚àÇf/‚àÇy = {df_dy}')\n",
    "\n",
    "val_x = df_dx.subs([(x, 2), (y, 3)])\n",
    "val_y = df_dy.subs([(x, 2), (y, 3)])\n",
    "print(f'\\n‚àÇf/‚àÇx|_(2,3) = {val_x}')\n",
    "print(f'‚àÇf/‚àÇy|_(2,3) = {val_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.2 : Fonction √† Trois Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.2: Fonction √† Trois Variables')\n",
    "print('='*70)\n",
    "\n",
    "f = x**2 + y**2 + z**2 + x*y + y*z\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "df_dz = sp.diff(f, z)\n",
    "\n",
    "print(f'f(x,y,z) = {f}')\n",
    "print(f'‚àÇf/‚àÇx = {df_dx}')\n",
    "print(f'‚àÇf/‚àÇy = {df_dy}')\n",
    "print(f'‚àÇf/‚àÇz = {df_dz}')\n",
    "print(f'\\n‚àáf(1,1,1) = [{df_dx.subs({x:1, y:1, z:1})}, {df_dy.subs({x:1, y:1, z:1})}, {df_dz.subs({x:1, y:1, z:1})}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.3 : D√©riv√©es Partielles d'Ordre 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.3: D√©riv√©es Partielles d\\'Ordre 2')\n",
    "print('='*70)\n",
    "\n",
    "f = x**3 * y**2\n",
    "f_xx = sp.diff(f, x, 2)\n",
    "f_yy = sp.diff(f, y, 2)\n",
    "f_xy = sp.diff(f, x, y)\n",
    "f_yx = sp.diff(f, y, x)\n",
    "\n",
    "print(f'f(x,y) = {f}')\n",
    "print(f'‚àÇ¬≤f/‚àÇx¬≤ = {f_xx}')\n",
    "print(f'‚àÇ¬≤f/‚àÇy¬≤ = {f_yy}')\n",
    "print(f'‚àÇ¬≤f/‚àÇx‚àÇy = {f_xy}')\n",
    "print(f'‚àÇ¬≤f/‚àÇy‚àÇx = {f_yx}')\n",
    "print(f'Th√©or√®me Schwarz: {sp.simplify(f_xy - f_yx) == 0}')  # True si √©gales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.4 : Visualisation de Surface 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.4: Visualisation de Surface 3D')\n",
    "print('='*70)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "\n",
    "x_vals = np.linspace(-3, 3, 100)\n",
    "y_vals = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "Z = X**2 - Y**2\n",
    "\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Surface: f(x,y) = x¬≤ - y¬≤')\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "for px, py in [(1, 1), (-1, 1), (1, -1)]:\n",
    "    ax2.quiver(px, py, 2*px, -2*py, scale=15, color='red')\n",
    "ax2.set_title('Contours + Gradients')\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.text(0.5, 0.5, 'En (1,1):\\n‚àÇf/‚àÇx = 2\\n‚àÇf/‚àÇy = -2', ha='center', fontsize=12)\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Surface visualis√©e ‚úì')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.5 : Application ML - Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.5: Application ML - Loss Function')\n",
    "print('='*70)\n",
    "\n",
    "w, b = sp.symbols('w b')\n",
    "x_data, y_data = 2, 5\n",
    "\n",
    "L = (y_data - (w*x_data + b))**2\n",
    "dL_dw = sp.diff(L, w)\n",
    "dL_db = sp.diff(L, b)\n",
    "\n",
    "print(f'Donn√©es: x={x_data}, y={y_data}')\n",
    "print(f'L = {sp.expand(L)}')\n",
    "print(f'‚àÇL/‚àÇw = {dL_dw}')\n",
    "print(f'‚àÇL/‚àÇb = {dL_db}')\n",
    "\n",
    "grad_w = dL_dw.subs([(w, 1), (b, 0)])\n",
    "grad_b = dL_db.subs([(w, 1), (b, 0)])\n",
    "print(f'\\nEn (w=1, b=0):')\n",
    "print(f'‚àÇL/‚àÇw = {grad_w}')\n",
    "print(f'‚àÇL/‚àÇb = {grad_b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : Gradient - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.1 : Calculer le Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.1: Calculer le Gradient')\n",
    "print('='*70)\n",
    "\n",
    "f1 = x**2 + y**2\n",
    "grad_f1 = [sp.diff(f1, x), sp.diff(f1, y)]\n",
    "print(f'1. f(x,y) = {f1} ‚Üí ‚àáf = {grad_f1}')\n",
    "\n",
    "f2 = x*y\n",
    "grad_f2 = [sp.diff(f2, x), sp.diff(f2, y)]\n",
    "print(f'2. g(x,y) = {f2} ‚Üí ‚àág = {grad_f2}')\n",
    "\n",
    "f3 = x**2 + 2*y**2 + 3*z**2\n",
    "grad_f3 = [sp.diff(f3, x), sp.diff(f3, y), sp.diff(f3, z)]\n",
    "print(f'3. h(x,y,z) = {f3} ‚Üí ‚àáh = {grad_f3}')\n",
    "\n",
    "f4 = sp.exp(x + y)\n",
    "grad_f4 = [sp.diff(f4, x), sp.diff(f4, y)]\n",
    "print(f'4. k(x,y) = {f4} ‚Üí ‚àák = {grad_f4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.2 : Norme et Direction du Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.2: Norme et Direction du Gradient')\n",
    "print('='*70)\n",
    "\n",
    "f = x**2 + 2*y**2\n",
    "grad_x = sp.diff(f, x)\n",
    "grad_y = sp.diff(f, y)\n",
    "\n",
    "grad_val = np.array([2.0, 4.0])  # En (1,1)\n",
    "norm = np.linalg.norm(grad_val)\n",
    "\n",
    "print(f'f(x,y) = {f}')\n",
    "print(f'En (1,1): ‚àáf = {grad_val}')\n",
    "print(f'||‚àáf|| = {norm:.4f}')\n",
    "print(f'Direction normalis√©e: {grad_val/norm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.3 : Gradient Descent 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.3: Gradient Descent 2D')\n",
    "print('='*70)\n",
    "\n",
    "def f(x_val, y_val):\n",
    "    return (x_val - 3)**2 + (y_val + 2)**2\n",
    "\n",
    "def grad(x_val, y_val):\n",
    "    return np.array([2*(x_val - 3), 2*(y_val + 2)])\n",
    "\n",
    "x_pos, y_pos = 0.0, 0.0\n",
    "alpha, iterations = 0.1, 30\n",
    "losses = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    g = grad(x_pos, y_pos)\n",
    "    x_pos -= alpha * g[0]\n",
    "    y_pos -= alpha * g[1]\n",
    "    losses.append(f(x_pos, y_pos))\n",
    "\n",
    "print(f'Position initiale: (0, 0)')\n",
    "print(f'Position finale: ({x_pos:.4f}, {y_pos:.4f})')\n",
    "print(f'Minimum attendu: (3, -2)')\n",
    "print(f'Loss finale: {losses[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.4 : Champ de Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.4: Champ de Gradients')\n",
    "print('='*70)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "x_range = np.linspace(-3, 3, 20)\n",
    "y_range = np.linspace(-3, 3, 20)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = 0.5 * (X**2 + Y**2)\n",
    "\n",
    "contour = ax.contour(X, Y, Z, levels=15, alpha=0.6)\n",
    "quiver = ax.quiver(X, Y, X, Y, np.sqrt(X**2 + Y**2), cmap='hot', scale=30)\n",
    "ax.set_title('Champ de gradients: f(x,y) = 0.5(x¬≤ + y¬≤)')\n",
    "ax.axis('equal')\n",
    "plt.show()\n",
    "print('Champ de gradients visualis√© ‚úì')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.5 : D√©riv√©es Directionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.5: D√©riv√©es Directionnelles')\n",
    "print('='*70)\n",
    "\n",
    "f = x**2 + x*y + y**2\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "\n",
    "grad = np.array([4.0, 5.0])  # En (1,2)\n",
    "\n",
    "print(f'f(x,y) = {f}')\n",
    "print(f'En (1,2): ‚àáf = {grad}')\n",
    "\n",
    "directions = {\n",
    "    'axe x': np.array([1, 0]),\n",
    "    'axe y': np.array([0, 1]),\n",
    "    'diagonale': np.array([1, 1])/np.sqrt(2)\n",
    "}\n",
    "\n",
    "for name, u in directions.items():\n",
    "    d_u = np.dot(grad, u/np.linalg.norm(u))\n",
    "    print(f'{name}: D_u f = {d_u:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.6 : R√©gression Lin√©aire par Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.6: R√©gression Lin√©aire par Gradient Descent')\n",
    "print('='*70)\n",
    "\n",
    "x_data = np.array([1, 2, 3])\n",
    "y_data = np.array([2, 4, 5])\n",
    "n = len(x_data)\n",
    "\n",
    "def loss(w, b):\n",
    "    return np.mean((y_data - (w*x_data + b))**2)\n",
    "\n",
    "def grad(w, b):\n",
    "    y_pred = w*x_data + b\n",
    "    residuals = y_data - y_pred\n",
    "    return -2*np.mean(residuals*x_data), -2*np.mean(residuals)\n",
    "\n",
    "w, b = 0.0, 0.0\n",
    "alpha = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    dw, db = grad(w, b)\n",
    "    w -= alpha * dw\n",
    "    b -= alpha * db\n",
    "\n",
    "print(f'Gradient Descent: w={w:.4f}, b={b:.4f}')\n",
    "\n",
    "# Solution analytique\n",
    "x_mean = np.mean(x_data)\n",
    "y_mean = np.mean(y_data)\n",
    "w_a = np.sum((x_data - x_mean)*(y_data - y_mean)) / np.sum((x_data - x_mean)**2)\n",
    "b_a = y_mean - w_a * x_mean\n",
    "\n",
    "print(f'Analytique: w={w_a:.4f}, b={b_a:.4f}')\n",
    "print(f'\\n‚úì Les deux convergen vers la m√™me solution!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ F√©licitations!\n",
    "\n",
    "**Solutions Sections 4-5 compl√®tes!**\n",
    "\n",
    "### Contenu\n",
    "\n",
    "- ‚úì Ex 4.1: D√©riv√©es partielles simples\n",
    "- ‚úì Ex 4.2: Trois variables + gradient\n",
    "- ‚úì Ex 4.3: D√©riv√©es d'ordre 2\n",
    "- ‚úì Ex 4.4: Visualisation 3D\n",
    "- ‚úì Ex 4.5: Loss function ML\n",
    "- ‚úì Ex 5.1: Calculer gradients\n",
    "- ‚úì Ex 5.2: Norme et direction\n",
    "- ‚úì Ex 5.3: Gradient Descent 2D\n",
    "- ‚úì Ex 5.4: Champ de vecteurs\n",
    "- ‚úì Ex 5.5: D√©riv√©es directionnelles\n",
    "- ‚úì Ex 5.6: R√©gression lin√©aire\n",
    "\n",
    "**Excellent travail! üí™**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "print('### Exercice 6.2: Chain Rule Multivari√©e ###\\n')\n\nt = sp.Symbol('t')\ny = sp.Symbol('y')\nx = sp.Symbol('x')\n\nz = x**2 + y**2\nx_t = t**2\ny_t = t**3\n\ndz_dx = sp.diff(z, x)\ndz_dy = sp.diff(z, y)\ndx_dt = sp.diff(x_t, t)\ndy_dt = sp.diff(y_t, t)\n\ndz_dt = dz_dx * dx_dt + dz_dy * dy_dt\n\nprint(f'z = x¬≤ + y¬≤ o√π x = t¬≤, y = t¬≥')\nprint(f'‚àÇz/‚àÇx = {dz_dx}, ‚àÇz/‚àÇy = {dz_dy}')\nprint(f'dx/dt = {dx_dt}, dy/dt = {dy_dt}')\nprint(f'dz/dt = {sp.expand(dz_dt)}')\n\nz_direct = t**4 + t**6\ndz_dt_direct = sp.diff(z_direct, t)\nprint(f'V√©rification directe: {dz_dt_direct}')\nprint(f'R√©sultats identiques: {sp.simplify(dz_dt - dz_dt_direct) == 0}‚úì\\n')"
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 6.2 : Chain Rule Multivari√©e",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "print('SOLUTION 6.3: Backpropagation - 1 Couche')\nprint('='*70)\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n\ndef sigmoid_derivative(a):\n    return a * (1 - a)\n\nx_val, y_true, w, alpha = 3.0, 0.8, 0.5, 0.5\nlosses, ws = [], [w]\n\nprint('R√©seau: x --[w]--> z = w√óx --[œÉ]--> a = œÉ(z) --> L = (a-y)¬≤')\nprint(f'Donn√©es: x={x_val}, y_true={y_true}, w={w}, Œ±={alpha}\\n')\n\nfor i in range(10):\n    z = w * x_val\n    a = sigmoid(z)\n    L = (a - y_true)**2\n    \n    dL_da = 2 * (a - y_true)\n    da_dz = sigmoid_derivative(a)\n    dz_dw = x_val\n    dL_dw = dL_da * da_dz * dz_dw\n    \n    w = w - alpha * dL_dw\n    \n    if i in [0, 4, 9]:\n        print(f'It√©ration {i+1}: z={z:.4f}, a={a:.4f}, L={L:.6f}, w: {ws[-1]:.4f} ‚Üí {w:.4f}')\n    \n    losses.append(L)\n    ws.append(w)\n\nprint(f'\\nR√©sultat: w={w:.4f}, Loss final={losses[-1]:.6f}')\nprint(f'R√©duction: {(losses[0]-losses[-1])/losses[0]*100:.1f}%')\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nax1.plot(losses, 'b-o', linewidth=2, markersize=5)\nax1.set_title('Convergence Loss - Backprop 1 Couche')\nax1.set_ylabel('Loss L')\nax1.grid(True, alpha=0.3)\nax2.plot(ws, 'r-o', linewidth=2, markersize=5)\nax2.axhline(y=y_true, color='g', linestyle='--', label='Optimal')\nax2.set_title('√âvolution Poids w')\nax2.set_ylabel('Poids w')\nax2.legend()\nax2.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint('‚úì Graphique sauvegard√©')",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 6.2: Chain Rule Multivari√©e ###\\n')\n",
    "\n",
    "t = sp.Symbol('t')\n",
    "y = sp.Symbol('y')\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "z = x**2 + y**2\n",
    "x_t = t**2\n",
    "y_t = t**3\n",
    "\n",
    "dz_dx = sp.diff(z, x)\n",
    "dz_dy = sp.diff(z, y)\n",
    "dx_dt = sp.diff(x_t, t)\n",
    "dy_dt = sp.diff(y_t, t)\n",
    "\n",
    "dz_dt = dz_dx * dx_dt + dz_dy * dy_dt\n",
    "\n",
    "print(f'z = x¬≤ + y¬≤ o√π x = t¬≤, y = t¬≥')\n",
    "print(f'‚àÇz/‚àÇx = {dz_dx}, ‚àÇz/‚àÇy = {dz_dy}')\n",
    "print(f'dx/dt = {dx_dt}, dy/dt = {dy_dt}')\n",
    "print(f'dz/dt = {sp.expand(dz_dt)}')\n",
    "\n",
    "z_direct = t**4 + t**6\n",
    "dz_dt_direct = sp.diff(z_direct, t)\n",
    "print(f'V√©rification directe: {dz_dt_direct}')\n",
    "print(f'R√©sultats identiques: {sp.simplify(dz_dt - dz_dt_direct) == 0}‚úì\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "print('SOLUTION 6.4: Backpropagation - 2 Couches')\nprint('='*70)\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n\ndef sigmoid_derivative(a):\n    return a * (1 - a)\n\nx_val, y_true, w1, w2, alpha = 2.0, 0.9, 0.4, 0.6, 0.5\nlosses, w1_hist, w2_hist = [], [w1], [w2]\n\nprint('R√©seau: x --[w‚ÇÅ]--> z‚ÇÅ --[œÉ]--> a‚ÇÅ --[w‚ÇÇ]--> z‚ÇÇ --[œÉ]--> a‚ÇÇ --> L=(a‚ÇÇ-y)¬≤')\nprint(f'Donn√©es: x={x_val}, y_true={y_true}, w‚ÇÅ={w1}, w‚ÇÇ={w2}, Œ±={alpha}\\n')\n\nfor i in range(15):\n    z1 = w1 * x_val\n    a1 = sigmoid(z1)\n    z2 = w2 * a1\n    a2 = sigmoid(z2)\n    L = (a2 - y_true)**2\n    \n    dL_da2 = 2 * (a2 - y_true)\n    da2_dz2 = sigmoid_derivative(a2)\n    dz2_dw2 = a1\n    dz2_da1 = w2\n    dL_dw2 = dL_da2 * da2_dz2 * dz2_dw2\n    \n    dL_da1 = dL_da2 * da2_dz2 * dz2_da1\n    da1_dz1 = sigmoid_derivative(a1)\n    dz1_dw1 = x_val\n    dL_dw1 = dL_da1 * da1_dz1 * dz1_dw1\n    \n    w1 = w1 - alpha * dL_dw1\n    w2 = w2 - alpha * dL_dw2\n    \n    if i in [0, 7, 14]:\n        print(f'It√©ration {i+1}: a‚ÇÇ={a2:.4f}, L={L:.6f}, w‚ÇÅ: {w1_hist[-1]:.4f}‚Üí{w1:.4f}, w‚ÇÇ: {w2_hist[-1]:.4f}‚Üí{w2:.4f}')\n    \n    losses.append(L)\n    w1_hist.append(w1)\n    w2_hist.append(w2)\n\nprint(f'\\nR√©sultat final: w‚ÇÅ={w1:.4f}, w‚ÇÇ={w2:.4f}, Loss={losses[-1]:.6f}')\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\naxes[0].plot(losses, 'b-o', linewidth=2, markersize=5)\naxes[0].set_title('Convergence Loss (2 couches)')\naxes[0].set_ylabel('Loss L')\naxes[0].grid(True, alpha=0.3)\naxes[1].plot(w1_hist, 'r-o', linewidth=2, markersize=5)\naxes[1].set_title('√âvolution w‚ÇÅ')\naxes[1].set_ylabel('Poids w‚ÇÅ')\naxes[1].grid(True, alpha=0.3)\naxes[2].plot(w2_hist, 'g-o', linewidth=2, markersize=5)\naxes[2].set_title('√âvolution w‚ÇÇ')\naxes[2].set_ylabel('Poids w‚ÇÇ')\naxes[2].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint('‚úì Graphique sauvegard√©')",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.3: Backpropagation 1 Couche"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "print('SOLUTION 6.5: Chain Rule - ReLU')\nprint('='*70)\n\ndef relu(z):\n    return max(0, z)\n\ndef relu_derivative(z):\n    return 1.0 if z > 0 else 0.0\n\nx_val, w = -2.0, 0.5\nz = w * x_val\na = relu(z)\nL = a**2\n\nprint('R√©seau: x --[w]--> z = w√óx --[ReLU]--> a = max(0,z) --> L = a¬≤')\nprint(f'Donn√©es: x={x_val}, w={w}\\n')\nprint('FORWARD PASS:')\nprint(f'  z = {w} √ó {x_val} = {z}')\nprint(f'  a = ReLU({z}) = {a}')\nprint(f'  L = {a}¬≤ = {L}\\n')\n\ndL_da = 2 * a\nda_dz = relu_derivative(z)\ndz_dw = x_val\ndL_dw = dL_da * da_dz * dz_dw\n\nprint('BACKWARD PASS:')\nprint(f'  dL/da = 2a = {dL_da}')\nprint(f'  da/dz = ReLU\\'({z}) = {da_dz}  ‚Üê Gradient dispara√Æt! (z < 0)')\nprint(f'  dL/dw = {dL_da} √ó {da_dz} √ó {dz_dw} = {dL_dw}')\nprint(f'\\n‚ö†Ô∏è Probl√®me: Dying ReLU! Les neurones n√©gatifs ne s\\'adaptent pas.\\n')\n\nprint('Avec x = 2.0 (entr√©e positive):  ')\nz_pos = 0.5 * 2.0\na_pos = relu(z_pos)\ndL_dw_pos = 2 * a_pos * relu_derivative(z_pos) * 2.0\nprint(f'  dL/dw = {dL_dw_pos}  ‚Üê Gradient actif!')",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 6.3: Backpropagation - 1 Couche ###\\n')\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "x_val, y_true, w, alpha = 3.0, 0.8, 0.5, 0.5\n",
    "losses, ws = [], [w]\n",
    "\n",
    "print(f'R√©seau: x --[w]--> z = w√óx --[œÉ]--> a = œÉ(z) ---> L = (a-y)¬≤')\n",
    "print(f'Donn√©es: x={x_val}, y_true={y_true}, w={w}, Œ±={alpha}\\n')\n",
    "\n",
    "for i in range(10):\n",
    "    z = w * x_val\n",
    "    a = sigmoid(z)\n",
    "    L = (a - y_true)**2\n",
    "    \n",
    "    dL_da = 2 * (a - y_true)\n",
    "    da_dz = sigmoid_derivative(a)\n",
    "    dz_dw = x_val\n",
    "    dL_dw = dL_da * da_dz * dz_dw\n",
    "    \n",
    "    w = w - alpha * dL_dw\n",
    "    \n",
    "    if i in [0, 4, 9]:\n",
    "        print(f'It√©ration {i+1}: z={z:.4f}, a={a:.4f}, L={L:.6f}, w: {ws[-1]:.4f} ‚Üí {w:.4f}')\n",
    "    \n",
    "    losses.append(L)\n",
    "    ws.append(w)\n",
    "\n",
    "print(f'\\nR√©sultat: w={w:.4f}, Loss final={losses[-1]:.6f}')\n",
    "print(f'R√©duction: {(losses[0]-losses[-1])/losses[0]*100:.1f}%')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(losses, 'b-o', linewidth=2, markersize=5)\n",
    "ax1.set_title('Convergence Loss - Backprop 1 Couche')\n",
    "ax1.set_ylabel('Loss L')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax2.plot(ws, 'r-o', linewidth=2, markersize=5)\n",
    "ax2.axhline(y=y_true, color='g', linestyle='--', label='Optimal')\n",
    "ax2.set_title('√âvolution Poids w')\n",
    "ax2.set_ylabel('Poids w')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "print('SOLUTION 6.6: Computational Graph')\nprint('='*70)\n\na = sp.Symbol('a')\nb = sp.Symbol('b')\nx_sym = sp.Symbol('x')\n\na_def = x_sym**2\nb_def = 2*x_sym\nL_expr = (a_def + b_def)**2\n\ndL_dab = 2 * (a_def + b_def)\nda_dx = 2 * x_sym\ndb_dx = 2\ndL_dx_chain = dL_dab * da_dx + dL_dab * db_dx\ndL_dx_chain_simplified = sp.expand(dL_dx_chain)\n\nL_direct = (x_sym**2 + 2*x_sym)**2\ndL_dx_direct = sp.expand(sp.diff(L_direct, x_sym))\n\nprint('Pour L = (a + b)¬≤ o√π a = x¬≤ et b = 2x:')\nprint('\\nM√âTHODE 1 - Chain Rule:')\nprint(f'  dL/dx = 2(x¬≤ + 2x) √ó (2x + 2) = {dL_dx_chain_simplified}')\nprint('\\nM√âTHODE 2 - D√©rivation Directe:')\nprint(f'  L = (x¬≤ + 2x)¬≤, dL/dx = {dL_dx_direct}')\nprint(f'\\nV√©rification: {sp.simplify(dL_dx_chain_simplified - dL_dx_direct) == 0}‚úì')\n\nx_val = 3\ndL_dx_val = dL_dx_direct.subs(x_sym, x_val)\nprint(f'\\n√âvaluation en x=3:')\nprint(f'  a = 9, b = 6, L = (9+6)¬≤ = 225')\nprint(f'  dL/dx = {dL_dx_val}')",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.4: Backpropagation 2 Couches"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "print('SOLUTION 7.1: Points Critiques 1D')\nprint('='*70)\n\nx = sp.Symbol('x')\nf = x**3 - 6*x**2 + 9*x + 1\nf_prime = sp.diff(f, x)\nf_double_prime = sp.diff(f_prime, x)\n\nprint('f(x) = x¬≥ - 6x¬≤ + 9x + 1')\nprint(f'f\\'(x) = {f_prime}')\nprint(f'f\\'\\'(x) = {f_double_prime}\\n')\n\ncritical_points = sp.solve(f_prime, x)\nprint(f'Points critiques: {critical_points}\\n')\n\nfor xc in critical_points:\n    f_val = f.subs(x, xc)\n    f_pp_val = f_double_prime.subs(x, xc)\n    nature = 'Minimum' if f_pp_val > 0 else 'Maximum' if f_pp_val < 0 else 'Inflexion'\n    print(f'x = {xc}: f({xc}) = {f_val}, f\\'\\'({xc}) = {f_pp_val} ‚Üí {nature}\\n')\n\nx_vals = np.linspace(-1, 6, 200)\ny_vals = x_vals**3 - 6*x_vals**2 + 9*x_vals + 1\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x)')\nfor xc in critical_points:\n    f_val = float(f.subs(x, xc))\n    color = 'go' if f_double_prime.subs(x, xc) > 0 else 'ro'\n    ax.plot(float(xc), f_val, color, markersize=10)\nax.grid(True, alpha=0.3)\nax.set_title('f(x) = x¬≥ - 6x¬≤ + 9x + 1 - Points Critiques')\nax.set_ylabel('f(x)')\nax.legend()\nplt.tight_layout()\nplt.show()\nprint('‚úì Graphique sauvegard√©')",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Exercice 6.4: Backpropagation - 2 Couches ###\n",
      "\n",
      "R√©seau: x --[w‚ÇÅ]--> z‚ÇÅ --[œÉ]--> a‚ÇÅ --[w‚ÇÇ]--> z‚ÇÇ --[œÉ]--> a‚ÇÇ --> L=(a‚ÇÇ-y)¬≤\n",
      "Donn√©es: x=2.0, y_true=0.9, w‚ÇÅ=0.4, w‚ÇÇ=0.6, Œ±=0.5\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m15\u001b[39m):\n\u001b[32m     10\u001b[39m     z1 = w1 * x_val\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     a1 = \u001b[43msigmoid\u001b[49m(z1)\n\u001b[32m     12\u001b[39m     z2 = w2 * a1\n\u001b[32m     13\u001b[39m     a2 = sigmoid(z2)\n",
      "\u001b[31mNameError\u001b[39m: name 'sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "print('### Exercice 6.4: Backpropagation - 2 Couches ###\\n')\n",
    "\n",
    "x_val, y_true, w1, w2, alpha = 2.0, 0.9, 0.4, 0.6, 0.5\n",
    "losses, w1_hist, w2_hist = [], [w1], [w2]\n",
    "\n",
    "print(f'R√©seau: x --[w‚ÇÅ]--> z‚ÇÅ --[œÉ]--> a‚ÇÅ --[w‚ÇÇ]--> z‚ÇÇ --[œÉ]--> a‚ÇÇ --> L=(a‚ÇÇ-y)¬≤')\n",
    "print(f'Donn√©es: x={x_val}, y_true={y_true}, w‚ÇÅ={w1}, w‚ÇÇ={w2}, Œ±={alpha}\\n')\n",
    "\n",
    "for i in range(15):\n",
    "    z1 = w1 * x_val\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = w2 * a1\n",
    "    a2 = sigmoid(z2)\n",
    "    L = (a2 - y_true)**2\n",
    "    \n",
    "    dL_da2 = 2 * (a2 - y_true)\n",
    "    da2_dz2 = sigmoid_derivative(a2)\n",
    "    dz2_dw2 = a1\n",
    "    dz2_da1 = w2\n",
    "    dL_dw2 = dL_da2 * da2_dz2 * dz2_dw2\n",
    "    \n",
    "    dL_da1 = dL_da2 * da2_dz2 * dz2_da1\n",
    "    da1_dz1 = sigmoid_derivative(a1)\n",
    "    dz1_dw1 = x_val\n",
    "    dL_dw1 = dL_da1 * da1_dz1 * dz1_dw1\n",
    "    \n",
    "    w1 = w1 - alpha * dL_dw1\n",
    "    w2 = w2 - alpha * dL_dw2\n",
    "    \n",
    "    if i in [0, 7, 14]:\n",
    "        print(f'It√©ration {i+1}: a‚ÇÇ={a2:.4f}, L={L:.6f}, w‚ÇÅ: {w1_hist[-1]:.4f}‚Üí{w1:.4f}, w‚ÇÇ: {w2_hist[-1]:.4f}‚Üí{w2:.4f}')\n",
    "    \n",
    "    losses.append(L)\n",
    "    w1_hist.append(w1)\n",
    "    w2_hist.append(w2)\n",
    "\n",
    "print(f'\\nR√©sultat final: w‚ÇÅ={w1:.4f}, w‚ÇÇ={w2:.4f}, Loss={losses[-1]:.6f}\\n')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "axes[0].plot(losses, 'b-o', linewidth=2, markersize=5)\n",
    "axes[0].set_title('Convergence Loss (2 couches)')\n",
    "axes[0].set_ylabel('Loss L')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[1].plot(w1_hist, 'r-o', linewidth=2, markersize=5)\n",
    "axes[1].set_title('√âvolution w‚ÇÅ')\n",
    "axes[1].set_ylabel('Poids w‚ÇÅ')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[2].plot(w2_hist, 'g-o', linewidth=2, markersize=5)\n",
    "axes[2].set_title('√âvolution w‚ÇÇ')\n",
    "axes[2].set_ylabel('Poids w‚ÇÇ')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "print('SOLUTION 7.2: Optimisation 2D - Hessienne')\nprint('='*70)\n\nx, y = sp.symbols('x y')\nf = x**2 - 4*x + y**2 + 6*y + 5\n\ndf_dx = sp.diff(f, x)\ndf_dy = sp.diff(f, y)\n\nprint(f'f(x,y) = x¬≤ - 4x + y¬≤ + 6y + 5')\nprint(f'‚àÇf/‚àÇx = {df_dx}, ‚àÇf/‚àÇy = {df_dy}\\n')\n\ncritical = sp.solve([df_dx, df_dy], [x, y])\nxc, yc = float(critical[x]), float(critical[y])\nf_val = float(f.subs(critical))\n\nprint(f'Point critique: ({xc}, {yc})')\nprint(f'f({xc}, {yc}) = {f_val}')\n\n# Hessienne\nfxx = sp.diff(f, x, 2)\nfyy = sp.diff(f, y, 2)\nfxy = sp.diff(f, x, y)\nprint(f'\\nHessienne H = [[‚àÇ¬≤f/‚àÇx¬≤, ‚àÇ¬≤f/‚àÇx‚àÇy], [‚àÇ¬≤f/‚àÇy‚àÇx, ‚àÇ¬≤f/‚àÇy¬≤]]')\nprint(f'           = [[{fxx}, {fxy}], [{fxy}, {fyy}]]')\nprint(f'\\nValeurs propres: [2, 2] > 0 ‚Üí MINIMUM ‚úì')\n\nx_range = np.linspace(-2, 6, 30)\ny_range = np.linspace(-8, 2, 30)\nX, Y = np.meshgrid(x_range, y_range)\nZ = X**2 - 4*X + Y**2 + 6*Y + 5\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax.scatter([xc], [yc], [f_val], color='red', s=100, label='Minimum')\nax.set_title('Surface 3D avec Minimum')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.legend()\nplt.tight_layout()\nplt.show()\nprint('‚úì Graphique 3D sauvegard√©')",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.5: Chain Rule - ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "print('SOLUTION 7.3: Point-Selle')\nprint('='*70)\n\nf_saddle = x**2 - y**2\nprint(f'f(x,y) = x¬≤ - y¬≤ (fonction selle)')\nprint(f'‚àÇf/‚àÇx = {sp.diff(f_saddle, x)}, ‚àÇf/‚àÇy = {sp.diff(f_saddle, y)}')\nprint(f'Point critique: (0, 0)')\n\n# Hessienne pour selle\nfxx_s = sp.diff(f_saddle, x, 2)\nfyy_s = sp.diff(f_saddle, y, 2)\nfxy_s = sp.diff(f_saddle, x, y)\nprint(f'\\nHessienne H = [[{fxx_s}, {fxy_s}], [{fxy_s}, {fyy_s}]]')\nprint(f'Valeurs propres: [2, -2] (signes oppos√©s)')\nprint(f'‚Üí POINT-SELLE! ‚úì\\n')\n\nx_range_s = np.linspace(-3, 3, 50)\ny_range_s = np.linspace(-3, 3, 50)\nX_s, Y_s = np.meshgrid(x_range_s, y_range_s)\nZ_s = X_s**2 - Y_s**2\n\nfig = plt.figure(figsize=(12, 5))\nax1 = fig.add_subplot(121, projection='3d')\nax1.plot_surface(X_s, Y_s, Z_s, cmap='coolwarm', alpha=0.8)\nax1.scatter([0], [0], [0], color='red', s=100)\nax1.set_title('Selle de Cheval')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\n\nax2 = fig.add_subplot(122)\nax2.contour(X_s, Y_s, Z_s, levels=15)\nax2.plot(0, 0, 'r*', markersize=15)\nax2.set_title('Contours')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\nprint('‚úì Graphique point-selle sauvegard√©')",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 6.5: Chain Rule - ReLU ###\\n')\n",
    "\n",
    "def relu(z):\n",
    "    return max(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return 1.0 if z > 0 else 0.0\n",
    "\n",
    "x_val, w = -2.0, 0.5\n",
    "z = w * x_val\n",
    "a = relu(z)\n",
    "L = a**2\n",
    "\n",
    "print(f'R√©seau: x --[w]--> z = w√óx --[ReLU]--> a = max(0,z) --> L = a¬≤')\n",
    "print(f'Donn√©es: x={x_val}, w={w}\\n')\n",
    "print(f'FORWARD PASS:')\n",
    "print(f'  z = {w} √ó {x_val} = {z}')\n",
    "print(f'  a = ReLU({z}) = {a}')\n",
    "print(f'  L = {a}¬≤ = {L}\\n')\n",
    "\n",
    "dL_da = 2 * a\n",
    "da_dz = relu_derivative(z)\n",
    "dz_dw = x_val\n",
    "dL_dw = dL_da * da_dz * dz_dw\n",
    "\n",
    "print(f'BACKWARD PASS:')\n",
    "print(f'  dL/da = 2a = {dL_da}')\n",
    "print(f'  da/dz = ReLU\\'({z}) = {da_dz}  ‚Üê Gradient dispara√Æt! (z < 0)')\n",
    "print(f'  dL/dw = {dL_da} √ó {da_dz} √ó {dz_dw} = {dL_dw}')\n",
    "print(f'\\n‚ö†Ô∏è Probl√®me: Dying ReLU! Les neurones n√©gatifs ne s\\'adaptent pas.\\n')\n",
    "\n",
    "print(f'Avec x = 2.0 (entr√©e positive):  ')\n",
    "z_pos = 0.5 * 2.0\n",
    "a_pos = relu(z_pos)\n",
    "dL_dw_pos = 2 * a_pos * relu_derivative(z_pos) * 2.0\n",
    "print(f'  dL/dw = {dL_dw_pos}  ‚Üê Gradient actif!\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "print('SOLUTION 7.4: Optimisation avec Contrainte')\nprint('='*70)\n\nprint('Minimiser: f(x,y) = x¬≤ + y¬≤')\nprint('Contrainte: x + y = 4\\n')\n\ng = x**2 + (4-x)**2\ng_prime = sp.diff(g, x)\nx_opt = sp.solve(g_prime, x)[0]\ny_opt = 4 - x_opt\n\nprint('Par substitution: y = 4 - x')\nprint(f'g(x) = x¬≤ + (4-x)¬≤ = {sp.expand(g)}')\nprint(f'g\\'(x) = {g_prime} = 0')\nprint(f'‚Üí x = {x_opt}')\nprint(f'‚Üí y = {y_opt}')\nprint(f'Minimum: f({x_opt}, {y_opt}) = {float(g.subs(x, x_opt))}\\n')\n\nx_range_c = np.linspace(-1, 5, 100)\ny_range_c = np.linspace(-1, 5, 100)\nX_c, Y_c = np.meshgrid(x_range_c, y_range_c)\nZ_c = X_c**2 + Y_c**2\n\nfig, ax = plt.subplots(figsize=(10, 8))\nax.contour(X_c, Y_c, Z_c, levels=20, cmap='viridis')\nax.plot(x_range_c, 4 - x_range_c, 'r-', linewidth=2, label='Contrainte: x+y=4')\nax.plot(float(x_opt), float(y_opt), 'r*', markersize=20, label=f'Optimal: ({float(x_opt)}, {float(y_opt)})')\nax.set_title('Minimisation avec Contrainte')\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_xlim(-1, 5)\nax.set_ylim(-1, 5)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.tight_layout()\nplt.show()\nprint('‚úì Graphique sauvegard√©')",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.6: Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "print('SOLUTION 7.5: Comparaison des M√©thodes d\\'Optimisation')\nprint('='*70)\n\nfrom scipy.optimize import minimize\n\nf_opt = (x - 2)**2 + (y + 1)**2\nprint('Fonction: f(x,y) = (x-2)¬≤ + (y+1)¬≤')\nprint('Optimum th√©orique: (2, -1)\\n')\n\n# M√©thode 1: Analytique\ndf_dx_opt = sp.diff(f_opt, x)\ndf_dy_opt = sp.diff(f_opt, y)\ncritical_opt = sp.solve([df_dx_opt, df_dy_opt], [x, y])\nx_a, y_a = float(critical_opt[x]), float(critical_opt[y])\nf_a = float(f_opt.subs(critical_opt))\nprint(f'M√©thode 1 - Analytique: ({x_a:.4f}, {y_a:.4f}), f={f_a:.6f}')\n\n# M√©thode 2: Gradient Descent\ndef f_num(xy):\n    return (xy[0]-2)**2 + (xy[1]+1)**2\n\ndef grad(xy):\n    return np.array([2*(xy[0]-2), 2*(xy[1]+1)])\n\nxy_gd = np.array([0.0, 0.0])\nfor _ in range(50):\n    xy_gd = xy_gd - 0.1 * grad(xy_gd)\n\nprint(f'M√©thode 2 - Gradient Descent (50 iter): ({xy_gd[0]:.4f}, {xy_gd[1]:.4f}), f={f_num(xy_gd):.6f}')\n\n# M√©thode 3: SciPy\nresult = minimize(f_num, [0.0, 0.0], method='BFGS')\nprint(f'M√©thode 3 - SciPy BFGS: ({result.x[0]:.4f}, {result.x[1]:.4f}), f={result.fun:.6f}')\nprint(f'  It√©rations: {result.nit}, √âvaluations: {result.nfev}\\n')\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\nlosses_gd = [f_num(np.array([0.0, 0.0]))]\nxy = np.array([0.0, 0.0])\nfor _ in range(50):\n    xy = xy - 0.1 * grad(xy)\n    losses_gd.append(f_num(xy))\n\naxes[0].semilogy(losses_gd, 'b-o', linewidth=2, markersize=4, label='Gradient Descent')\naxes[0].set_title('Convergence Gradient Descent')\naxes[0].set_ylabel('Loss f(x,y)')\naxes[0].set_xlabel('It√©ration')\naxes[0].grid(True, alpha=0.3)\naxes[0].legend()\n\nx_range_comp = np.linspace(-2, 4, 50)\ny_range_comp = np.linspace(-4, 2, 50)\nX_comp, Y_comp = np.meshgrid(x_range_comp, y_range_comp)\nZ_comp = (X_comp - 2)**2 + (Y_comp + 1)**2\naxes[1].contour(X_comp, Y_comp, Z_comp, levels=20, cmap='viridis')\naxes[1].plot(2, -1, 'r*', markersize=20, label='Optimum')\naxes[1].plot(0, 0, 'go', markersize=8, label='D√©part')\naxes[1].set_title('Contours et Point Optimal')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('y')\n\nplt.tight_layout()\nplt.show()\nprint('‚úì Graphiques sauvegard√©s')",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 6.6: Computational Graph ###\\n')\n",
    "\n",
    "a = sp.Symbol('a')\n",
    "b = sp.Symbol('b')\n",
    "x_sym = sp.Symbol('x')\n",
    "\n",
    "a_def = x_sym**2\n",
    "b_def = 2*x_sym\n",
    "L_expr = (a_def + b_def)**2\n",
    "\n",
    "dL_dab = 2 * (a_def + b_def)\n",
    "da_dx = 2 * x_sym\n",
    "db_dx = 2\n",
    "dL_dx_chain = dL_dab * da_dx + dL_dab * db_dx\n",
    "dL_dx_chain_simplified = sp.expand(dL_dx_chain)\n",
    "\n",
    "L_direct = (x_sym**2 + 2*x_sym)**2\n",
    "dL_dx_direct = sp.expand(sp.diff(L_direct, x_sym))\n",
    "\n",
    "print('Pour L = (a + b)¬≤ o√π a = x¬≤ et b = 2x:')\n",
    "print('\\nM√âTHODE 1 - Chain Rule:')\n",
    "print(f'  dL/dx = 2(x¬≤ + 2x) √ó (2x + 2) = {dL_dx_chain_simplified}')\n",
    "print('\\nM√âTHODE 2 - D√©rivation Directe:')\n",
    "print(f'  L = (x¬≤ + 2x)¬≤, dL/dx = {dL_dx_direct}')\n",
    "print(f'\\nV√©rification: {sp.simplify(dL_dx_chain_simplified - dL_dx_direct) == 0}‚úì')\n",
    "\n",
    "x_val = 3\n",
    "dL_dx_val = dL_dx_direct.subs(x_sym, x_val)\n",
    "print(f'\\n√âvaluation en x=3:')\n",
    "print(f'  a = 9, b = 6, L = (9+6)¬≤ = 225')\n",
    "print(f'  dL/dx = {dL_dx_val}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèîÔ∏è Section 7 : Optimisation - Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Solutions Section 7: Optimisation - Trouver Minima/Maxima')\n",
    "print('='*60)\n",
    "\n",
    "# EXERCICE 7.1\n",
    "print('\\n### Exercice 7.1: Points Critiques 1D ###\\n')\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "f = x**3 - 6*x**2 + 9*x + 1\n",
    "f_prime = sp.diff(f, x)\n",
    "f_double_prime = sp.diff(f_prime, x)\n",
    "\n",
    "print(f'f(x) = x¬≥ - 6x¬≤ + 9x + 1')\n",
    "print(f'f\\'(x) = {f_prime}')\n",
    "print(f'f\\'\\'(x) = {f_double_prime}\\n')\n",
    "\n",
    "critical_points = sp.solve(f_prime, x)\n",
    "print(f'Points critiques: {critical_points}\\n')\n",
    "\n",
    "for xc in critical_points:\n",
    "    f_val = f.subs(x, xc)\n",
    "    f_pp_val = f_double_prime.subs(x, xc)\n",
    "    nature = 'Minimum' if f_pp_val > 0 else 'Maximum' if f_pp_val < 0 else 'Inflexion'\n",
    "    print(f'x = {xc}: f({xc}) = {f_val}, f\\'\\'({xc}) = {f_pp_val} ‚Üí {nature}\\n')\n",
    "\n",
    "x_vals = np.linspace(-1, 6, 200)\n",
    "y_vals = x_vals**3 - 6*x_vals**2 + 9*x_vals + 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x)')\n",
    "for xc in critical_points:\n",
    "    f_val = float(f.subs(x, xc))\n",
    "    color = 'go' if f_double_prime.subs(x, xc) > 0 else 'ro'\n",
    "    ax.plot(float(xc), f_val, color, markersize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_title('f(x) = x¬≥ - 6x¬≤ + 9x + 1 - Points Critiques')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.2: Optimisation 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 7.2: Optimisation 2D ###\\n')\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = x**2 - 4*x + y**2 + 6*y + 5\n",
    "\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "\n",
    "print(f'f(x,y) = x¬≤ - 4x + y¬≤ + 6y + 5')\n",
    "print(f'‚àÇf/‚àÇx = {df_dx}, ‚àÇf/‚àÇy = {df_dy}\\n')\n",
    "\n",
    "critical = sp.solve([df_dx, df_dy], [x, y])\n",
    "xc, yc = float(critical[x]), float(critical[y])\n",
    "f_val = float(f.subs(critical))\n",
    "\n",
    "print(f'Point critique: ({xc}, {yc})')\n",
    "print(f'f({xc}, {yc}) = {f_val}')\n",
    "print(f'Hessienne: [[2, 0], [0, 2]] ‚Üí Valeurs propres = [2, 2] > 0 ‚Üí Minimum!\\n')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "x_range = np.linspace(-2, 6, 30)\n",
    "y_range = np.linspace(-8, 2, 30)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = X**2 - 4*X + Y**2 + 6*Y + 5\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax.scatter([xc], [yc], [f_val], color='red', s=100, label='Minimum')\n",
    "ax.set_title('Surface 3D avec Minimum')\n",
    "ax.set_ylabel('y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique 3D sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.3: Point-Selle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 7.3: Point-Selle ###\\n')\n",
    "\n",
    "f_saddle = x**2 - y**2\n",
    "print(f'f(x,y) = x¬≤ - y¬≤ (fonction selle)')\n",
    "print(f'‚àÇf/‚àÇx = {sp.diff(f_saddle, x)}, ‚àÇf/‚àÇy = {sp.diff(f_saddle, y)}')\n",
    "print(f'Point critique: (0, 0)')\n",
    "print(f'Hessienne: [[2, 0], [0, -2]] ‚Üí Valeurs propres = [2, -2] (signes oppos√©s)')\n",
    "print(f'‚Üí Point-selle!\\n')\n",
    "\n",
    "x_range_s = np.linspace(-3, 3, 50)\n",
    "y_range_s = np.linspace(-3, 3, 50)\n",
    "X_s, Y_s = np.meshgrid(x_range_s, y_range_s)\n",
    "Z_s = X_s**2 - Y_s**2\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X_s, Y_s, Z_s, cmap='coolwarm', alpha=0.8)\n",
    "ax1.scatter([0], [0], [0], color='red', s=100)\n",
    "ax1.set_title('Selle de Cheval')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.contour(X_s, Y_s, Z_s, levels=15)\n",
    "ax2.plot(0, 0, 'r*', markersize=15)\n",
    "ax2.set_title('Contours')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique point-selle sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.4: Optimisation avec Contrainte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 7.4: Optimisation avec Contrainte ###\\n')\n",
    "\n",
    "print('Minimiser: f(x,y) = x¬≤ + y¬≤')\n",
    "print('Contrainte: x + y = 4\\n')\n",
    "\n",
    "g = x**2 + (4-x)**2\n",
    "g_prime = sp.diff(g, x)\n",
    "x_opt = sp.solve(g_prime, x)[0]\n",
    "y_opt = 4 - x_opt\n",
    "\n",
    "print(f'Par substitution: y = 4 - x')\n",
    "print(f'g(x) = x¬≤ + (4-x)¬≤ = {sp.expand(g)}')\n",
    "print(f'g\\'(x) = {g_prime} = 0 ‚Üí x = {x_opt}')\n",
    "print(f'y = {y_opt}')\n",
    "print(f'Minimum: f({x_opt}, {y_opt}) = {float(g.subs(x, x_opt))}\\n')\n",
    "\n",
    "x_range_c = np.linspace(-1, 5, 100)\n",
    "y_range_c = np.linspace(-1, 5, 100)\n",
    "X_c, Y_c = np.meshgrid(x_range_c, y_range_c)\n",
    "Z_c = X_c**2 + Y_c**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.contour(X_c, Y_c, Z_c, levels=20, cmap='viridis')\n",
    "ax.plot(x_range_c, 4 - x_range_c, 'r-', linewidth=2, label='Contrainte: x+y=4')\n",
    "ax.plot(float(x_opt), float(y_opt), 'r*', markersize=20, label=f'Optimal: ({float(x_opt)}, {float(y_opt)})')\n",
    "ax.set_title('Minimisation avec Contrainte')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-1, 5)\n",
    "ax.set_ylim(-1, 5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.5: Comparaison des M√©thodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 7.5: Comparaison des M√©thodes d\\'Optimisation ###\\n')\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "f_opt = (x - 2)**2 + (y + 1)**2\n",
    "print('Fonction: f(x,y) = (x-2)¬≤ + (y+1)¬≤')\n",
    "print('Optimum: (2, -1)\\n')\n",
    "\n",
    "# M√©thode 1: Analytique\n",
    "df_dx_opt = sp.diff(f_opt, x)\n",
    "df_dy_opt = sp.diff(f_opt, y)\n",
    "critical_opt = sp.solve([df_dx_opt, df_dy_opt], [x, y])\n",
    "x_a, y_a = float(critical_opt[x]), float(critical_opt[y])\n",
    "f_a = float(f_opt.subs(critical_opt))\n",
    "print(f'M√©thode 1 - Analytique: ({x_a:.4f}, {y_a:.4f}), f={f_a:.6f}')\n",
    "\n",
    "# M√©thode 2: Gradient Descent\n",
    "def f_num(xy):\n",
    "    return (xy[0]-2)**2 + (xy[1]+1)**2\n",
    "\n",
    "def grad(xy):\n",
    "    return np.array([2*(xy[0]-2), 2*(xy[1]+1)])\n",
    "\n",
    "xy_gd = np.array([0.0, 0.0])\n",
    "for _ in range(50):\n",
    "    xy_gd = xy_gd - 0.1 * grad(xy_gd)\n",
    "\n",
    "print(f'M√©thode 2 - Gradient Descent: ({xy_gd[0]:.4f}, {xy_gd[1]:.4f}), f={f_num(xy_gd):.6f}')\n",
    "\n",
    "# M√©thode 3: SciPy\n",
    "result = minimize(f_num, [0.0, 0.0], method='BFGS')\n",
    "print(f'M√©thode 3 - SciPy BFGS: ({result.x[0]:.4f}, {result.x[1]:.4f}), f={result.fun:.6f}')\n",
    "print(f'\\nIt√©rations SciPy: {result.nit}, √âvaluations: {result.nfev}\\n')\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "losses_gd = [f_num(np.array([0.0, 0.0]))]\n",
    "xy = np.array([0.0, 0.0])\n",
    "for _ in range(50):\n",
    "    xy = xy - 0.1 * grad(xy)\n",
    "    losses_gd.append(f_num(xy))\n",
    "\n",
    "axes[0].semilogy(losses_gd, 'b-o', linewidth=2, markersize=4)\n",
    "axes[0].set_title('Convergence Gradient Descent')\n",
    "axes[0].set_ylabel('Loss f(x,y)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "x_range_comp = np.linspace(-2, 4, 50)\n",
    "y_range_comp = np.linspace(-4, 2, 50)\n",
    "X_comp, Y_comp = np.meshgrid(x_range_comp, y_range_comp)\n",
    "Z_comp = (X_comp - 2)**2 + (Y_comp + 1)**2\n",
    "axes[1].contour(X_comp, Y_comp, Z_comp, levels=20, cmap='viridis')\n",
    "axes[1].plot(2, -1, 'r*', markersize=20, label='Optimum')\n",
    "axes[1].plot(0, 0, 'go', markersize=8, label='D√©part')\n",
    "axes[1].set_title('Contours et Point Optimal')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphiques sauvegard√©s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ F√©licitations !\n",
    "\n",
    "**Solutions compl√®tes des sections 6 et 7!**\n",
    "\n",
    "### üí° Points cl√©s\n",
    "\n",
    "**Section 6 - R√®gle de la Cha√Æne:**\n",
    "- (f‚àòg)'(x) = f'(g(x)) √ó g'(x)\n",
    "- Backpropagation = chain rule appliqu√©e\n",
    "- ReLU: gradient nul pour z < 0 (dying ReLU)\n",
    "\n",
    "**Section 7 - Optimisation:**\n",
    "- Points critiques: ‚àáf = 0\n",
    "- Hessienne pour classifier (min/max/selle)\n",
    "- Gradient Descent vs Analytique vs SciPy\n",
    "\n",
    "**Excellent travail! üí™**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}