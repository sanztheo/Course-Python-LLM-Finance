{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcda SOLUTIONS - Calcul Diff\u00e9rentiel\n",
    "\n",
    "**Solutions compl\u00e8tes et d\u00e9taill\u00e9es** \ud83c\udfaf\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcd6 Comment utiliser ?\n",
    "\n",
    "1. Essayez l'exercice vous-m\u00eame d'abord\n",
    "2. Comparez avec la solution\n",
    "3. Comprenez la m\u00e9thode et les \u00e9tapes\n",
    "4. Ex\u00e9cutez le code pour v\u00e9rifier\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : D\u00e9riv\u00e9es Partielles - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.1 : D\u00e9riv\u00e9es Partielles Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x, y, z = sp.symbols('x y z')\n",
    "\n",
    "# Solution 4.1: D\u00e9riv\u00e9es Partielles Simples\n",
    "print('SOLUTION 4.1: D\u00e9riv\u00e9es Partielles Simples')\n",
    "print('='*70)\n",
    "\n",
    "f = x**2 * y + 3*x*y**2\n",
    "\n",
    "# D\u00e9riv\u00e9e partielle par rapport \u00e0 x (y est constante)\n",
    "df_dx = sp.diff(f, x)\n",
    "# D\u00e9riv\u00e9e partielle par rapport \u00e0 y (x est constante)\n",
    "df_dy = sp.diff(f, y)\n",
    "\n",
    "print(f'f(x,y) = {f}')\n",
    "print(f'\u2202f/\u2202x = {df_dx}  (traiter y comme constante)')\n",
    "print(f'\u2202f/\u2202y = {df_dy}  (traiter x comme constante)')\n",
    "\n",
    "# \u00c9valuation en (2, 3)\n",
    "val_x = df_dx.subs([(x, 2), (y, 3)])\n",
    "val_y = df_dy.subs([(x, 2), (y, 3)])\n",
    "print(f'\\n\u2202f/\u2202x|_(2,3) = {val_x}')\n",
    "print(f'\u2202f/\u2202y|_(2,3) = {val_y}')\n",
    "print(f'\\nInterpr\u00e9tation: En (2,3), f augmente de {val_x} unit\u00e9s si x augmente')\n",
    "print(f'                et de {val_y} unit\u00e9s si y augmente')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.2 : Fonction \u00e0 Trois Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.2: Fonction \u00e0 Trois Variables\n",
    "print('\\n\\nSOLUTION 4.2: Fonction \u00e0 Trois Variables')\n",
    "print('='*70)\n",
    "\n",
    "f = x**2 + y**2 + z**2 + x*y + y*z\n",
    "\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "df_dz = sp.diff(f, z)\n",
    "\n",
    "print(f'f(x,y,z) = {f}')\n",
    "print(f'\\nD\u00e9riv\u00e9es partielles:')\n",
    "print(f'\u2202f/\u2202x = {df_dx}')\n",
    "print(f'\u2202f/\u2202y = {df_dy}')\n",
    "print(f'\u2202f/\u2202z = {df_dz}')\n",
    "\n",
    "print(f'\\nLe gradient \u2207f(x,y,z) = ')\n",
    "print(f'[ {df_dx} ]')\n",
    "print(f'[ {df_dy} ]')\n",
    "print(f'[ {df_dz} ]')\n",
    "\n",
    "# Exemple d'\u00e9valuation\n",
    "point = {x: 1, y: 1, z: 1}\n",
    "grad_val = [df_dx.subs(point), df_dy.subs(point), df_dz.subs(point)]\n",
    "print(f'\\n\u2207f(1,1,1) = {grad_val}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.3 : D\u00e9riv\u00e9es Partielles d'Ordre 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.3: D\u00e9riv\u00e9es Partielles d'Ordre 2\n",
    "print('\\n\\nSOLUTION 4.3: D\u00e9riv\u00e9es Partielles d\\'Ordre 2')\n",
    "print('='*70)\n",
    "\n",
    "f = x**3 * y**2\n",
    "\n",
    "# D\u00e9riv\u00e9es secondes\n",
    "f_xx = sp.diff(f, x, 2)  # D\u00e9riv\u00e9e 2 fois par rapport \u00e0 x\n",
    "f_yy = sp.diff(f, y, 2)  # D\u00e9riv\u00e9e 2 fois par rapport \u00e0 y\n",
    "f_xy = sp.diff(f, x, y)  # D\u00e9riv\u00e9e d'abord par x, puis par y\n",
    "f_yx = sp.diff(f, y, x)  # D\u00e9riv\u00e9e d'abord par y, puis par x\n",
    "\n",
    "print(f'f(x,y) = {f}')\n",
    "print(f'\\nPremi\u00e8res d\u00e9riv\u00e9es:')\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "print(f'\u2202f/\u2202x = {df_dx}')\n",
    "print(f'\u2202f/\u2202y = {df_dy}')\n",
    "\n",
    "print(f'\\nD\u00e9riv\u00e9es secondes:')\n",
    "print(f'\u2202\u00b2f/\u2202x\u00b2 = {f_xx}')\n",
    "print(f'\u2202\u00b2f/\u2202y\u00b2 = {f_yy}')\n",
    "print(f'\u2202\u00b2f/\u2202x\u2202y = {f_xy}')\n",
    "print(f'\u2202\u00b2f/\u2202y\u2202x = {f_yx}')\n",
    "\n",
    "# V\u00e9rification du th\u00e9or\u00e8me de Schwarz\n",
    "difference = sp.simplify(f_xy - f_yx)\n",
    "print(f'\\nTh\u00e9or\u00e8me de Schwarz: \u2202\u00b2f/\u2202x\u2202y = \u2202\u00b2f/\u2202y\u2202x?')\n",
    "print(f'Diff\u00e9rence: {difference}')\n",
    "print(f'\u2713 \u00c9gales! (pour fonctions bien-behaved)' if difference == 0 else '\u2717 Diff\u00e9rentes!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.4 : Visualisation de Surface 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.4: Visualisation de Surface 3D\n",
    "print('\\n\\nSOLUTION 4.4: Visualisation de Surface 3D')\n",
    "print('='*70)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Surface 3D\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "x_vals = np.linspace(-3, 3, 100)\n",
    "y_vals = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "Z = X**2 - Y**2\n",
    "\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "ax1.set_xlabel('x', fontsize=10)\n",
    "ax1.set_ylabel('y', fontsize=10)\n",
    "ax1.set_zlabel('f(x,y)', fontsize=10)\n",
    "ax1.set_title('Surface: f(x,y) = x\u00b2 - y\u00b2', fontsize=11)\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Contour plot avec gradients\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Calcul des gradients en plusieurs points\n",
    "points = [(1, 1), (-1, 1), (1, -1), (-1, -1)]\n",
    "for px, py in points:\n",
    "    grad_x = 2*px     # \u2202f/\u2202x = 2x\n",
    "    grad_y = -2*py    # \u2202f/\u2202y = -2y\n",
    "    ax2.quiver(px, py, grad_x, grad_y, scale=15, color='red', width=0.003)\n",
    "    ax2.plot(px, py, 'ro', markersize=6)\n",
    "\n",
    "ax2.set_xlabel('x', fontsize=10)\n",
    "ax2.set_ylabel('y', fontsize=10)\n",
    "ax2.set_title('Contours + Champ de gradients', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# D\u00e9riv\u00e9es num\u00e9riques\n",
    "ax3 = fig.add_subplot(133)\n",
    "point_x, point_y = 1, 1\n",
    "grad_x_val = 2*point_x\n",
    "grad_y_val = -2*point_y\n",
    "text_content = f'En ({point_x},{point_y}):\\n'\n",
    "text_content += f'\u2202f/\u2202x = 2x = {grad_x_val}\\n'\n",
    "text_content += f'\u2202f/\u2202y = -2y = {grad_y_val}\\n\\n'\n",
    "text_content += f'Gradient \u2207f = [{grad_x_val}, {grad_y_val}]\\n'\n",
    "text_content += f'Norme: |\u2207f| = {np.sqrt(grad_x_val**2 + grad_y_val**2):.3f}'\n",
    "ax3.text(0.1, 0.5, text_content, fontsize=11, family='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "         verticalalignment='center')\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Observations:')\n",
    "print('1. Les gradients (vecteurs rouges) sont perpendiculaires aux courbes de niveau')\n",
    "print('2. Les gradients pointent vers l\\'augmentation la plus rapide de f')\n",
    "print('3. C\\'est une surface de type \"selle de cheval\" (hyperbolo\u00efde)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.5 : Application ML - Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.5: Application ML - Loss Function\n",
    "print('\\n\\nSOLUTION 4.5: Application ML - Loss Function')\n",
    "print('='*70)\n",
    "\n",
    "w, b = sp.symbols('w b')\n",
    "x_data, y_data = 2, 5\n",
    "\n",
    "# Loss function: L = (y - \u0177)\u00b2 o\u00f9 \u0177 = wx + b\n",
    "y_pred = w * x_data + b\n",
    "L = (y_data - y_pred)**2\n",
    "\n",
    "print(f'Donn\u00e9es: x={x_data}, y={y_data}')\n",
    "print(f'Mod\u00e8le: \u0177 = wx + b = {x_data}w + b')\n",
    "print(f'Loss: L = (y - \u0177)\u00b2 = (5 - (2w + b))\u00b2')\n",
    "print(f'D\u00e9velopp\u00e9: L = {sp.expand(L)}')\n",
    "\n",
    "# D\u00e9riv\u00e9es partielles\n",
    "dL_dw = sp.diff(L, w)\n",
    "dL_db = sp.diff(L, b)\n",
    "\n",
    "print(f'\\nGradients:')\n",
    "print(f'\u2202L/\u2202w = {dL_dw}')\n",
    "print(f'\u2202L/\u2202b = {dL_db}')\n",
    "\n",
    "# \u00c9valuation en (w=1, b=0)\n",
    "w_val, b_val = 1, 0\n",
    "grad_w = dL_dw.subs([(w, w_val), (b, b_val)])\n",
    "grad_b = dL_db.subs([(w, w_val), (b, b_val)])\n",
    "loss_val = L.subs([(w, w_val), (b, b_val)])\n",
    "\n",
    "print(f'\\n\u00c9valuation en (w={w_val}, b={b_val}):')\n",
    "print(f'L({w_val},{b_val}) = {loss_val}')\n",
    "print(f'\u2202L/\u2202w|_(1,0) = {grad_w}')\n",
    "print(f'\u2202L/\u2202b|_(1,0) = {grad_b}')\n",
    "\n",
    "print(f'\\nInterprtation pour Gradient Descent:')\n",
    "print(f'Les gradients sont N\u00c9GATIFS: w_new = w - \u03b1\u2217\u2202L/\u2202w = 1 - \u03b1\u2217({grad_w}) \u2192 augmente w')\n",
    "print(f'                              b_new = b - \u03b1\u2217\u2202L/\u2202b = 0 - \u03b1\u2217({grad_b}) \u2192 augmente b')\n",
    "print(f'Cela a du sens: on augmente w et b pour que \u0177 = 2w + b se rapproche de y = 5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : Gradient - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.1 : Calculer le Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.1: Calculer le Gradient\n",
    "print('\\n\\nSOLUTION 5.1: Calculer le Gradient')\n",
    "print('='*70)\n",
    "\n",
    "x, y, z = sp.symbols('x y z')\n",
    "\n",
    "# Fonction 1\n",
    "f1 = x**2 + y**2\n",
    "grad_f1 = [sp.diff(f1, x), sp.diff(f1, y)]\n",
    "print(f'1. f(x,y) = {f1}')\n",
    "print(f'   \u2207f = {grad_f1}')\n",
    "\n",
    "# Fonction 2\n",
    "f2 = x*y\n",
    "grad_f2 = [sp.diff(f2, x), sp.diff(f2, y)]\n",
    "print(f'\\n2. g(x,y) = {f2}')\n",
    "print(f'   \u2207g = {grad_f2}')\n",
    "\n",
    "# Fonction 3\n",
    "f3 = x**2 + 2*y**2 + 3*z**2\n",
    "grad_f3 = [sp.diff(f3, x), sp.diff(f3, y), sp.diff(f3, z)]\n",
    "print(f'\\n3. h(x,y,z) = {f3}')\n",
    "print(f'   \u2207h = {grad_f3}')\n",
    "\n",
    "# Fonction 4\n",
    "f4 = sp.exp(x + y)\n",
    "grad_f4 = [sp.diff(f4, x), sp.diff(f4, y)]\n",
    "print(f'\\n4. k(x,y) = {f4}')\n",
    "print(f'   \u2207k = {grad_f4}')\n",
    "print(f'   Simplifi\u00e9e: \u2207k = [e^(x+y), e^(x+y)]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.2 : Norme et Direction du Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.2: Norme et Direction du Gradient\n",
    "print('\\n\\nSOLUTION 5.2: Norme et Direction du Gradient')\n",
    "print('='*70)\n",
    "\n",
    "# f(x,y) = x\u00b2 + 2y\u00b2\n",
    "f = x**2 + 2*y**2\n",
    "grad_x = sp.diff(f, x)\n",
    "grad_y = sp.diff(f, y)\n",
    "\n",
    "print(f'f(x,y) = {f}')\n",
    "print(f'\u2207f = [{grad_x}, {grad_y}]')\n",
    "\n",
    "# \u00c9valuation en (1, 1)\n",
    "point = {x: 1, y: 1}\n",
    "grad_val_x = grad_x.subs(point)\n",
    "grad_val_y = grad_y.subs(point)\n",
    "grad_val = np.array([float(grad_val_x), float(grad_val_y)])\n",
    "\n",
    "print(f'\\nEn (1, 1):')\n",
    "print(f'\u2207f(1,1) = [{grad_val_x}, {grad_val_y}] = {grad_val}')\n",
    "\n",
    "# Norme du gradient\n",
    "norm = np.linalg.norm(grad_val)\n",
    "print(f'\\nNorme: ||\u2207f|| = \u221a(2\u00b2 + 4\u00b2) = \u221a20 = {norm:.4f}')\n",
    "\n",
    "# Direction normalis\u00e9e (vecteur unitaire)\n",
    "direction = grad_val / norm\n",
    "print(f'\\nDirection normalis\u00e9e (vecteur unitaire):')\n",
    "print(f'\u00fb = \u2207f / ||\u2207f|| = [{direction[0]:.4f}, {direction[1]:.4f}]')\n",
    "\n",
    "print(f'\\nInterpr\u00e9tation:')\n",
    "print(f'- Le gradient \u2207f = [2, 4] indique la MONT\u00c9E la plus rapide')\n",
    "print(f'- Pour la DESCENTE, on va dans la direction -\u2207f = [-2, -4]')\n",
    "print(f'- Le taux de mont\u00e9e maximal est ||\u2207f|| = {norm:.4f} unit\u00e9s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.3 : Gradient Descent 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.3: Gradient Descent 2D\n",
    "print('\\n\\nSOLUTION 5.3: Gradient Descent 2D')\n",
    "print('='*70)\n",
    "\n",
    "# f(x,y) = (x-3)\u00b2 + (y+2)\u00b2\n",
    "# \u2207f = [2(x-3), 2(y+2)]\n",
    "\n",
    "def f(x_val, y_val):\n",
    "    return (x_val - 3)**2 + (y_val + 2)**2\n",
    "\n",
    "def gradient(x_val, y_val):\n",
    "    grad_x = 2*(x_val - 3)\n",
    "    grad_y = 2*(y_val + 2)\n",
    "    return np.array([grad_x, grad_y])\n",
    "\n",
    "# Gradient Descent\n",
    "x_pos, y_pos = 0.0, 0.0\n",
    "alpha = 0.1\n",
    "iterations = 30\n",
    "\n",
    "trajectory = [(x_pos, y_pos)]\n",
    "losses = [f(x_pos, y_pos)]\n",
    "\n",
    "print(f'Param\u00e8tres:')\n",
    "print(f'Position initiale: ({x_pos}, {y_pos})')\n",
    "print(f'Learning rate \u03b1 = {alpha}')\n",
    "print(f'It\u00e9rations: {iterations}')\n",
    "print(f'\\nAlgorithme: (x,y) = (x,y) - \u03b1\u2207f')\n",
    "\n",
    "for i in range(iterations):\n",
    "    grad = gradient(x_pos, y_pos)\n",
    "    x_pos -= alpha * grad[0]\n",
    "    y_pos -= alpha * grad[1]\n",
    "    trajectory.append((x_pos, y_pos))\n",
    "    losses.append(f(x_pos, y_pos))\n",
    "\n",
    "trajectory = np.array(trajectory)\n",
    "print(f'\\nR\u00e9sultats:')\n",
    "print(f'Position finale: ({x_pos:.6f}, {y_pos:.6f})')\n",
    "print(f'Minimum attendu: (3, -2)')\n",
    "print(f'Loss initiale: {losses[0]:.6f}')\n",
    "print(f'Loss finale: {losses[-1]:.6f}')\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Contour plot avec trajectoire\n",
    "ax1 = axes[0]\n",
    "x_range = np.linspace(-1, 5, 100)\n",
    "y_range = np.linspace(-4, 1, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = (X - 3)**2 + (Y + 2)**2\n",
    "\n",
    "contour = ax1.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "ax1.plot(trajectory[:, 0], trajectory[:, 1], 'r.-', linewidth=2, markersize=4, label='Trajectoire')\n",
    "ax1.plot(0, 0, 'go', markersize=10, label='D\u00e9part')\n",
    "ax1.plot(3, -2, 'r*', markersize=20, label='Minimum')\n",
    "ax1.set_xlabel('x', fontsize=11)\n",
    "ax1.set_ylabel('y', fontsize=11)\n",
    "ax1.set_title('Gradient Descent: Trajectoire', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss vs iteration\n",
    "ax2 = axes[1]\n",
    "ax2.semilogy(range(len(losses)), losses, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "ax2.set_xlabel('It\u00e9ration', fontsize=11)\n",
    "ax2.set_ylabel('Loss (\u00e9chelle log)', fontsize=11)\n",
    "ax2.set_title('D\u00e9croissance de la Loss', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.4 : Champ de Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.4: Champ de Gradients\n",
    "print('\\n\\nSOLUTION 5.4: Champ de Gradients')\n",
    "print('='*70)\n",
    "\n",
    "# f(x,y) = 0.5*(x\u00b2 + y\u00b2)\n",
    "# \u2207f = [x, y]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Cr\u00e9ation de la grille\n",
    "x_range = np.linspace(-3, 3, 20)\n",
    "y_range = np.linspace(-3, 3, 20)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = 0.5 * (X**2 + Y**2)\n",
    "\n",
    "# Calcul du gradient\n",
    "U = X  # \u2202f/\u2202x = x\n",
    "V = Y  # \u2202f/\u2202y = y\n",
    "\n",
    "# Contours\n",
    "contour = ax.contour(X, Y, Z, levels=15, cmap='viridis', alpha=0.6)\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Champ de vecteurs (gradients)\n",
    "quiver = ax.quiver(X, Y, U, V, np.sqrt(U**2 + V**2), cmap='hot', scale=30, width=0.003)\n",
    "cbar = plt.colorbar(quiver, ax=ax)\n",
    "cbar.set_label('||\u2207f||', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('f(x,y) = 0.5(x\u00b2 + y\u00b2): Contours + Champ de Gradients', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Observations cl\u00e9s:')\n",
    "print(f'1. Les vecteurs de gradient sont PERPENDICULAIRES aux courbes de niveau')\n",
    "print(f'2. Les vecteurs pointent VERS LES COURBES PLUS \u00c9LEV\u00c9ES (mont\u00e9e)')\n",
    "print(f'3. La norme des vecteurs augmente en s\\'\u00e9loignant du centre')\n",
    "print(f'4. Au centre (0,0), le gradient est nul (c\\'est le minimum)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.5 : D\u00e9riv\u00e9es Directionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.5: D\u00e9riv\u00e9es Directionnelles\n",
    "print('\\n\\nSOLUTION 5.5: D\u00e9riv\u00e9es Directionnelles')\n",
    "print('='*70)\n",
    "\n",
    "# f(x,y) = x\u00b2 + xy + y\u00b2\n",
    "f = x**2 + x*y + y**2\n",
    "\n",
    "# Gradient\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "\n",
    "print(f'f(x,y) = {f}')\n",
    "print(f'\u2207f = [{df_dx}, {df_dy}]')\n",
    "\n",
    "# \u00c9valuation en (1, 2)\n",
    "point = {x: 1, y: 2}\n",
    "grad_x_val = float(df_dx.subs(point))\n",
    "grad_y_val = float(df_dy.subs(point))\n",
    "grad = np.array([grad_x_val, grad_y_val])\n",
    "\n",
    "print(f'\\nEn (1,2): \u2207f = {grad}')\n",
    "\n",
    "# Diff\u00e9rentes directions\n",
    "directions = {\n",
    "    'axe x': np.array([1, 0]),\n",
    "    'axe y': np.array([0, 1]),\n",
    "    'diagonale': np.array([1, 1]) / np.sqrt(2)\n",
    "}\n",
    "\n",
    "print(f'\\nD\u00e9riv\u00e9es directionnelles: D_u f = \u2207f \u00b7 u')\n",
    "for name, direction in directions.items():\n",
    "    # Normalisation\n",
    "    u = direction / np.linalg.norm(direction)\n",
    "    # D\u00e9riv\u00e9e directionnelle = produit scalaire\n",
    "    d_u = np.dot(grad, u)\n",
    "    print(f'\\nDirection {name}: u = {u}')\n",
    "    print(f'D_u f(1,2) = \u2207f \u00b7 u = {d_u:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.6 : R\u00e9gression Lin\u00e9aire par Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.6: R\u00e9gression Lin\u00e9aire par Gradient Descent\n",
    "print('\\n\\nSOLUTION 5.6: R\u00e9gression Lin\u00e9aire par Gradient Descent')\n",
    "print('='*70)\n",
    "\n",
    "# Donn\u00e9es\n",
    "x_data = np.array([1, 2, 3])\n",
    "y_data = np.array([2, 4, 5])\n",
    "n = len(x_data)\n",
    "\n",
    "# Mod\u00e8le: \u0177 = wx + b\n",
    "# Loss: L = (1/n) * \u03a3(y_i - (w*x_i + b))\u00b2\n",
    "\n",
    "def loss_function(w, b):\n",
    "    y_pred = w * x_data + b\n",
    "    return np.mean((y_data - y_pred)**2)\n",
    "\n",
    "def gradient(w, b):\n",
    "    y_pred = w * x_data + b\n",
    "    residuals = y_data - y_pred\n",
    "    dL_dw = -2 * np.mean(residuals * x_data)\n",
    "    dL_db = -2 * np.mean(residuals)\n",
    "    return dL_dw, dL_db\n",
    "\n",
    "# Gradient Descent\n",
    "w, b = 0.0, 0.0\n",
    "alpha = 0.01\n",
    "iterations = 100\n",
    "\n",
    "losses = []\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "print(f'Donn\u00e9es: x={list(x_data)}, y={list(y_data)}')\n",
    "print(f'\\nMod\u00e8le: \u0177 = wx + b')\n",
    "print(f'Loss: L = (1/{n}) * \u03a3(y_i - \u0177_i)\u00b2')\n",
    "print(f'\\nParam\u00e8tres:')\n",
    "print(f'Learning rate \u03b1 = {alpha}')\n",
    "print(f'It\u00e9rations: {iterations}')\n",
    "\n",
    "for i in range(iterations):\n",
    "    dL_dw, dL_db = gradient(w, b)\n",
    "    w -= alpha * dL_dw\n",
    "    b -= alpha * dL_db\n",
    "    loss = loss_function(w, b)\n",
    "    losses.append(loss)\n",
    "    weights.append(w)\n",
    "    biases.append(b)\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f'It\u00e9ration {i+1:3d}: w={w:.4f}, b={b:.4f}, Loss={loss:.6f}')\n",
    "\n",
    "print(f'\\nR\u00e9sultats finaux:')\n",
    "print(f'w = {w:.6f}')\n",
    "print(f'b = {b:.6f}')\n",
    "print(f'Droite: \u0177 = {w:.4f}x + {b:.4f}')\n",
    "\n",
    "# Solution analytique (r\u00e9gression lin\u00e9aire)\n",
    "x_mean = np.mean(x_data)\n",
    "y_mean = np.mean(y_data)\n",
    "w_analytical = np.sum((x_data - x_mean) * (y_data - y_mean)) / np.sum((x_data - x_mean)**2)\n",
    "b_analytical = y_mean - w_analytical * x_mean\n",
    "\n",
    "print(f'\\nSolution analytique (r\u00e9gression lin\u00e9aire):')\n",
    "print(f'w = {w_analytical:.6f}')\n",
    "print(f'b = {b_analytical:.6f}')\n",
    "print(f'Droite: \u0177 = {w_analytical:.4f}x + {b_analytical:.4f}')\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Donn\u00e9es et droite de r\u00e9gression\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(x_data, y_data, s=100, color='red', label='Donn\u00e9es', zorder=3)\n",
    "x_line = np.linspace(0, 4, 100)\n",
    "y_line_gd = w * x_line + b\n",
    "y_line_analytical = w_analytical * x_line + b_analytical\n",
    "ax1.plot(x_line, y_line_gd, 'b-', linewidth=2, label=f'GD: \u0177={w:.4f}x+{b:.4f}')\n",
    "ax1.plot(x_line, y_line_analytical, 'g--', linewidth=2, label=f'Ana: \u0177={w_analytical:.4f}x+{b_analytical:.4f}')\n",
    "ax1.set_xlabel('x', fontsize=11)\n",
    "ax1.set_ylabel('y', fontsize=11)\n",
    "ax1.set_title('R\u00e9gression Lin\u00e9aire', fontsize=12)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss vs iteration\n",
    "ax2 = axes[1]\n",
    "ax2.semilogy(range(iterations), losses, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('It\u00e9ration', fontsize=11)\n",
    "ax2.set_ylabel('Loss (\u00e9chelle log)', fontsize=11)\n",
    "ax2.set_title('Convergence de la Loss', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n\u2713 Les deux m\u00e9thodes convergent vers la m\u00eame solution!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83c\udf89 F\u00e9licitations !\n",
    "\n",
    "**Vous avez toutes les solutions pour les Sections 4 et 5 !**\n",
    "\n",
    "### \ud83d\udca1 Conseils\n",
    "\n",
    "- Refaites les exercices difficiles\n",
    "- Pratiquez avec vos propres exemples\n",
    "- Comprenez POURQUOI, pas seulement COMMENT\n",
    "\n",
    "### \ud83d\ude80 Prochaines \u00c9tapes\n",
    "\n",
    "1. Sections 6 et 7 (R\u00e8gle de la Cha\u00eene et Optimisation)\n",
    "2. Projet: Visualisation de Gradients\n",
    "3. Alg\u00e8bre Lin\u00e9aire\n",
    "\n",
    "**Excellent travail! \ud83d\udcaa**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}