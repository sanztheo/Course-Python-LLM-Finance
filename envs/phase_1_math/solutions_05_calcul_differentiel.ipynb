{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö SOLUTIONS - Calcul Diff√©rentiel\n\n**Solutions compl√®tes pour Sections 4 et 5** üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : D√©riv√©es Partielles - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.1 : D√©riv√©es Partielles Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx, y, z = sp.symbols('x y z')\nprint('SOLUTION 4.1: D√©riv√©es Partielles Simples')\nprint('='*70)\n\nf = x**2 * y + 3*x*y**2\ndf_dx = sp.diff(f, x)\ndf_dy = sp.diff(f, y)\n\nprint(f'f(x,y) = {f}')\nprint(f'‚àÇf/‚àÇx = {df_dx}')\nprint(f'‚àÇf/‚àÇy = {df_dy}')\n\nval_x = df_dx.subs([(x, 2), (y, 3)])\nval_y = df_dy.subs([(x, 2), (y, 3)])\nprint(f'\\n‚àÇf/‚àÇx|_(2,3) = {val_x}')\nprint(f'‚àÇf/‚àÇy|_(2,3) = {val_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.2 : Fonction √† Trois Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.2: Fonction √† Trois Variables')\nprint('='*70)\n\nf = x**2 + y**2 + z**2 + x*y + y*z\ndf_dx = sp.diff(f, x)\ndf_dy = sp.diff(f, y)\ndf_dz = sp.diff(f, z)\n\nprint(f'f(x,y,z) = {f}')\nprint(f'‚àÇf/‚àÇx = {df_dx}')\nprint(f'‚àÇf/‚àÇy = {df_dy}')\nprint(f'‚àÇf/‚àÇz = {df_dz}')\nprint(f'\\n‚àáf(1,1,1) = [{df_dx.subs({x:1, y:1, z:1})}, {df_dy.subs({x:1, y:1, z:1})}, {df_dz.subs({x:1, y:1, z:1})}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.3 : D√©riv√©es Partielles d'Ordre 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.3: D√©riv√©es Partielles d\\'Ordre 2')\nprint('='*70)\n\nf = x**3 * y**2\nf_xx = sp.diff(f, x, 2)\nf_yy = sp.diff(f, y, 2)\nf_xy = sp.diff(f, x, y)\nf_yx = sp.diff(f, y, x)\n\nprint(f'f(x,y) = {f}')\nprint(f'‚àÇ¬≤f/‚àÇx¬≤ = {f_xx}')\nprint(f'‚àÇ¬≤f/‚àÇy¬≤ = {f_yy}')\nprint(f'‚àÇ¬≤f/‚àÇx‚àÇy = {f_xy}')\nprint(f'‚àÇ¬≤f/‚àÇy‚àÇx = {f_yx}')\nprint(f'Th√©or√®me Schwarz: {sp.simplify(f_xy - f_yx) == 0}')  # True si √©gales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.4 : Visualisation de Surface 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.4: Visualisation de Surface 3D')\nprint('='*70)\n\nfig = plt.figure(figsize=(15, 5))\nax1 = fig.add_subplot(131, projection='3d')\n\nx_vals = np.linspace(-3, 3, 100)\ny_vals = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x_vals, y_vals)\nZ = X**2 - Y**2\n\nsurf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('Surface: f(x,y) = x¬≤ - y¬≤')\n\nax2 = fig.add_subplot(132)\ncontour = ax2.contour(X, Y, Z, levels=15, cmap='viridis')\nfor px, py in [(1, 1), (-1, 1), (1, -1)]:\n    ax2.quiver(px, py, 2*px, -2*py, scale=15, color='red')\nax2.set_title('Contours + Gradients')\n\nax3 = fig.add_subplot(133)\nax3.text(0.5, 0.5, 'En (1,1):\\n‚àÇf/‚àÇx = 2\\n‚àÇf/‚àÇy = -2', ha='center', fontsize=12)\nax3.axis('off')\n\nplt.tight_layout()\nplt.show()\nprint('Surface visualis√©e ‚úì')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.5 : Application ML - Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.5: Application ML - Loss Function')\nprint('='*70)\n\nw, b = sp.symbols('w b')\nx_data, y_data = 2, 5\n\nL = (y_data - (w*x_data + b))**2\ndL_dw = sp.diff(L, w)\ndL_db = sp.diff(L, b)\n\nprint(f'Donn√©es: x={x_data}, y={y_data}')\nprint(f'L = {sp.expand(L)}')\nprint(f'‚àÇL/‚àÇw = {dL_dw}')\nprint(f'‚àÇL/‚àÇb = {dL_db}')\n\ngrad_w = dL_dw.subs([(w, 1), (b, 0)])\ngrad_b = dL_db.subs([(w, 1), (b, 0)])\nprint(f'\\nEn (w=1, b=0):')\nprint(f'‚àÇL/‚àÇw = {grad_w}')\nprint(f'‚àÇL/‚àÇb = {grad_b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : Gradient - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.1 : Calculer le Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.1: Calculer le Gradient')\nprint('='*70)\n\nf1 = x**2 + y**2\ngrad_f1 = [sp.diff(f1, x), sp.diff(f1, y)]\nprint(f'1. f(x,y) = {f1} ‚Üí ‚àáf = {grad_f1}')\n\nf2 = x*y\ngrad_f2 = [sp.diff(f2, x), sp.diff(f2, y)]\nprint(f'2. g(x,y) = {f2} ‚Üí ‚àág = {grad_f2}')\n\nf3 = x**2 + 2*y**2 + 3*z**2\ngrad_f3 = [sp.diff(f3, x), sp.diff(f3, y), sp.diff(f3, z)]\nprint(f'3. h(x,y,z) = {f3} ‚Üí ‚àáh = {grad_f3}')\n\nf4 = sp.exp(x + y)\ngrad_f4 = [sp.diff(f4, x), sp.diff(f4, y)]\nprint(f'4. k(x,y) = {f4} ‚Üí ‚àák = {grad_f4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.2 : Norme et Direction du Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.2: Norme et Direction du Gradient')\nprint('='*70)\n\nf = x**2 + 2*y**2\ngrad_x = sp.diff(f, x)\ngrad_y = sp.diff(f, y)\n\ngrad_val = np.array([2.0, 4.0])  # En (1,1)\nnorm = np.linalg.norm(grad_val)\n\nprint(f'f(x,y) = {f}')\nprint(f'En (1,1): ‚àáf = {grad_val}')\nprint(f'||‚àáf|| = {norm:.4f}')\nprint(f'Direction normalis√©e: {grad_val/norm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.3 : Gradient Descent 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.3: Gradient Descent 2D')\nprint('='*70)\n\ndef f(x_val, y_val):\n    return (x_val - 3)**2 + (y_val + 2)**2\n\ndef grad(x_val, y_val):\n    return np.array([2*(x_val - 3), 2*(y_val + 2)])\n\nx_pos, y_pos = 0.0, 0.0\nalpha, iterations = 0.1, 30\nlosses = []\n\nfor i in range(iterations):\n    g = grad(x_pos, y_pos)\n    x_pos -= alpha * g[0]\n    y_pos -= alpha * g[1]\n    losses.append(f(x_pos, y_pos))\n\nprint(f'Position initiale: (0, 0)')\nprint(f'Position finale: ({x_pos:.4f}, {y_pos:.4f})')\nprint(f'Minimum attendu: (3, -2)')\nprint(f'Loss finale: {losses[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.4 : Champ de Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.4: Champ de Gradients')\nprint('='*70)\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nx_range = np.linspace(-3, 3, 20)\ny_range = np.linspace(-3, 3, 20)\nX, Y = np.meshgrid(x_range, y_range)\nZ = 0.5 * (X**2 + Y**2)\n\ncontour = ax.contour(X, Y, Z, levels=15, alpha=0.6)\nquiver = ax.quiver(X, Y, X, Y, np.sqrt(X**2 + Y**2), cmap='hot', scale=30)\nax.set_title('Champ de gradients: f(x,y) = 0.5(x¬≤ + y¬≤)')\nax.axis('equal')\nplt.show()\nprint('Champ de gradients visualis√© ‚úì')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.5 : D√©riv√©es Directionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.5: D√©riv√©es Directionnelles')\nprint('='*70)\n\nf = x**2 + x*y + y**2\ndf_dx = sp.diff(f, x)\ndf_dy = sp.diff(f, y)\n\ngrad = np.array([4.0, 5.0])  # En (1,2)\n\nprint(f'f(x,y) = {f}')\nprint(f'En (1,2): ‚àáf = {grad}')\n\ndirections = {\n    'axe x': np.array([1, 0]),\n    'axe y': np.array([0, 1]),\n    'diagonale': np.array([1, 1])/np.sqrt(2)\n}\n\nfor name, u in directions.items():\n    d_u = np.dot(grad, u/np.linalg.norm(u))\n    print(f'{name}: D_u f = {d_u:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.6 : R√©gression Lin√©aire par Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.6: R√©gression Lin√©aire par Gradient Descent')\nprint('='*70)\n\nx_data = np.array([1, 2, 3])\ny_data = np.array([2, 4, 5])\nn = len(x_data)\n\ndef loss(w, b):\n    return np.mean((y_data - (w*x_data + b))**2)\n\ndef grad(w, b):\n    y_pred = w*x_data + b\n    residuals = y_data - y_pred\n    return -2*np.mean(residuals*x_data), -2*np.mean(residuals)\n\nw, b = 0.0, 0.0\nalpha = 0.01\n\nfor i in range(100):\n    dw, db = grad(w, b)\n    w -= alpha * dw\n    b -= alpha * db\n\nprint(f'Gradient Descent: w={w:.4f}, b={b:.4f}')\n\n# Solution analytique\nx_mean = np.mean(x_data)\ny_mean = np.mean(y_data)\nw_a = np.sum((x_data - x_mean)*(y_data - y_mean)) / np.sum((x_data - x_mean)**2)\nb_a = y_mean - w_a * x_mean\n\nprint(f'Analytique: w={w_a:.4f}, b={b_a:.4f}')\nprint(f'\\n‚úì Les deux convergen vers la m√™me solution!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## üéâ F√©licitations!\n\n",
    "**Solutions Sections 4-5 compl√®tes!**\n\n",
    "### Contenu\n\n",
    "- ‚úì Ex 4.1: D√©riv√©es partielles simples\n",
    "- ‚úì Ex 4.2: Trois variables + gradient\n",
    "- ‚úì Ex 4.3: D√©riv√©es d'ordre 2\n",
    "- ‚úì Ex 4.4: Visualisation 3D\n",
    "- ‚úì Ex 4.5: Loss function ML\n",
    "- ‚úì Ex 5.1: Calculer gradients\n",
    "- ‚úì Ex 5.2: Norme et direction\n",
    "- ‚úì Ex 5.3: Gradient Descent 2D\n",
    "- ‚úì Ex 5.4: Champ de vecteurs\n",
    "- ‚úì Ex 5.5: D√©riv√©es directionnelles\n",
    "- ‚úì Ex 5.6: R√©gression lin√©aire\n\n",
    "**Excellent travail! üí™**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.2: Chain Rule Multivari√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 6.2: Chain Rule Multivari√©e ###\\n')\n",
    "\n",
    "t = sp.Symbol('t')\n",
    "y = sp.Symbol('y')\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "z = x**2 + y**2\n",
    "x_t = t**2\n",
    "y_t = t**3\n",
    "\n",
    "dz_dx = sp.diff(z, x)\n",
    "dz_dy = sp.diff(z, y)\n",
    "dx_dt = sp.diff(x_t, t)\n",
    "dy_dt = sp.diff(y_t, t)\n",
    "\n",
    "dz_dt = dz_dx * dx_dt + dz_dy * dy_dt\n",
    "\n",
    "print(f'z = x¬≤ + y¬≤ o√π x = t¬≤, y = t¬≥')\n",
    "print(f'‚àÇz/‚àÇx = {dz_dx}, ‚àÇz/‚àÇy = {dz_dy}')\n",
    "print(f'dx/dt = {dx_dt}, dy/dt = {dy_dt}')\n",
    "print(f'dz/dt = {sp.expand(dz_dt)}')\n",
    "\n",
    "z_direct = t**4 + t**6\n",
    "dz_dt_direct = sp.diff(z_direct, t)\n",
    "print(f'V√©rification directe: {dz_dt_direct}')\n",
    "print(f'R√©sultats identiques: {sp.simplify(dz_dt - dz_dt_direct) == 0}‚úì\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.3: Backpropagation 1 Couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 6.3: Backpropagation - 1 Couche ###\\n')\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "x_val, y_true, w, alpha = 3.0, 0.8, 0.5, 0.5\n",
    "losses, ws = [], [w]\n",
    "\n",
    "print(f'R√©seau: x --[w]--> z = w√óx --[œÉ]--> a = œÉ(z) ---> L = (a-y)¬≤')\n",
    "print(f'Donn√©es: x={x_val}, y_true={y_true}, w={w}, Œ±={alpha}\\n')\n",
    "\n",
    "for i in range(10):\n",
    "    z = w * x_val\n",
    "    a = sigmoid(z)\n",
    "    L = (a - y_true)**2\n",
    "    \n",
    "    dL_da = 2 * (a - y_true)\n",
    "    da_dz = sigmoid_derivative(a)\n",
    "    dz_dw = x_val\n",
    "    dL_dw = dL_da * da_dz * dz_dw\n",
    "    \n",
    "    w = w - alpha * dL_dw\n",
    "    \n",
    "    if i in [0, 4, 9]:\n",
    "        print(f'It√©ration {i+1}: z={z:.4f}, a={a:.4f}, L={L:.6f}, w: {ws[-1]:.4f} ‚Üí {w:.4f}')\n",
    "    \n",
    "    losses.append(L)\n",
    "    ws.append(w)\n",
    "\n",
    "print(f'\\nR√©sultat: w={w:.4f}, Loss final={losses[-1]:.6f}')\n",
    "print(f'R√©duction: {(losses[0]-losses[-1])/losses[0]*100:.1f}%')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(losses, 'b-o', linewidth=2, markersize=5)\n",
    "ax1.set_title('Convergence Loss - Backprop 1 Couche')\n",
    "ax1.set_ylabel('Loss L')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax2.plot(ws, 'r-o', linewidth=2, markersize=5)\n",
    "ax2.axhline(y=y_true, color='g', linestyle='--', label='Optimal')\n",
    "ax2.set_title('√âvolution Poids w')\n",
    "ax2.set_ylabel('Poids w')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.4: Backpropagation 2 Couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 6.4: Backpropagation - 2 Couches ###\\n')\n",
    "\n",
    "x_val, y_true, w1, w2, alpha = 2.0, 0.9, 0.4, 0.6, 0.5\n",
    "losses, w1_hist, w2_hist = [], [w1], [w2]\n",
    "\n",
    "print(f'R√©seau: x --[w‚ÇÅ]--> z‚ÇÅ --[œÉ]--> a‚ÇÅ --[w‚ÇÇ]--> z‚ÇÇ --[œÉ]--> a‚ÇÇ --> L=(a‚ÇÇ-y)¬≤')\n",
    "print(f'Donn√©es: x={x_val}, y_true={y_true}, w‚ÇÅ={w1}, w‚ÇÇ={w2}, Œ±={alpha}\\n')\n",
    "\n",
    "for i in range(15):\n",
    "    z1 = w1 * x_val\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = w2 * a1\n",
    "    a2 = sigmoid(z2)\n",
    "    L = (a2 - y_true)**2\n",
    "    \n",
    "    dL_da2 = 2 * (a2 - y_true)\n",
    "    da2_dz2 = sigmoid_derivative(a2)\n",
    "    dz2_dw2 = a1\n",
    "    dz2_da1 = w2\n",
    "    dL_dw2 = dL_da2 * da2_dz2 * dz2_dw2\n",
    "    \n",
    "    dL_da1 = dL_da2 * da2_dz2 * dz2_da1\n",
    "    da1_dz1 = sigmoid_derivative(a1)\n",
    "    dz1_dw1 = x_val\n",
    "    dL_dw1 = dL_da1 * da1_dz1 * dz1_dw1\n",
    "    \n",
    "    w1 = w1 - alpha * dL_dw1\n",
    "    w2 = w2 - alpha * dL_dw2\n",
    "    \n",
    "    if i in [0, 7, 14]:\n",
    "        print(f'It√©ration {i+1}: a‚ÇÇ={a2:.4f}, L={L:.6f}, w‚ÇÅ: {w1_hist[-1]:.4f}‚Üí{w1:.4f}, w‚ÇÇ: {w2_hist[-1]:.4f}‚Üí{w2:.4f}')\n",
    "    \n",
    "    losses.append(L)\n",
    "    w1_hist.append(w1)\n",
    "    w2_hist.append(w2)\n",
    "\n",
    "print(f'\\nR√©sultat final: w‚ÇÅ={w1:.4f}, w‚ÇÇ={w2:.4f}, Loss={losses[-1]:.6f}\\n')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "axes[0].plot(losses, 'b-o', linewidth=2, markersize=5)\n",
    "axes[0].set_title('Convergence Loss (2 couches)')\n",
    "axes[0].set_ylabel('Loss L')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[1].plot(w1_hist, 'r-o', linewidth=2, markersize=5)\n",
    "axes[1].set_title('√âvolution w‚ÇÅ')\n",
    "axes[1].set_ylabel('Poids w‚ÇÅ')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[2].plot(w2_hist, 'g-o', linewidth=2, markersize=5)\n",
    "axes[2].set_title('√âvolution w‚ÇÇ')\n",
    "axes[2].set_ylabel('Poids w‚ÇÇ')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.5: Chain Rule - ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 6.5: Chain Rule - ReLU ###\\n')\n",
    "\n",
    "def relu(z):\n",
    "    return max(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return 1.0 if z > 0 else 0.0\n",
    "\n",
    "x_val, w = -2.0, 0.5\n",
    "z = w * x_val\n",
    "a = relu(z)\n",
    "L = a**2\n",
    "\n",
    "print(f'R√©seau: x --[w]--> z = w√óx --[ReLU]--> a = max(0,z) --> L = a¬≤')\n",
    "print(f'Donn√©es: x={x_val}, w={w}\\n')\n",
    "print(f'FORWARD PASS:')\n",
    "print(f'  z = {w} √ó {x_val} = {z}')\n",
    "print(f'  a = ReLU({z}) = {a}')\n",
    "print(f'  L = {a}¬≤ = {L}\\n')\n",
    "\n",
    "dL_da = 2 * a\n",
    "da_dz = relu_derivative(z)\n",
    "dz_dw = x_val\n",
    "dL_dw = dL_da * da_dz * dz_dw\n",
    "\n",
    "print(f'BACKWARD PASS:')\n",
    "print(f'  dL/da = 2a = {dL_da}')\n",
    "print(f'  da/dz = ReLU\\'({z}) = {da_dz}  ‚Üê Gradient dispara√Æt! (z < 0)')\n",
    "print(f'  dL/dw = {dL_da} √ó {da_dz} √ó {dz_dw} = {dL_dw}')\n",
    "print(f'\\n‚ö†Ô∏è Probl√®me: Dying ReLU! Les neurones n√©gatifs ne s\\'adaptent pas.\\n')\n",
    "\n",
    "print(f'Avec x = 2.0 (entr√©e positive):  ')\n",
    "z_pos = 0.5 * 2.0\n",
    "a_pos = relu(z_pos)\n",
    "dL_dw_pos = 2 * a_pos * relu_derivative(z_pos) * 2.0\n",
    "print(f'  dL/dw = {dL_dw_pos}  ‚Üê Gradient actif!\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.6: Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 6.6: Computational Graph ###\\n')\n",
    "\n",
    "a = sp.Symbol('a')\n",
    "b = sp.Symbol('b')\n",
    "x_sym = sp.Symbol('x')\n",
    "\n",
    "a_def = x_sym**2\n",
    "b_def = 2*x_sym\n",
    "L_expr = (a_def + b_def)**2\n",
    "\n",
    "dL_dab = 2 * (a_def + b_def)\n",
    "da_dx = 2 * x_sym\n",
    "db_dx = 2\n",
    "dL_dx_chain = dL_dab * da_dx + dL_dab * db_dx\n",
    "dL_dx_chain_simplified = sp.expand(dL_dx_chain)\n",
    "\n",
    "L_direct = (x_sym**2 + 2*x_sym)**2\n",
    "dL_dx_direct = sp.expand(sp.diff(L_direct, x_sym))\n",
    "\n",
    "print('Pour L = (a + b)¬≤ o√π a = x¬≤ et b = 2x:')\n",
    "print('\\nM√âTHODE 1 - Chain Rule:')\n",
    "print(f'  dL/dx = 2(x¬≤ + 2x) √ó (2x + 2) = {dL_dx_chain_simplified}')\n",
    "print('\\nM√âTHODE 2 - D√©rivation Directe:')\n",
    "print(f'  L = (x¬≤ + 2x)¬≤, dL/dx = {dL_dx_direct}')\n",
    "print(f'\\nV√©rification: {sp.simplify(dL_dx_chain_simplified - dL_dx_direct) == 0}‚úì')\n",
    "\n",
    "x_val = 3\n",
    "dL_dx_val = dL_dx_direct.subs(x_sym, x_val)\n",
    "print(f'\\n√âvaluation en x=3:')\n",
    "print(f'  a = 9, b = 6, L = (9+6)¬≤ = 225')\n",
    "print(f'  dL/dx = {dL_dx_val}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèîÔ∏è Section 7 : Optimisation - Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Solutions Section 7: Optimisation - Trouver Minima/Maxima')\n",
    "print('='*60)\n",
    "\n",
    "# EXERCICE 7.1\n",
    "print('\\n### Exercice 7.1: Points Critiques 1D ###\\n')\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "f = x**3 - 6*x**2 + 9*x + 1\n",
    "f_prime = sp.diff(f, x)\n",
    "f_double_prime = sp.diff(f_prime, x)\n",
    "\n",
    "print(f'f(x) = x¬≥ - 6x¬≤ + 9x + 1')\n",
    "print(f'f\\'(x) = {f_prime}')\n",
    "print(f'f\\'\\'(x) = {f_double_prime}\\n')\n",
    "\n",
    "critical_points = sp.solve(f_prime, x)\n",
    "print(f'Points critiques: {critical_points}\\n')\n",
    "\n",
    "for xc in critical_points:\n",
    "    f_val = f.subs(x, xc)\n",
    "    f_pp_val = f_double_prime.subs(x, xc)\n",
    "    nature = 'Minimum' if f_pp_val > 0 else 'Maximum' if f_pp_val < 0 else 'Inflexion'\n",
    "    print(f'x = {xc}: f({xc}) = {f_val}, f\\'\\'({xc}) = {f_pp_val} ‚Üí {nature}\\n')\n",
    "\n",
    "x_vals = np.linspace(-1, 6, 200)\n",
    "y_vals = x_vals**3 - 6*x_vals**2 + 9*x_vals + 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_vals, y_vals, 'b-', linewidth=2, label='f(x)')\n",
    "for xc in critical_points:\n",
    "    f_val = float(f.subs(x, xc))\n",
    "    color = 'go' if f_double_prime.subs(x, xc) > 0 else 'ro'\n",
    "    ax.plot(float(xc), f_val, color, markersize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_title('f(x) = x¬≥ - 6x¬≤ + 9x + 1 - Points Critiques')\n",
    "ax.set_ylabel('f(x)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.2: Optimisation 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 7.2: Optimisation 2D ###\\n')\n",
    "\n",
    "x, y = sp.symbols('x y')\n",
    "f = x**2 - 4*x + y**2 + 6*y + 5\n",
    "\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "\n",
    "print(f'f(x,y) = x¬≤ - 4x + y¬≤ + 6y + 5')\n",
    "print(f'‚àÇf/‚àÇx = {df_dx}, ‚àÇf/‚àÇy = {df_dy}\\n')\n",
    "\n",
    "critical = sp.solve([df_dx, df_dy], [x, y])\n",
    "xc, yc = float(critical[x]), float(critical[y])\n",
    "f_val = float(f.subs(critical))\n",
    "\n",
    "print(f'Point critique: ({xc}, {yc})')\n",
    "print(f'f({xc}, {yc}) = {f_val}')\n",
    "print(f'Hessienne: [[2, 0], [0, 2]] ‚Üí Valeurs propres = [2, 2] > 0 ‚Üí Minimum!\\n')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "x_range = np.linspace(-2, 6, 30)\n",
    "y_range = np.linspace(-8, 2, 30)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = X**2 - 4*X + Y**2 + 6*Y + 5\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax.scatter([xc], [yc], [f_val], color='red', s=100, label='Minimum')\n",
    "ax.set_title('Surface 3D avec Minimum')\n",
    "ax.set_ylabel('y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique 3D sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.3: Point-Selle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 7.3: Point-Selle ###\\n')\n",
    "\n",
    "f_saddle = x**2 - y**2\n",
    "print(f'f(x,y) = x¬≤ - y¬≤ (fonction selle)')\n",
    "print(f'‚àÇf/‚àÇx = {sp.diff(f_saddle, x)}, ‚àÇf/‚àÇy = {sp.diff(f_saddle, y)}')\n",
    "print(f'Point critique: (0, 0)')\n",
    "print(f'Hessienne: [[2, 0], [0, -2]] ‚Üí Valeurs propres = [2, -2] (signes oppos√©s)')\n",
    "print(f'‚Üí Point-selle!\\n')\n",
    "\n",
    "x_range_s = np.linspace(-3, 3, 50)\n",
    "y_range_s = np.linspace(-3, 3, 50)\n",
    "X_s, Y_s = np.meshgrid(x_range_s, y_range_s)\n",
    "Z_s = X_s**2 - Y_s**2\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X_s, Y_s, Z_s, cmap='coolwarm', alpha=0.8)\n",
    "ax1.scatter([0], [0], [0], color='red', s=100)\n",
    "ax1.set_title('Selle de Cheval')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.contour(X_s, Y_s, Z_s, levels=15)\n",
    "ax2.plot(0, 0, 'r*', markersize=15)\n",
    "ax2.set_title('Contours')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique point-selle sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.4: Optimisation avec Contrainte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 7.4: Optimisation avec Contrainte ###\\n')\n",
    "\n",
    "print('Minimiser: f(x,y) = x¬≤ + y¬≤')\n",
    "print('Contrainte: x + y = 4\\n')\n",
    "\n",
    "g = x**2 + (4-x)**2\n",
    "g_prime = sp.diff(g, x)\n",
    "x_opt = sp.solve(g_prime, x)[0]\n",
    "y_opt = 4 - x_opt\n",
    "\n",
    "print(f'Par substitution: y = 4 - x')\n",
    "print(f'g(x) = x¬≤ + (4-x)¬≤ = {sp.expand(g)}')\n",
    "print(f'g\\'(x) = {g_prime} = 0 ‚Üí x = {x_opt}')\n",
    "print(f'y = {y_opt}')\n",
    "print(f'Minimum: f({x_opt}, {y_opt}) = {float(g.subs(x, x_opt))}\\n')\n",
    "\n",
    "x_range_c = np.linspace(-1, 5, 100)\n",
    "y_range_c = np.linspace(-1, 5, 100)\n",
    "X_c, Y_c = np.meshgrid(x_range_c, y_range_c)\n",
    "Z_c = X_c**2 + Y_c**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.contour(X_c, Y_c, Z_c, levels=20, cmap='viridis')\n",
    "ax.plot(x_range_c, 4 - x_range_c, 'r-', linewidth=2, label='Contrainte: x+y=4')\n",
    "ax.plot(float(x_opt), float(y_opt), 'r*', markersize=20, label=f'Optimal: ({float(x_opt)}, {float(y_opt)})')\n",
    "ax.set_title('Minimisation avec Contrainte')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(-1, 5)\n",
    "ax.set_ylim(-1, 5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphique sauvegard√©\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.5: Comparaison des M√©thodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('### Exercice 7.5: Comparaison des M√©thodes d\\'Optimisation ###\\n')\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "f_opt = (x - 2)**2 + (y + 1)**2\n",
    "print('Fonction: f(x,y) = (x-2)¬≤ + (y+1)¬≤')\n",
    "print('Optimum: (2, -1)\\n')\n",
    "\n",
    "# M√©thode 1: Analytique\n",
    "df_dx_opt = sp.diff(f_opt, x)\n",
    "df_dy_opt = sp.diff(f_opt, y)\n",
    "critical_opt = sp.solve([df_dx_opt, df_dy_opt], [x, y])\n",
    "x_a, y_a = float(critical_opt[x]), float(critical_opt[y])\n",
    "f_a = float(f_opt.subs(critical_opt))\n",
    "print(f'M√©thode 1 - Analytique: ({x_a:.4f}, {y_a:.4f}), f={f_a:.6f}')\n",
    "\n",
    "# M√©thode 2: Gradient Descent\n",
    "def f_num(xy):\n",
    "    return (xy[0]-2)**2 + (xy[1]+1)**2\n",
    "\n",
    "def grad(xy):\n",
    "    return np.array([2*(xy[0]-2), 2*(xy[1]+1)])\n",
    "\n",
    "xy_gd = np.array([0.0, 0.0])\n",
    "for _ in range(50):\n",
    "    xy_gd = xy_gd - 0.1 * grad(xy_gd)\n",
    "\n",
    "print(f'M√©thode 2 - Gradient Descent: ({xy_gd[0]:.4f}, {xy_gd[1]:.4f}), f={f_num(xy_gd):.6f}')\n",
    "\n",
    "# M√©thode 3: SciPy\n",
    "result = minimize(f_num, [0.0, 0.0], method='BFGS')\n",
    "print(f'M√©thode 3 - SciPy BFGS: ({result.x[0]:.4f}, {result.x[1]:.4f}), f={result.fun:.6f}')\n",
    "print(f'\\nIt√©rations SciPy: {result.nit}, √âvaluations: {result.nfev}\\n')\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "losses_gd = [f_num(np.array([0.0, 0.0]))]\n",
    "xy = np.array([0.0, 0.0])\n",
    "for _ in range(50):\n",
    "    xy = xy - 0.1 * grad(xy)\n",
    "    losses_gd.append(f_num(xy))\n",
    "\n",
    "axes[0].semilogy(losses_gd, 'b-o', linewidth=2, markersize=4)\n",
    "axes[0].set_title('Convergence Gradient Descent')\n",
    "axes[0].set_ylabel('Loss f(x,y)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "x_range_comp = np.linspace(-2, 4, 50)\n",
    "y_range_comp = np.linspace(-4, 2, 50)\n",
    "X_comp, Y_comp = np.meshgrid(x_range_comp, y_range_comp)\n",
    "Z_comp = (X_comp - 2)**2 + (Y_comp + 1)**2\n",
    "axes[1].contour(X_comp, Y_comp, Z_comp, levels=20, cmap='viridis')\n",
    "axes[1].plot(2, -1, 'r*', markersize=20, label='Optimum')\n",
    "axes[1].plot(0, 0, 'go', markersize=8, label='D√©part')\n",
    "axes[1].set_title('Contours et Point Optimal')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('‚úì Graphiques sauvegard√©s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ F√©licitations !\n",
    "\n",
    "**Solutions compl√®tes des sections 6 et 7!**\n",
    "\n",
    "### üí° Points cl√©s\n",
    "\n",
    "**Section 6 - R√®gle de la Cha√Æne:**\n",
    "- (f‚àòg)'(x) = f'(g(x)) √ó g'(x)\n",
    "- Backpropagation = chain rule appliqu√©e\n",
    "- ReLU: gradient nul pour z < 0 (dying ReLU)\n",
    "\n",
    "**Section 7 - Optimisation:**\n",
    "- Points critiques: ‚àáf = 0\n",
    "- Hessienne pour classifier (min/max/selle)\n",
    "- Gradient Descent vs Analytique vs SciPy\n",
    "\n",
    "**Excellent travail! üí™**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}