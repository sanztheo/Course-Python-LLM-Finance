{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcda SOLUTIONS - Calcul Diff\u00e9rentiel\n\n**Solutions compl\u00e8tes pour Sections 4 et 5** \ud83c\udfaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : D\u00e9riv\u00e9es Partielles - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.1 : D\u00e9riv\u00e9es Partielles Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx, y, z = sp.symbols('x y z')\nprint('SOLUTION 4.1: D\u00e9riv\u00e9es Partielles Simples')\nprint('='*70)\n\nf = x**2 * y + 3*x*y**2\ndf_dx = sp.diff(f, x)\ndf_dy = sp.diff(f, y)\n\nprint(f'f(x,y) = {f}')\nprint(f'\u2202f/\u2202x = {df_dx}')\nprint(f'\u2202f/\u2202y = {df_dy}')\n\nval_x = df_dx.subs([(x, 2), (y, 3)])\nval_y = df_dy.subs([(x, 2), (y, 3)])\nprint(f'\\n\u2202f/\u2202x|_(2,3) = {val_x}')\nprint(f'\u2202f/\u2202y|_(2,3) = {val_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.2 : Fonction \u00e0 Trois Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.2: Fonction \u00e0 Trois Variables')\nprint('='*70)\n\nf = x**2 + y**2 + z**2 + x*y + y*z\ndf_dx = sp.diff(f, x)\ndf_dy = sp.diff(f, y)\ndf_dz = sp.diff(f, z)\n\nprint(f'f(x,y,z) = {f}')\nprint(f'\u2202f/\u2202x = {df_dx}')\nprint(f'\u2202f/\u2202y = {df_dy}')\nprint(f'\u2202f/\u2202z = {df_dz}')\nprint(f'\\n\u2207f(1,1,1) = [{df_dx.subs({x:1, y:1, z:1})}, {df_dy.subs({x:1, y:1, z:1})}, {df_dz.subs({x:1, y:1, z:1})}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.3 : D\u00e9riv\u00e9es Partielles d'Ordre 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.3: D\u00e9riv\u00e9es Partielles d\\'Ordre 2')\nprint('='*70)\n\nf = x**3 * y**2\nf_xx = sp.diff(f, x, 2)\nf_yy = sp.diff(f, y, 2)\nf_xy = sp.diff(f, x, y)\nf_yx = sp.diff(f, y, x)\n\nprint(f'f(x,y) = {f}')\nprint(f'\u2202\u00b2f/\u2202x\u00b2 = {f_xx}')\nprint(f'\u2202\u00b2f/\u2202y\u00b2 = {f_yy}')\nprint(f'\u2202\u00b2f/\u2202x\u2202y = {f_xy}')\nprint(f'\u2202\u00b2f/\u2202y\u2202x = {f_yx}')\nprint(f'Th\u00e9or\u00e8me Schwarz: {sp.simplify(f_xy - f_yx) == 0}')  # True si \u00e9gales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.4 : Visualisation de Surface 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.4: Visualisation de Surface 3D')\nprint('='*70)\n\nfig = plt.figure(figsize=(15, 5))\nax1 = fig.add_subplot(131, projection='3d')\n\nx_vals = np.linspace(-3, 3, 100)\ny_vals = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x_vals, y_vals)\nZ = X**2 - Y**2\n\nsurf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('Surface: f(x,y) = x\u00b2 - y\u00b2')\n\nax2 = fig.add_subplot(132)\ncontour = ax2.contour(X, Y, Z, levels=15, cmap='viridis')\nfor px, py in [(1, 1), (-1, 1), (1, -1)]:\n    ax2.quiver(px, py, 2*px, -2*py, scale=15, color='red')\nax2.set_title('Contours + Gradients')\n\nax3 = fig.add_subplot(133)\nax3.text(0.5, 0.5, 'En (1,1):\\n\u2202f/\u2202x = 2\\n\u2202f/\u2202y = -2', ha='center', fontsize=12)\nax3.axis('off')\n\nplt.tight_layout()\nplt.show()\nprint('Surface visualis\u00e9e \u2713')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.5 : Application ML - Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 4.5: Application ML - Loss Function')\nprint('='*70)\n\nw, b = sp.symbols('w b')\nx_data, y_data = 2, 5\n\nL = (y_data - (w*x_data + b))**2\ndL_dw = sp.diff(L, w)\ndL_db = sp.diff(L, b)\n\nprint(f'Donn\u00e9es: x={x_data}, y={y_data}')\nprint(f'L = {sp.expand(L)}')\nprint(f'\u2202L/\u2202w = {dL_dw}')\nprint(f'\u2202L/\u2202b = {dL_db}')\n\ngrad_w = dL_dw.subs([(w, 1), (b, 0)])\ngrad_b = dL_db.subs([(w, 1), (b, 0)])\nprint(f'\\nEn (w=1, b=0):')\nprint(f'\u2202L/\u2202w = {grad_w}')\nprint(f'\u2202L/\u2202b = {grad_b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 : Gradient - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.1 : Calculer le Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.1: Calculer le Gradient')\nprint('='*70)\n\nf1 = x**2 + y**2\ngrad_f1 = [sp.diff(f1, x), sp.diff(f1, y)]\nprint(f'1. f(x,y) = {f1} \u2192 \u2207f = {grad_f1}')\n\nf2 = x*y\ngrad_f2 = [sp.diff(f2, x), sp.diff(f2, y)]\nprint(f'2. g(x,y) = {f2} \u2192 \u2207g = {grad_f2}')\n\nf3 = x**2 + 2*y**2 + 3*z**2\ngrad_f3 = [sp.diff(f3, x), sp.diff(f3, y), sp.diff(f3, z)]\nprint(f'3. h(x,y,z) = {f3} \u2192 \u2207h = {grad_f3}')\n\nf4 = sp.exp(x + y)\ngrad_f4 = [sp.diff(f4, x), sp.diff(f4, y)]\nprint(f'4. k(x,y) = {f4} \u2192 \u2207k = {grad_f4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.2 : Norme et Direction du Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.2: Norme et Direction du Gradient')\nprint('='*70)\n\nf = x**2 + 2*y**2\ngrad_x = sp.diff(f, x)\ngrad_y = sp.diff(f, y)\n\ngrad_val = np.array([2.0, 4.0])  # En (1,1)\nnorm = np.linalg.norm(grad_val)\n\nprint(f'f(x,y) = {f}')\nprint(f'En (1,1): \u2207f = {grad_val}')\nprint(f'||\u2207f|| = {norm:.4f}')\nprint(f'Direction normalis\u00e9e: {grad_val/norm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.3 : Gradient Descent 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.3: Gradient Descent 2D')\nprint('='*70)\n\ndef f(x_val, y_val):\n    return (x_val - 3)**2 + (y_val + 2)**2\n\ndef grad(x_val, y_val):\n    return np.array([2*(x_val - 3), 2*(y_val + 2)])\n\nx_pos, y_pos = 0.0, 0.0\nalpha, iterations = 0.1, 30\nlosses = []\n\nfor i in range(iterations):\n    g = grad(x_pos, y_pos)\n    x_pos -= alpha * g[0]\n    y_pos -= alpha * g[1]\n    losses.append(f(x_pos, y_pos))\n\nprint(f'Position initiale: (0, 0)')\nprint(f'Position finale: ({x_pos:.4f}, {y_pos:.4f})')\nprint(f'Minimum attendu: (3, -2)')\nprint(f'Loss finale: {losses[-1]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.4 : Champ de Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.4: Champ de Gradients')\nprint('='*70)\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nx_range = np.linspace(-3, 3, 20)\ny_range = np.linspace(-3, 3, 20)\nX, Y = np.meshgrid(x_range, y_range)\nZ = 0.5 * (X**2 + Y**2)\n\ncontour = ax.contour(X, Y, Z, levels=15, alpha=0.6)\nquiver = ax.quiver(X, Y, X, Y, np.sqrt(X**2 + Y**2), cmap='hot', scale=30)\nax.set_title('Champ de gradients: f(x,y) = 0.5(x\u00b2 + y\u00b2)')\nax.axis('equal')\nplt.show()\nprint('Champ de gradients visualis\u00e9 \u2713')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.5 : D\u00e9riv\u00e9es Directionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.5: D\u00e9riv\u00e9es Directionnelles')\nprint('='*70)\n\nf = x**2 + x*y + y**2\ndf_dx = sp.diff(f, x)\ndf_dy = sp.diff(f, y)\n\ngrad = np.array([4.0, 5.0])  # En (1,2)\n\nprint(f'f(x,y) = {f}')\nprint(f'En (1,2): \u2207f = {grad}')\n\ndirections = {\n    'axe x': np.array([1, 0]),\n    'axe y': np.array([0, 1]),\n    'diagonale': np.array([1, 1])/np.sqrt(2)\n}\n\nfor name, u in directions.items():\n    d_u = np.dot(grad, u/np.linalg.norm(u))\n    print(f'{name}: D_u f = {d_u:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.6 : R\u00e9gression Lin\u00e9aire par Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nSOLUTION 5.6: R\u00e9gression Lin\u00e9aire par Gradient Descent')\nprint('='*70)\n\nx_data = np.array([1, 2, 3])\ny_data = np.array([2, 4, 5])\nn = len(x_data)\n\ndef loss(w, b):\n    return np.mean((y_data - (w*x_data + b))**2)\n\ndef grad(w, b):\n    y_pred = w*x_data + b\n    residuals = y_data - y_pred\n    return -2*np.mean(residuals*x_data), -2*np.mean(residuals)\n\nw, b = 0.0, 0.0\nalpha = 0.01\n\nfor i in range(100):\n    dw, db = grad(w, b)\n    w -= alpha * dw\n    b -= alpha * db\n\nprint(f'Gradient Descent: w={w:.4f}, b={b:.4f}')\n\n# Solution analytique\nx_mean = np.mean(x_data)\ny_mean = np.mean(y_data)\nw_a = np.sum((x_data - x_mean)*(y_data - y_mean)) / np.sum((x_data - x_mean)**2)\nb_a = y_mean - w_a * x_mean\n\nprint(f'Analytique: w={w_a:.4f}, b={b_a:.4f}')\nprint(f'\\n\u2713 Les deux convergen vers la m\u00eame solution!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "## \ud83c\udf89 F\u00e9licitations!\n\n",
    "**Solutions Sections 4-5 compl\u00e8tes!**\n\n",
    "### Contenu\n\n",
    "- \u2713 Ex 4.1: D\u00e9riv\u00e9es partielles simples\n",
    "- \u2713 Ex 4.2: Trois variables + gradient\n",
    "- \u2713 Ex 4.3: D\u00e9riv\u00e9es d'ordre 2\n",
    "- \u2713 Ex 4.4: Visualisation 3D\n",
    "- \u2713 Ex 4.5: Loss function ML\n",
    "- \u2713 Ex 5.1: Calculer gradients\n",
    "- \u2713 Ex 5.2: Norme et direction\n",
    "- \u2713 Ex 5.3: Gradient Descent 2D\n",
    "- \u2713 Ex 5.4: Champ de vecteurs\n",
    "- \u2713 Ex 5.5: D\u00e9riv\u00e9es directionnelles\n",
    "- \u2713 Ex 5.6: R\u00e9gression lin\u00e9aire\n\n",
    "**Excellent travail! \ud83d\udcaa**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}