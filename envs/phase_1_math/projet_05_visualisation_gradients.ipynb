{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® PROJET : Visualisation de Gradients et Gradient Descent\n\n",
    "**Un projet pratique pour comprendre le calcul diff√©rentiel en action !** üöÄ\n\n",
    "---\n\n",
    "## üéØ Objectifs du Projet\n\n",
    "Dans ce projet, vous allez :\n\n",
    "1. **Visualiser des gradients** sur des fonctions 2D\n",
    "2. **Impl√©menter Gradient Descent** from scratch\n",
    "3. **Cr√©er des surfaces 3D** avec vecteurs de gradient\n",
    "4. **Animer l'optimisation** en temps r√©el\n",
    "5. **Appliquer au Machine Learning** (r√©gression lin√©aire)\n\n",
    "### ü§ñ Pourquoi ce projet est important ?\n\n",
    "C'est **exactement** ce qui se passe dans l'entra√Ænement d'un r√©seau de neurones !\n",
    "- Gradient Descent = algorithme d'optimisation\n",
    "- Visualisation = comprendre le paysage de la loss\n",
    "- Animation = voir l'apprentissage en action\n\n",
    "---\n\n",
    "## üìö Pr√©requis\n\n",
    "Assurez-vous d'avoir compl√©t√© :\n",
    "- ‚úÖ Cours : Maths_05_Calcul_Differentiel.ipynb\n",
    "- ‚úÖ Exercices : exercices_05_calcul_differentiel.ipynb\n\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import sympy as sp\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis ! Pr√™t √† visualiser des gradients !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Partie 1 : Visualisation de Gradients 2D\n\n",
    "Commen√ßons par visualiser le gradient sur une fonction simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Fonction Quadratique Simple : $f(x, y) = x^2 + y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir la fonction et son gradient\n",
    "def f_quadratic(x, y):\n",
    "    \"\"\"Fonction parabolo√Øde: f(x,y) = x¬≤ + y¬≤\"\"\"\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradient_f_quadratic(x, y):\n",
    "    \"\"\"Gradient de f: ‚àáf = (2x, 2y)\"\"\"\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "# Cr√©er une grille\n",
    "x_range = np.linspace(-3, 3, 20)\n",
    "y_range = np.linspace(-3, 3, 20)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f_quadratic(X, Y)\n",
    "\n",
    "# Calculer les gradients sur la grille\n",
    "U = 2 * X  # ‚àÇf/‚àÇx\n",
    "V = 2 * Y  # ‚àÇf/‚àÇy\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Contours avec gradients\n",
    "contour = ax1.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "ax1.quiver(X, Y, U, V, color='red', alpha=0.6, scale=50)\n",
    "ax1.set_xlabel('x', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_title('f(x,y) = x¬≤ + y¬≤ - Contours et Gradients', fontweight='bold', fontsize=14)\n",
    "ax1.plot(0, 0, 'r*', markersize=20, label='Minimum (0, 0)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axis('equal')\n",
    "\n",
    "# Heatmap avec gradients\n",
    "im = ax2.imshow(Z, extent=[-3, 3, -3, 3], origin='lower', cmap='hot', aspect='auto')\n",
    "ax2.quiver(X, Y, -U, -V, color='cyan', alpha=0.8, scale=50, label='Direction descente (-‚àáf)')\n",
    "ax2.set_xlabel('x', fontsize=12)\n",
    "ax2.set_ylabel('y', fontsize=12)\n",
    "ax2.set_title('Direction de Descente (Gradient N√©gatif)', fontweight='bold', fontsize=14)\n",
    "ax2.plot(0, 0, 'c*', markersize=20, label='Minimum')\n",
    "ax2.legend()\n",
    "plt.colorbar(im, ax=ax2, label='f(x, y)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìå Observation : Les gradients pointent VERS L'EXT√âRIEUR (mont√©e)\")\n",
    "print(\"üìå Pour descendre, on suit -‚àáf (gradient n√©gatif)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Fonction Plus Complexe : Fonction de Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de Rosenbrock (difficile √† optimiser !)\n",
    "def rosenbrock(x, y):\n",
    "    \"\"\"f(x,y) = (1-x)¬≤ + 100(y-x¬≤)¬≤\"\"\"\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def gradient_rosenbrock(x, y):\n",
    "    \"\"\"Gradient de Rosenbrock\"\"\"\n",
    "    df_dx = -2*(1-x) - 400*x*(y - x**2)\n",
    "    df_dy = 200*(y - x**2)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Grille\n",
    "x_range = np.linspace(-2, 2, 40)\n",
    "y_range = np.linspace(-1, 3, 40)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Contours avec √©chelle log pour mieux voir\n",
    "contour = plt.contour(X, Y, np.log(Z + 1), levels=20, cmap='twilight')\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "plt.contourf(X, Y, np.log(Z + 1), levels=20, cmap='twilight', alpha=0.3)\n",
    "\n",
    "# Gradients en quelques points\n",
    "step = 5\n",
    "for i in range(0, len(x_range), step):\n",
    "    for j in range(0, len(y_range), step):\n",
    "        x_pt, y_pt = X[j, i], Y[j, i]\n",
    "        grad = gradient_rosenbrock(x_pt, y_pt)\n",
    "        # Normaliser pour visualisation\n",
    "        if np.linalg.norm(grad) > 0:\n",
    "            grad_norm = -grad / (np.linalg.norm(grad) + 1) * 0.15\n",
    "            plt.arrow(x_pt, y_pt, grad_norm[0], grad_norm[1], \n",
    "                     head_width=0.05, head_length=0.05, fc='yellow', ec='orange', alpha=0.7)\n",
    "\n",
    "plt.plot(1, 1, 'r*', markersize=25, label='Minimum global (1, 1)')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Fonction de Rosenbrock - Paysage d\\'Optimisation Difficile', fontweight='bold', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.colorbar(label='log(f(x,y) + 1)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üî• La fonction de Rosenbrock est c√©l√®bre pour √™tre difficile √† optimiser !\")\n",
    "print(\"   Vall√©e √©troite en forme de banane menant au minimum.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üé¢ Partie 2 : Surfaces 3D avec Vecteurs de Gradient\n\n",
    "Visualisons les fonctions en 3D pour mieux comprendre le paysage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface 3D avec contours\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Fonction quadratique\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "x_range = np.linspace(-3, 3, 50)\n",
    "y_range = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f_quadratic(X, Y)\n",
    "\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x, y)')\n",
    "ax1.set_title('f(x,y) = x¬≤ + y¬≤ (Parabolo√Øde)', fontweight='bold')\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Fonction avec point-selle\n",
    "ax2 = fig.add_subplot(222, projection='3d')\n",
    "Z_saddle = X**2 - Y**2\n",
    "surf2 = ax2.plot_surface(X, Y, Z_saddle, cmap='coolwarm', alpha=0.8, edgecolor='none')\n",
    "ax2.plot([0], [0], [0], 'ro', markersize=10, label='Point-selle')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('f(x, y)')\n",
    "ax2.set_title('f(x,y) = x¬≤ - y¬≤ (Point-Selle)', fontweight='bold')\n",
    "fig.colorbar(surf2, ax=ax2, shrink=0.5)\n",
    "ax2.legend()\n",
    "\n",
    "# Fonction de Himmelblau (4 minima locaux !)\n",
    "def himmelblau(x, y):\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "ax3 = fig.add_subplot(223, projection='3d')\n",
    "x_range = np.linspace(-5, 5, 60)\n",
    "y_range = np.linspace(-5, 5, 60)\n",
    "X_h, Y_h = np.meshgrid(x_range, y_range)\n",
    "Z_h = himmelblau(X_h, Y_h)\n",
    "\n",
    "surf3 = ax3.plot_surface(X_h, Y_h, np.log(Z_h + 1), cmap='plasma', alpha=0.8, edgecolor='none')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_zlabel('log(f + 1)')\n",
    "ax3.set_title('Himmelblau (4 Minima Locaux)', fontweight='bold')\n",
    "fig.colorbar(surf3, ax=ax3, shrink=0.5)\n",
    "\n",
    "# Rosenbrock en 3D\n",
    "ax4 = fig.add_subplot(224, projection='3d')\n",
    "x_range = np.linspace(-2, 2, 50)\n",
    "y_range = np.linspace(-1, 3, 50)\n",
    "X_r, Y_r = np.meshgrid(x_range, y_range)\n",
    "Z_r = rosenbrock(X_r, Y_r)\n",
    "\n",
    "surf4 = ax4.plot_surface(X_r, Y_r, np.log(Z_r + 1), cmap='hot', alpha=0.8, edgecolor='none')\n",
    "ax4.plot([1], [1], [0], 'c*', markersize=15, label='Minimum (1,1)')\n",
    "ax4.set_xlabel('x')\n",
    "ax4.set_ylabel('y')\n",
    "ax4.set_zlabel('log(f + 1)')\n",
    "ax4.set_title('Rosenbrock (Vall√©e Banane)', fontweight='bold')\n",
    "fig.colorbar(surf4, ax=ax4, shrink=0.5)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üèîÔ∏è Diff√©rents paysages d'optimisation :\")\n",
    "print(\"   - Parabolo√Øde : 1 minimum global (facile)\")\n",
    "print(\"   - Point-selle : ni min ni max (pi√®ge !)\")\n",
    "print(\"   - Himmelblau : 4 minima locaux (complexe)\")\n",
    "print(\"   - Rosenbrock : vall√©e √©troite (difficile)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ö° Partie 3 : Impl√©mentation de Gradient Descent\n\n",
    "Impl√©mentons l'algorithme from scratch !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Gradient Descent r√©utilisable\n",
    "class GradientDescent:\n",
    "    def __init__(self, f, grad_f, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Gradient Descent Optimizer\n",
    "        \n",
    "        Args:\n",
    "            f: fonction √† minimiser\n",
    "            grad_f: gradient de la fonction\n",
    "            learning_rate: taux d'apprentissage (alpha)\n",
    "            max_iterations: nombre max d'it√©rations\n",
    "            tolerance: crit√®re d'arr√™t (gradient proche de 0)\n",
    "        \"\"\"\n",
    "        self.f = f\n",
    "        self.grad_f = grad_f\n",
    "        self.lr = learning_rate\n",
    "        self.max_iter = max_iterations\n",
    "        self.tol = tolerance\n",
    "        self.history = []\n",
    "        \n",
    "    def optimize(self, x0, verbose=True):\n",
    "        \"\"\"\n",
    "        Ex√©cute l'optimisation\n",
    "        \n",
    "        Args:\n",
    "            x0: point de d√©part (array)\n",
    "            verbose: afficher les logs\n",
    "        \n",
    "        Returns:\n",
    "            x_opt: point optimal trouv√©\n",
    "        \"\"\"\n",
    "        x = np.array(x0, dtype=float)\n",
    "        self.history = [x.copy()]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{'Iter':<8} {'x':<20} {'f(x)':<15} {'||‚àáf||'}\")\n",
    "            print(\"=\" * 65)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            # Calculer gradient\n",
    "            if len(x) == 1:\n",
    "                grad = np.array([self.grad_f(x[0])])\n",
    "            elif len(x) == 2:\n",
    "                grad = self.grad_f(x[0], x[1])\n",
    "            else:\n",
    "                grad = self.grad_f(*x)\n",
    "            \n",
    "            # Mise √† jour\n",
    "            x = x - self.lr * grad\n",
    "            self.history.append(x.copy())\n",
    "            \n",
    "            # Logs\n",
    "            if verbose and i % (self.max_iter // 10) == 0:\n",
    "                if len(x) == 1:\n",
    "                    f_val = self.f(x[0])\n",
    "                elif len(x) == 2:\n",
    "                    f_val = self.f(x[0], x[1])\n",
    "                else:\n",
    "                    f_val = self.f(*x)\n",
    "                grad_norm = np.linalg.norm(grad)\n",
    "                print(f\"{i:<8} {str(x):<20} {f_val:<15.6f} {grad_norm:.6f}\")\n",
    "            \n",
    "            # Crit√®re d'arr√™t\n",
    "            if np.linalg.norm(grad) < self.tol:\n",
    "                if verbose:\n",
    "                    print(f\"\\n‚úÖ Convergence atteinte √† l'it√©ration {i}\")\n",
    "                break\n",
    "        \n",
    "        self.history = np.array(self.history)\n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ Classe GradientDescent cr√©√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Test sur Fonction Quadratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Gradient Descent\n",
    "print(\"üéØ Minimisation de f(x, y) = (x-3)¬≤ + (y+2)¬≤\")\n",
    "print(\"   Minimum th√©orique: (3, -2)\\n\")\n",
    "\n",
    "# Fonction √† minimiser\n",
    "def f_test(x, y):\n",
    "    return (x - 3)**2 + (y + 2)**2\n",
    "\n",
    "def grad_f_test(x, y):\n",
    "    return np.array([2*(x - 3), 2*(y + 2)])\n",
    "\n",
    "# Optimisation\n",
    "gd = GradientDescent(f_test, grad_f_test, learning_rate=0.1, max_iterations=100)\n",
    "x_opt = gd.optimize([0, 0])\n",
    "\n",
    "print(f\"\\nüìç Point optimal trouv√©: ({x_opt[0]:.6f}, {x_opt[1]:.6f})\")\n",
    "print(f\"üìç Minimum th√©orique:    (3.000000, -2.000000)\")\n",
    "print(f\"üìç Erreur: {np.linalg.norm(x_opt - np.array([3, -2])):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualisation de la Trajectoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la trajectoire\n",
    "x_range = np.linspace(-1, 5, 100)\n",
    "y_range = np.linspace(-4, 1, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f_test(X, Y)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Vue contours\n",
    "plt.subplot(1, 2, 1)\n",
    "contour = plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Trajectoire\n",
    "history = gd.history\n",
    "plt.plot(history[:, 0], history[:, 1], 'ro-', linewidth=2, markersize=5, alpha=0.7, label='Trajectoire')\n",
    "plt.plot(history[0, 0], history[0, 1], 'go', markersize=12, label='D√©part')\n",
    "plt.plot(history[-1, 0], history[-1, 1], 'r*', markersize=20, label='Arriv√©e')\n",
    "plt.plot(3, -2, 'ks', markersize=10, label='Minimum th√©orique')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Trajectoire de Gradient Descent', fontweight='bold', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Vue 3D\n",
    "ax = plt.subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6, edgecolor='none')\n",
    "\n",
    "# Trajectoire 3D\n",
    "z_history = [f_test(pt[0], pt[1]) for pt in history]\n",
    "ax.plot(history[:, 0], history[:, 1], z_history, 'r-', linewidth=3, label='Trajectoire')\n",
    "ax.plot([history[0, 0]], [history[0, 1]], [z_history[0]], 'go', markersize=10, label='D√©part')\n",
    "ax.plot([history[-1, 0]], [history[-1, 1]], [z_history[-1]], 'r*', markersize=15, label='Arriv√©e')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('f(x, y)')\n",
    "ax.set_title('Descente 3D', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéØ Nombre d'it√©rations: {len(history)}\")\n",
    "print(f\"üéØ Distance initiale au minimum: {np.linalg.norm(history[0] - np.array([3, -2])):.4f}\")\n",
    "print(f\"üéØ Distance finale au minimum: {np.linalg.norm(history[-1] - np.array([3, -2])):.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üé¨ Partie 4 : Animation de Gradient Descent\n\n",
    "Animons l'optimisation en temps r√©el !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation de Gradient Descent\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Fonction et gradient\n",
    "def f_anim(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def grad_f_anim(x, y):\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "# Optimiser\n",
    "gd_anim = GradientDescent(f_anim, grad_f_anim, learning_rate=0.15, max_iterations=50)\n",
    "x_opt_anim = gd_anim.optimize([3, 2], verbose=False)\n",
    "\n",
    "# Pr√©parer l'animation\n",
    "history_anim = gd_anim.history\n",
    "\n",
    "# Grille\n",
    "x_range = np.linspace(-4, 4, 100)\n",
    "y_range = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f_anim(X, Y)\n",
    "\n",
    "# Configuration figure\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contour = ax.contour(X, Y, Z, levels=15, cmap='viridis', alpha=0.6)\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Animation Gradient Descent', fontweight='bold', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# √âl√©ments anim√©s\n",
    "line, = ax.plot([], [], 'ro-', linewidth=2, markersize=5, label='Trajectoire')\n",
    "point, = ax.plot([], [], 'r*', markersize=20, label='Position actuelle')\n",
    "ax.plot(0, 0, 'ks', markersize=12, label='Minimum (0, 0)')\n",
    "ax.legend()\n",
    "\n",
    "text = ax.text(0.02, 0.95, '', transform=ax.transAxes, fontsize=11,\n",
    "              verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    point.set_data([], [])\n",
    "    text.set_text('')\n",
    "    return line, point, text\n",
    "\n",
    "def animate(i):\n",
    "    # Trajectoire jusqu'√† i\n",
    "    line.set_data(history_anim[:i+1, 0], history_anim[:i+1, 1])\n",
    "    point.set_data([history_anim[i, 0]], [history_anim[i, 1]])\n",
    "    \n",
    "    # Info\n",
    "    x_cur, y_cur = history_anim[i]\n",
    "    f_cur = f_anim(x_cur, y_cur)\n",
    "    grad_cur = grad_f_anim(x_cur, y_cur)\n",
    "    grad_norm = np.linalg.norm(grad_cur)\n",
    "    \n",
    "    text.set_text(f'It√©ration: {i}\\nx = ({x_cur:.4f}, {y_cur:.4f})\\nf(x) = {f_cur:.6f}\\n||‚àáf|| = {grad_norm:.6f}')\n",
    "    \n",
    "    return line, point, text\n",
    "\n",
    "anim = FuncAnimation(fig, animate, init_func=init, frames=len(history_anim), \n",
    "                    interval=200, blit=True, repeat=True)\n",
    "\n",
    "plt.close()  # Ne pas afficher la figure statique\n",
    "\n",
    "# Afficher l'animation\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ Partie 5 : Application ML - R√©gression Lin√©aire\n\n",
    "Utilisons Gradient Descent pour entra√Æner un mod√®le de r√©gression lin√©aire !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 G√©n√©ration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es synth√©tiques : y = 2x + 3 + bruit\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "# Vraie relation : y = 2x + 3\n",
    "X_data = np.random.uniform(0, 10, n_samples)\n",
    "y_data = 2 * X_data + 3 + np.random.normal(0, 2, n_samples)  # Bruit\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_data, y_data, alpha=0.6, s=50, label='Donn√©es')\n",
    "plt.plot([0, 10], [3, 23], 'r--', linewidth=2, label='Vraie relation: y = 2x + 3')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Donn√©es de R√©gression Lin√©aire', fontweight='bold', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ {n_samples} points g√©n√©r√©s\")\n",
    "print(f\"üìä Vraie relation: y = 2x + 3\")\n",
    "print(f\"üéØ Objectif: retrouver w=2 et b=3 avec Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Impl√©mentation de la Loss et du Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: Mean Squared Error (MSE)\n",
    "def mse_loss(w, b, X, y):\n",
    "    \"\"\"\n",
    "    MSE = (1/n) * Œ£(y_i - (w*x_i + b))¬≤\n",
    "    \"\"\"\n",
    "    predictions = w * X + b\n",
    "    errors = y - predictions\n",
    "    return np.mean(errors**2)\n",
    "\n",
    "def gradient_mse(w, b, X, y):\n",
    "    \"\"\"\n",
    "    ‚àÇL/‚àÇw = -(2/n) * Œ£(y_i - (w*x_i + b)) * x_i\n",
    "    ‚àÇL/‚àÇb = -(2/n) * Œ£(y_i - (w*x_i + b))\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    predictions = w * X + b\n",
    "    errors = y - predictions\n",
    "    \n",
    "    dw = -(2/n) * np.sum(errors * X)\n",
    "    db = -(2/n) * np.sum(errors)\n",
    "    \n",
    "    return np.array([dw, db])\n",
    "\n",
    "# Wrapper pour utiliser GradientDescent\n",
    "def loss_wrapper(params):\n",
    "    return mse_loss(params[0], params[1], X_data, y_data)\n",
    "\n",
    "def grad_wrapper(w, b):\n",
    "    return gradient_mse(w, b, X_data, y_data)\n",
    "\n",
    "print(\"‚úÖ Loss et gradient d√©finis\")\n",
    "print(\"   Loss: MSE = (1/n) * Œ£(y_i - (w*x_i + b))¬≤\")\n",
    "print(\"   Gradients calcul√©s analytiquement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Entra√Ænement avec Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement\n",
    "print(\"üöÄ Entra√Ænement du mod√®le avec Gradient Descent\\n\")\n",
    "\n",
    "gd_ml = GradientDescent(loss_wrapper, grad_wrapper, learning_rate=0.01, max_iterations=500)\n",
    "params_opt = gd_ml.optimize([0, 0])  # Partir de w=0, b=0\n",
    "\n",
    "w_final, b_final = params_opt\n",
    "\n",
    "print(f\"\\nüìä R√©sultats:\")\n",
    "print(f\"   Param√®tres trouv√©s: w = {w_final:.6f}, b = {b_final:.6f}\")\n",
    "print(f\"   Vrais param√®tres:   w = 2.000000, b = 3.000000\")\n",
    "print(f\"   Erreur w: {abs(w_final - 2):.6f}\")\n",
    "print(f\"   Erreur b: {abs(b_final - 3):.6f}\")\n",
    "print(f\"\\n   Loss finale: {mse_loss(w_final, b_final, X_data, y_data):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualisation des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation finale\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Donn√©es et pr√©dictions\n",
    "ax1.scatter(X_data, y_data, alpha=0.6, s=50, label='Donn√©es', color='blue')\n",
    "ax1.plot([0, 10], [3, 23], 'r--', linewidth=2, label='Vraie: y = 2x + 3', alpha=0.7)\n",
    "\n",
    "# Mod√®le appris\n",
    "x_line = np.array([0, 10])\n",
    "y_pred_line = w_final * x_line + b_final\n",
    "ax1.plot(x_line, y_pred_line, 'g-', linewidth=3, \n",
    "         label=f'Apprise: y = {w_final:.2f}x + {b_final:.2f}')\n",
    "\n",
    "ax1.set_xlabel('x', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_title('R√©gression Lin√©aire - R√©sultat', fontweight='bold', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# √âvolution de la loss\n",
    "history_params = gd_ml.history\n",
    "loss_history = [mse_loss(p[0], p[1], X_data, y_data) for p in history_params]\n",
    "\n",
    "ax2.plot(loss_history, linewidth=2, color='red')\n",
    "ax2.set_xlabel('It√©ration', fontsize=12)\n",
    "ax2.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax2.set_title('√âvolution de la Loss pendant l\\'Entra√Ænement', fontweight='bold', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Mod√®le entra√Æn√© avec succ√®s !\")\n",
    "print(\"üìâ La loss d√©cro√Æt exponentiellement (√©chelle log)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Paysage de la Loss en 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation 3D du paysage de la loss\n",
    "w_range = np.linspace(0, 4, 50)\n",
    "b_range = np.linspace(0, 6, 50)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "L = np.zeros_like(W)\n",
    "\n",
    "for i in range(len(w_range)):\n",
    "    for j in range(len(b_range)):\n",
    "        L[j, i] = mse_loss(W[j, i], B[j, i], X_data, y_data)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Surface 3D\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(W, B, L, cmap='hot', alpha=0.7, edgecolor='none')\n",
    "\n",
    "# Trajectoire d'optimisation\n",
    "w_history = history_params[:, 0]\n",
    "b_history = history_params[:, 1]\n",
    "l_history = np.array(loss_history)\n",
    "\n",
    "ax1.plot(w_history, b_history, l_history, 'g-', linewidth=3, label='Trajectoire GD')\n",
    "ax1.plot([w_history[0]], [b_history[0]], [l_history[0]], 'go', markersize=10, label='D√©part')\n",
    "ax1.plot([w_history[-1]], [b_history[-1]], [l_history[-1]], 'r*', markersize=15, label='Arriv√©e')\n",
    "ax1.plot([2], [3], [mse_loss(2, 3, X_data, y_data)], 'ks', markersize=10, label='Optimum th√©orique')\n",
    "\n",
    "ax1.set_xlabel('w (pente)', fontsize=11)\n",
    "ax1.set_ylabel('b (intercept)', fontsize=11)\n",
    "ax1.set_zlabel('Loss (MSE)', fontsize=11)\n",
    "ax1.set_title('Paysage de la Loss 3D', fontweight='bold', fontsize=13)\n",
    "ax1.legend()\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Contours 2D avec trajectoire\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W, B, L, levels=20, cmap='hot')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.plot(w_history, b_history, 'g-', linewidth=2, alpha=0.7, label='Trajectoire')\n",
    "ax2.plot(w_history[0], b_history[0], 'go', markersize=10, label='D√©part (0, 0)')\n",
    "ax2.plot(w_history[-1], b_history[-1], 'r*', markersize=15, label=f'Arriv√©e ({w_final:.2f}, {b_final:.2f})')\n",
    "ax2.plot(2, 3, 'ks', markersize=10, label='Optimum (2, 3)')\n",
    "\n",
    "ax2.set_xlabel('w (pente)', fontsize=11)\n",
    "ax2.set_ylabel('b (intercept)', fontsize=11)\n",
    "ax2.set_title('Trajectoire dans l\\'Espace des Param√®tres', fontweight='bold', fontsize=13)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üèîÔ∏è Le paysage de la loss est un parabolo√Øde (forme de bol)\")\n",
    "print(\"üéØ Gradient Descent descend progressivement vers le minimum\")\n",
    "print(\"‚úÖ C'est exactement ce qui se passe dans l'entra√Ænement d'un r√©seau de neurones !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Partie 6 : Exp√©rimentations et Insights\n\n",
    "Explorons l'impact des hyperparam√®tres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Impact du Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparer diff√©rents learning rates\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Contours\n",
    "w_range = np.linspace(0, 4, 50)\n",
    "b_range = np.linspace(0, 6, 50)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "L = np.zeros_like(W)\n",
    "for i in range(len(w_range)):\n",
    "    for j in range(len(b_range)):\n",
    "        L[j, i] = mse_loss(W[j, i], B[j, i], X_data, y_data)\n",
    "\n",
    "contour = plt.contour(W, B, L, levels=15, cmap='gray', alpha=0.3)\n",
    "plt.clabel(contour, inline=True, fontsize=7)\n",
    "\n",
    "# Tester chaque learning rate\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    gd_test = GradientDescent(loss_wrapper, grad_wrapper, learning_rate=lr, max_iterations=200)\n",
    "    params_test = gd_test.optimize([0, 0], verbose=False)\n",
    "    \n",
    "    hist = gd_test.history\n",
    "    plt.plot(hist[:, 0], hist[:, 1], color=color, linewidth=2, alpha=0.8, \n",
    "             label=f'Œ± = {lr} ({len(hist)} iter)')\n",
    "    plt.plot(hist[0, 0], hist[0, 1], 'o', color=color, markersize=8)\n",
    "    plt.plot(hist[-1, 0], hist[-1, 1], '*', color=color, markersize=15)\n",
    "\n",
    "plt.plot(2, 3, 'ks', markersize=12, label='Optimum (2, 3)', zorder=10)\n",
    "plt.xlabel('w', fontsize=12)\n",
    "plt.ylabel('b', fontsize=12)\n",
    "plt.title('Impact du Learning Rate sur la Convergence', fontweight='bold', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observations:\")\n",
    "print(\"   - Learning rate TROP PETIT (0.001) : convergence lente\")\n",
    "print(\"   - Learning rate OPTIMAL (0.01-0.05) : convergence rapide et stable\")\n",
    "print(\"   - Learning rate TROP GRAND (0.1) : peut diverger ou osciller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Conclusion du Projet\n\n",
    "**F√©licitations ! Vous avez :**\n\n",
    "‚úÖ Visualis√© des gradients sur diff√©rentes fonctions  \n",
    "‚úÖ Impl√©ment√© Gradient Descent from scratch  \n",
    "‚úÖ Cr√©√© des visualisations 3D de paysages d'optimisation  \n",
    "‚úÖ Anim√© l'algorithme d'optimisation  \n",
    "‚úÖ Appliqu√© au Machine Learning (r√©gression lin√©aire)  \n",
    "‚úÖ Explor√© l'impact des hyperparam√®tres  \n\n",
    "---\n\n",
    "### ü§ñ Lien avec le Deep Learning\n\n",
    "Ce que vous avez fait dans ce projet est **exactement** ce qui se passe dans l'entra√Ænement d'un r√©seau de neurones :\n\n",
    "1. **Loss Function** ‚Üí Erreur entre pr√©dictions et cibles\n",
    "2. **Gradient** ‚Üí Direction pour am√©liorer les param√®tres\n",
    "3. **Gradient Descent** ‚Üí Mise √† jour des poids\n",
    "4. **It√©rations** ‚Üí Epochs d'entra√Ænement\n",
    "5. **Learning Rate** ‚Üí Hyperparam√®tre critique\n",
    "\n",
    "**La seule diff√©rence ?** Les r√©seaux de neurones ont des millions de param√®tres au lieu de 2 !\n\n",
    "---\n\n",
    "### üìö Pour Aller Plus Loin\n\n",
    "**Prochains sujets √† explorer :**\n",
    "- Momentum et optimiseurs avanc√©s (Adam, RMSprop)\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- Mini-batch Gradient Descent\n",
    "- Backpropagation dans les r√©seaux de neurones\n",
    "- R√©gularisation (L1, L2)\n",
    "\n",
    "**Continuez votre apprentissage avec :**\n",
    "- Alg√®bre Lin√©aire (matrices, vecteurs)\n",
    "- Probabilit√©s et Statistiques\n",
    "- R√©seaux de Neurones\n",
    "\n",
    "---\n\n",
    "**üéØ Vous comprenez maintenant les math√©matiques derri√®re le Machine Learning ! üí™**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
