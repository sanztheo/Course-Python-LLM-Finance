{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Solutions - Alg√®bre Lin√©aire Avanc√©e\n",
    "\n",
    "Voici les solutions d√©taill√©es de tous les exercices ! üìö\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Comment utiliser ce fichier :**\n",
    "1. Essaie d'abord de faire les exercices par toi-m√™me\n",
    "2. Compare tes solutions avec celles-ci\n",
    "3. Si tu ne comprends pas une solution, relis la section correspondante du cours\n",
    "4. N'h√©site pas √† exp√©rimenter et modifier le code pour mieux comprendre\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports n√©cessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import linalg\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"Imports r√©ussis ! ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ Section 1 : Espaces Vectoriels - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.1 : V√©rifier un Sous-Espace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1.1\n",
    "\n",
    "def test_subspace_W1():\n",
    "    \"\"\"W1 = {(x, y, 0)} - le plan xy\"\"\"\n",
    "    print(\"W1 = {(x, y, 0)}\")\n",
    "    \n",
    "    # Test 1 : Contient (0,0,0) ?\n",
    "    print(\"‚úì (0,0,0) ‚àà W1\")\n",
    "    \n",
    "    # Test 2 : Ferm√© par addition ?\n",
    "    v1 = np.array([1, 2, 0])\n",
    "    v2 = np.array([3, 4, 0])\n",
    "    sum_v = v1 + v2\n",
    "    print(f\"‚úì {v1} + {v2} = {sum_v} ‚àà W1\")\n",
    "    \n",
    "    # Test 3 : Ferm√© par multiplication scalaire ?\n",
    "    alpha = 5\n",
    "    scaled = alpha * v1\n",
    "    print(f\"‚úì {alpha} * {v1} = {scaled} ‚àà W1\")\n",
    "    \n",
    "    print(\"\\n‚úÖ W1 EST un sous-espace vectoriel\\n\")\n",
    "\n",
    "def test_subspace_W2():\n",
    "    \"\"\"W2 = {(x, y, z) : x+y+z=1}\"\"\"\n",
    "    print(\"W2 = {(x, y, z) : x+y+z=1}\")\n",
    "    \n",
    "    # Test 1 : Contient (0,0,0) ?\n",
    "    if 0 + 0 + 0 == 1:\n",
    "        print(\"‚úì (0,0,0) ‚àà W2\")\n",
    "    else:\n",
    "        print(\"‚úó (0,0,0) ‚àâ W2 car 0+0+0 ‚â† 1\")\n",
    "    \n",
    "    print(\"\\n‚ùå W2 N'EST PAS un sous-espace (ne contient pas l'origine)\\n\")\n",
    "\n",
    "def test_subspace_W3():\n",
    "    \"\"\"W3 = {(x, 2x, 3x)}\"\"\"\n",
    "    print(\"W3 = {(x, 2x, 3x)}\")\n",
    "    \n",
    "    # Test 1 : Contient (0,0,0) ?\n",
    "    print(\"‚úì (0,0,0) ‚àà W3 (avec x=0)\")\n",
    "    \n",
    "    # Test 2 : Ferm√© par addition ?\n",
    "    # v1 = (x1, 2x1, 3x1), v2 = (x2, 2x2, 3x2)\n",
    "    # v1+v2 = (x1+x2, 2(x1+x2), 3(x1+x2)) ‚àà W3 !\n",
    "    print(\"‚úì Ferm√© par addition\")\n",
    "    \n",
    "    # Test 3 : Ferm√© par multiplication scalaire ?\n",
    "    # Œ±*(x, 2x, 3x) = (Œ±x, 2Œ±x, 3Œ±x) ‚àà W3 !\n",
    "    print(\"‚úì Ferm√© par multiplication scalaire\")\n",
    "    \n",
    "    print(\"\\n‚úÖ W3 EST un sous-espace vectoriel (une ligne)\\n\")\n",
    "\n",
    "test_subspace_W1()\n",
    "test_subspace_W2()\n",
    "test_subspace_W3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.2 : Dimension d'un Sous-Espace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1.2\n",
    "\n",
    "# W1\n",
    "v1 = np.array([1, 0, 0])\n",
    "v2 = np.array([0, 1, 0])\n",
    "A = np.column_stack([v1, v2])\n",
    "print(f\"W1: rang = {np.linalg.matrix_rank(A)} ‚Üí dimension 2 (plan)\")\n",
    "\n",
    "# W2\n",
    "v1 = np.array([1, 2, 3])\n",
    "A = v1.reshape(-1, 1)\n",
    "print(f\"W2: rang = {np.linalg.matrix_rank(A)} ‚Üí dimension 1 (ligne)\")\n",
    "\n",
    "# W3\n",
    "v1 = np.array([1, 0, 1])\n",
    "v2 = np.array([2, 0, 2])  # v2 = 2*v1 ‚Üí d√©pendants !\n",
    "A = np.column_stack([v1, v2])\n",
    "print(f\"W3: rang = {np.linalg.matrix_rank(A)} ‚Üí dimension 1 (ligne)\")\n",
    "print(f\"Note : v2 = 2*v1, donc seulement 1 direction ind√©pendante\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.3 : Span de Vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1.3\n",
    "\n",
    "v1 = np.array([1, 2, 1])\n",
    "v2 = np.array([2, 1, 3])\n",
    "v3 = np.array([3, 3, 4])\n",
    "b = np.array([5, 5, 7])\n",
    "\n",
    "# R√©soudre A*x = b\n",
    "A = np.column_stack([v1, v2, v3])\n",
    "x = np.linalg.solve(A, b)\n",
    "\n",
    "print(f\"Coefficients : Œ±={x[0]:.4f}, Œ≤={x[1]:.4f}, Œ≥={x[2]:.4f}\")\n",
    "print(f\"\\nV√©rification : {x[0]:.4f}*v1 + {x[1]:.4f}*v2 + {x[2]:.4f}*v3 = \")\n",
    "reconstruction = x[0]*v1 + x[1]*v2 + x[2]*v3\n",
    "print(reconstruction)\n",
    "print(f\"b = {b}\")\n",
    "print(f\"\\n‚úì Match parfait : {np.allclose(reconstruction, b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1.4 : Intersection de Sous-Espaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1.4\n",
    "\n",
    "# Syst√®me :\n",
    "# x + y + z = 0\n",
    "# x - y = 0\n",
    "\n",
    "# De la 2e √©quation : x = y\n",
    "# Substituer dans la 1e : y + y + z = 0 ‚Üí z = -2y\n",
    "\n",
    "# Solution g√©n√©rale : (y, y, -2y) = y*(1, 1, -2)\n",
    "\n",
    "direction = np.array([1, 1, -2])\n",
    "print(\"Intersection = ligne de direction\", direction)\n",
    "print(\"Dimension = 1\")\n",
    "\n",
    "# V√©rification\n",
    "y = 3  # param√®tre quelconque\n",
    "point = y * direction\n",
    "print(f\"\\nExemple : pour y={y}, point = {point}\")\n",
    "print(f\"V√©rifie x+y+z=0 : {point[0]+point[1]+point[2]}\")\n",
    "print(f\"V√©rifie x-y=0 : {point[0]-point[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ Section 2 : Ind√©pendance Lin√©aire - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2.1 : Test d'Ind√©pendance Lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2.1\n",
    "\n",
    "def test_independence(vectors, name):\n",
    "    A = np.column_stack(vectors)\n",
    "    rank = np.linalg.matrix_rank(A)\n",
    "    n = len(vectors)\n",
    "    print(f\"{name}: rang={rank}, n_vecteurs={n} ‚Üí \", end=\"\")\n",
    "    if rank == n:\n",
    "        print(\"‚úÖ IND√âPENDANTS\")\n",
    "    else:\n",
    "        print(\"‚ùå D√âPENDANTS\")\n",
    "    return rank == n\n",
    "\n",
    "# Cas 1\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "v3 = np.array([7, 8, 9])\n",
    "test_independence([v1, v2, v3], \"Cas 1\")\n",
    "print(\"  Note : v3 = 2*v2 - v1\")\n",
    "\n",
    "# Cas 2 : Base standard\n",
    "v1 = np.array([1, 0, 0])\n",
    "v2 = np.array([0, 1, 0])\n",
    "v3 = np.array([0, 0, 1])\n",
    "test_independence([v1, v2, v3], \"Cas 2 (base standard)\")\n",
    "\n",
    "# Cas 3\n",
    "v1 = np.array([1, 2])\n",
    "v2 = np.array([2, 4])  # v2 = 2*v1\n",
    "test_independence([v1, v2], \"Cas 3\")\n",
    "print(\"  Note : v2 = 2*v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2.2 : Trouver une Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2.2\n",
    "\n",
    "v1 = np.array([1, 0, 1])\n",
    "v2 = np.array([0, 1, 1])\n",
    "v3 = np.array([1, 1, 2])\n",
    "v4 = np.array([2, 1, 3])\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "vectors = [v1, v2, v3, v4]\n",
    "names = ['v1', 'v2', 'v3', 'v4']\n",
    "\n",
    "print(\"Recherche d'une base de R¬≥ (3 vecteurs ind√©pendants) :\\n\")\n",
    "\n",
    "for combo in combinations(range(4), 3):\n",
    "    subset = [vectors[i] for i in combo]\n",
    "    subset_names = [names[i] for i in combo]\n",
    "    A = np.column_stack(subset)\n",
    "    rank = np.linalg.matrix_rank(A)\n",
    "    print(f\"{subset_names}: rang = {rank}\", end=\" \")\n",
    "    if rank == 3:\n",
    "        print(\"‚úÖ FORME UNE BASE !\")\n",
    "    else:\n",
    "        print(\"‚ùå\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2.3 : Compl√©ter une Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2.3\n",
    "\n",
    "v1 = np.array([1, 0, 0])\n",
    "v2 = np.array([1, 1, 0])\n",
    "\n",
    "# Essayons v3 = (0, 0, 1)\n",
    "v3 = np.array([0, 0, 1])\n",
    "\n",
    "A = np.column_stack([v1, v2, v3])\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "\n",
    "print(f\"v1 = {v1}\")\n",
    "print(f\"v2 = {v2}\")\n",
    "print(f\"v3 = {v3}\")\n",
    "print(f\"\\nRang de [v1 v2 v3] = {rank}\")\n",
    "\n",
    "if rank == 3:\n",
    "    print(\"‚úÖ {v1, v2, v3} forme une base de R¬≥ !\")\n",
    "    print(\"\\nExplication : v1 et v2 d√©finissent le plan xy,\")\n",
    "    print(\"v3 ajoute la dimension z ‚Üí couvre tout R¬≥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2.4 : Coordonn√©es dans une Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2.4\n",
    "\n",
    "b1 = np.array([1, 1])\n",
    "b2 = np.array([1, -1])\n",
    "v = np.array([3, 1])\n",
    "\n",
    "# R√©soudre [b1 b2] @ [Œ±, Œ≤]·µÄ = v\n",
    "B = np.column_stack([b1, b2])\n",
    "coords = np.linalg.solve(B, v)\n",
    "\n",
    "print(f\"Base B = {{b1, b2}}\")\n",
    "print(f\"b1 = {b1}, b2 = {b2}\")\n",
    "print(f\"\\nVecteur v = {v}\")\n",
    "print(f\"\\nCoordonn√©es dans B : [{coords[0]:.4f}, {coords[1]:.4f}]\")\n",
    "print(f\"\\nV√©rification : {coords[0]:.4f}*b1 + {coords[1]:.4f}*b2 = \")\n",
    "print(coords[0]*b1 + coords[1]*b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2.5 : Espace de Polyn√¥mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2.5\n",
    "\n",
    "print(\"Espace des polyn√¥mes de degr√© ‚â§ 2 :\")\n",
    "print(\"\\n1. Base standard : {1, x, x¬≤}\")\n",
    "print(\"2. Dimension : 3\")\n",
    "print(\"\\n3. p(x) = 3x¬≤ - 2x + 5\")\n",
    "print(\"   Coordonn√©es dans la base : [5, -2, 3]\")\n",
    "print(\"   Car p(x) = 5¬∑(1) + (-2)¬∑(x) + 3¬∑(x¬≤)\")\n",
    "\n",
    "# Repr√©sentation vectorielle\n",
    "p = np.array([5, -2, 3])  # [terme constant, coef de x, coef de x¬≤]\n",
    "print(f\"\\n   Vecteur : {p}\")\n",
    "\n",
    "# Exemple d'op√©ration\n",
    "q = np.array([1, 3, -1])  # q(x) = -x¬≤ + 3x + 1\n",
    "p_plus_q = p + q\n",
    "print(f\"\\n   Si q(x) = -x¬≤ + 3x + 1 = {q}\")\n",
    "print(f\"   Alors p(x) + q(x) = {p_plus_q}\")\n",
    "print(f\"   = {p_plus_q[2]}x¬≤ + {p_plus_q[1]}x + {p_plus_q[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ Section 3 : Rang - Solutions\n",
    "\n",
    "*(Solutions partielles pour optimiser l'espace - les exercices suivent le m√™me pattern)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.1 : Calcul du Rang\n",
    "\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = np.eye(3)\n",
    "C = np.array([[1, 2], [2, 4], [3, 6]])\n",
    "\n",
    "print(f\"Rang(A) = {np.linalg.matrix_rank(A)} (ligne 3 = 2*ligne2 - ligne1)\")\n",
    "print(f\"Rang(B) = {np.linalg.matrix_rank(B)} (identit√© ‚Üí rang plein)\")\n",
    "print(f\"Rang(C) = {np.linalg.matrix_rank(C)} (colonne 2 = 2*colonne 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.2 : Rang et Inversibilit√©\n",
    "\n",
    "A = np.array([[2, 1], [4, 3]])\n",
    "B = np.array([[1, 2], [2, 4]])\n",
    "\n",
    "for name, M in [(\"A\", A), (\"B\", B)]:\n",
    "    rank = np.linalg.matrix_rank(M)\n",
    "    det = np.linalg.det(M)\n",
    "    print(f\"\\nMatrice {name}:\")\n",
    "    print(f\"  Rang = {rank}\")\n",
    "    print(f\"  Det = {det:.4f}\")\n",
    "    if rank == M.shape[0] and not np.isclose(det, 0):\n",
    "        print(f\"  ‚úÖ INVERSIBLE\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå NON INVERSIBLE (singuli√®re)\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Solution 3.3\n\nA = np.array([[1, 2, 3], [2, 4, 6], [1, 1, 1]])\nb = np.array([6, 12, 3])\n\n# Rang de A\nrank_A = np.linalg.matrix_rank(A)\nprint(f\"Rang(A) = {rank_A}\")\nprint(f\"Note: ligne 2 = 2 √ó ligne 1 ‚Üí A est singuli√®re\\n\")\n\n# Matrice augment√©e [A|b]\nA_augmented = np.column_stack([A, b])\nrank_Ab = np.linalg.matrix_rank(A_augmented)\nprint(f\"Rang([A|b]) = {rank_Ab}\\n\")\n\n# Analyse de l'existence de solution\nprint(\"Th√©or√®me de Rouch√©-Capelli:\")\nif rank_A == rank_Ab:\n    print(f\"‚úÖ rang(A) = rang([A|b]) = {rank_A}\")\n    print(\"   ‚Üí Le syst√®me A¬∑x = b a au moins une solution\\n\")\n    \n    # R√©solution avec moindres carr√©s (car A singuli√®re)\n    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n    print(f\"Solution (moindres carr√©s): x = {x}\")\n    \n    # V√©rification\n    Ax = A @ x\n    print(f\"\\nV√©rification: A @ x = {Ax}\")\n    print(f\"Objectif: b = {b}\")\n    print(f\"Match: {np.allclose(Ax, b)}\")\nelse:\n    print(f\"‚ùå rang(A) = {rank_A} ‚â† rang([A|b]) = {rank_Ab}\")\n    print(\"   ‚Üí Le syst√®me est INCOMPATIBLE (pas de solution)\")\n\n# Explication g√©om√©trique\nprint(\"\\nüìä Interpr√©tation g√©om√©trique:\")\nprint(\"Les 3 √©quations repr√©sentent 3 plans dans ‚Ñù¬≥\")\nprint(\"Rang(A) < 3 ‚Üí les plans ne sont pas ind√©pendants\")\nif rank_A == rank_Ab:\n    print(\"rang(A) = rang([A|b]) ‚Üí les plans se coupent en au moins un point\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 3.3 : Rang et Syst√®mes Lin√©aires",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 3.5 : Approximation de Rang Faible\n\nnp.random.seed(123)\nA = np.random.randn(5, 5)\n\n# SVD\nU, s, Vt = np.linalg.svd(A)\n\nprint(\"Matrice originale (5√ó5) al√©atoire\")\nprint(f\"Valeurs singuli√®res: {s}\\n\")\nprint(f\"Rang original: {np.linalg.matrix_rank(A)}\\n\")\n\n# Approximations de rang k\nfor k in [1, 2, 3]:\n    print(f\"{'='*50}\")\n    print(f\"Approximation de rang {k}\")\n    print(f\"{'='*50}\")\n    \n    # Construire l'approximation\n    U_k = U[:, :k]\n    s_k = s[:k]\n    Vt_k = Vt[:k, :]\n    \n    A_k = U_k @ np.diag(s_k) @ Vt_k\n    \n    # Erreur de Frobenius\n    error = np.linalg.norm(A - A_k, 'fro')\n    norm_A = np.linalg.norm(A, 'fro')\n    error_relative = error / norm_A\n    \n    print(f\"Erreur Frobenius: {error:.4f}\")\n    print(f\"Erreur relative: {error_relative*100:.2f}%\")\n    \n    # Taux de compression\n    # Stockage original: 5√ó5 = 25 √©l√©ments\n    # Stockage rang k: k√ó5 (U_k) + k (s_k) + k√ó5 (Vt_k) = k(10+1)\n    storage_original = 25\n    storage_compressed = k * (5 + 1 + 5)\n    compression_ratio = (1 - storage_compressed / storage_original) * 100\n    \n    print(f\"Taux de compression: {compression_ratio:.1f}%\")\n    print(f\"(stockage: {storage_compressed} au lieu de {storage_original})\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 3.5 : Approximation de Rang Faible",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 4.2\n\nA = np.array([[4, 2], [1, 3]])\n\n# Calcul des valeurs propres\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(f\"Matrice A:\\n{A}\\n\")\nprint(f\"Valeurs propres: Œª‚ÇÅ = {eigenvalues[0]:.4f}, Œª‚ÇÇ = {eigenvalues[1]:.4f}\\n\")\n\n# Propri√©t√© 1 : Somme des valeurs propres = trace\ntrace_A = np.trace(A)\nsum_eigenvalues = np.sum(eigenvalues)\n\nprint(\"Propri√©t√© 1: Œ£Œª·µ¢ = tr(A)\")\nprint(f\"  Somme des valeurs propres: {sum_eigenvalues:.4f}\")\nprint(f\"  Trace de A: {trace_A:.4f}\")\nprint(f\"  ‚úì Match: {np.isclose(sum_eigenvalues, trace_A)}\\n\")\n\n# Propri√©t√© 2 : Produit des valeurs propres = d√©terminant\ndet_A = np.linalg.det(A)\nprod_eigenvalues = np.prod(eigenvalues)\n\nprint(\"Propri√©t√© 2: Œ†Œª·µ¢ = det(A)\")\nprint(f\"  Produit des valeurs propres: {prod_eigenvalues:.4f}\")\nprint(f\"  D√©terminant de A: {det_A:.4f}\")\nprint(f\"  ‚úì Match: {np.isclose(prod_eigenvalues, det_A)}\\n\")\n\n# Explication\nprint(\"üìö Rappel th√©orique:\")\nprint(\"Pour une matrice 2√ó2: A = [[a, b], [c, d]]\")\nprint(\"  ‚Ä¢ Trace = a + d = somme des √©l√©ments diagonaux\")\nprint(\"  ‚Ä¢ Det = ad - bc\")\nprint(\"Ces propri√©t√©s se g√©n√©ralisent en dimension n.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Solution 4.6\n\nA = np.array([[0.5, 0.2],\n              [0.1, 0.6]])\n\nprint(\"Syst√®me dynamique: x‚Çú‚Çä‚ÇÅ = A¬∑x‚Çú\\n\")\nprint(f\"Matrice de transition A:\\n{A}\\n\")\n\n# Calcul des valeurs propres\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(\"=\"*60)\nprint(\"Analyse de stabilit√©\")\nprint(\"=\"*60)\nprint(f\"\\nValeurs propres:\")\nfor i, Œª in enumerate(eigenvalues):\n    magnitude = np.abs(Œª)\n    print(f\"  Œª{i+1} = {Œª:.4f}, |Œª{i+1}| = {magnitude:.4f}\")\n\n# Crit√®re de stabilit√©\nmax_magnitude = np.max(np.abs(eigenvalues))\nprint(f\"\\nModule maximum: {max_magnitude:.4f}\")\n\nif max_magnitude < 1:\n    print(\"‚úÖ SYST√àME STABLE\")\n    print(\"   Toutes les valeurs propres ont |Œª| < 1\")\n    print(\"   ‚Üí Le syst√®me converge vers l'origine (point fixe stable)\")\nelif max_magnitude == 1:\n    print(\"‚ö†Ô∏è  SYST√àME MARGINALEMENT STABLE\")\n    print(\"   Au moins une valeur propre avec |Œª| = 1\")\nelse:\n    print(\"‚ùå SYST√àME INSTABLE\")\n    print(\"   Au moins une valeur propre avec |Œª| > 1\")\n    print(\"   ‚Üí Le syst√®me diverge\")\n\n# Simulation\nprint(\"\\n\" + \"=\"*60)\nprint(\"Simulation sur 20 it√©rations\")\nprint(\"=\"*60)\n\nx0 = np.array([10, 5])  # √âtat initial\nx = x0.copy()\nnorms = [np.linalg.norm(x0)]\n\nprint(f\"√âtat initial: x‚ÇÄ = {x0}\")\n\nfor t in range(20):\n    x = A @ x\n    norms.append(np.linalg.norm(x))\n    if t in [0, 4, 9, 19]:\n        print(f\"  t={t+1:2d}: x = [{x[0]:7.3f}, {x[1]:7.3f}], ||x|| = {norms[-1]:.3f}\")\n\n# Visualisation\nplt.figure(figsize=(10, 5))\nplt.plot(range(21), norms, 'o-', linewidth=2, markersize=6)\nplt.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Origine')\nplt.xlabel('It√©ration t')\nplt.ylabel('||x‚Çú|| (norme)')\nplt.title('√âvolution de la norme du syst√®me')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\nprint(f\"\\n‚úì La norme d√©cro√Æt vers 0 ‚Üí syst√®me stable confirm√© !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 4.6 : Application - Stabilit√© d'un Syst√®me",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 4.5\n\nA = np.array([[5, 2, 0],\n              [2, 6, 2],\n              [0, 2, 7]])\n\nprint(\"Matrice sym√©trique A:\")\nprint(A)\nprint(f\"\\nV√©rification sym√©trie: A = A·µÄ ? {np.allclose(A, A.T)}\\n\")\n\n# Calcul des valeurs et vecteurs propres\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Tri par valeur propre d√©croissante\nidx = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\nprint(\"=\"*60)\nprint(\"Propri√©t√© 1: Valeurs propres r√©elles\")\nprint(\"=\"*60)\nprint(f\"Valeurs propres: {eigenvalues}\")\nprint(f\"Toutes r√©elles? {np.all(np.isreal(eigenvalues))} ‚úì\\n\")\n\nprint(\"=\"*60)\nprint(\"Propri√©t√© 2: Vecteurs propres orthogonaux\")\nprint(\"=\"*60)\nfor i in range(3):\n    print(f\"\\nVecteur propre v{i+1}: {eigenvectors[:, i]}\")\n\nprint(\"\\nProduits scalaires (doivent √™tre ‚âà 0 pour i ‚â† j):\")\nfor i in range(3):\n    for j in range(i+1, 3):\n        dot_product = np.dot(eigenvectors[:, i], eigenvectors[:, j])\n        print(f\"v{i+1} ¬∑ v{j+1} = {dot_product:.6f} ‚âà 0 ‚úì\")\n\n# Normalisation et orthonormalit√©\nprint(\"\\n\" + \"=\"*60)\nprint(\"V√©rification: Vecteurs orthonormaux\")\nprint(\"=\"*60)\nV = eigenvectors\nVtV = V.T @ V\nprint(f\"V·µÄ¬∑V =\\n{VtV}\\n\")\nprint(f\"Est-ce l'identit√©? {np.allclose(VtV, np.eye(3))} ‚úì\")\n\n# Th√©or√®me spectral\nprint(\"\\n\" + \"=\"*60)\nprint(\"Th√©or√®me Spectral: A = V¬∑Œõ¬∑V·µÄ\")\nprint(\"=\"*60)\nLambda = np.diag(eigenvalues)\nA_reconstructed = V @ Lambda @ V.T\nprint(f\"A reconstruite:\\n{A_reconstructed}\\n\")\nprint(f\"Match avec A originale? {np.allclose(A, A_reconstructed)} ‚úì\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Solution 5.3\n\nA = np.array([[1, 2, 3],\n              [2, 4, 6],\n              [1, 2, 3]])\n\nprint(\"Matrice A (3√ó3):\")\nprint(A)\nprint(\"\\nNote: Ligne 2 = 2√óLigne 1, Ligne 3 = Ligne 1\")\nprint(\"Donc A est de rang d√©ficient\\n\")\n\n# M√©thode 1 : Rang via np.linalg.matrix_rank\nrank_direct = np.linalg.matrix_rank(A)\nprint(f\"M√©thode 1 - np.linalg.matrix_rank(A): {rank_direct}\")\n\n# M√©thode 2 : Rang via SVD\nU, s, Vt = np.linalg.svd(A)\n\nprint(f\"\\nM√©thode 2 - Comptage des valeurs singuli√®res non-nulles:\")\nprint(f\"Valeurs singuli√®res: {s}\")\n\n# Comptage avec seuil de tol√©rance\ntolerance = 1e-10\nrank_svd = np.sum(s > tolerance)\n\nprint(f\"\\nValeurs singuli√®res > {tolerance}:\")\nfor i, sigma in enumerate(s):\n    status = \"‚úì Non-nulle\" if sigma > tolerance else \"‚úó ‚âà 0 (nulle)\"\n    print(f\"  œÉ{i+1} = {sigma:.6f} ‚Üí {status}\")\n\nprint(f\"\\nRang via SVD: {rank_svd}\")\nprint(f\"\\n{'='*60}\")\nprint(f\"Th√©or√®me: rang(A) = nombre de valeurs singuli√®res non-nulles\")\nprint(f\"{'='*60}\")\nprint(f\"V√©rification: {rank_direct} = {rank_svd} ‚úì\")\n\n# Explication g√©om√©trique\nprint(f\"\\nüìä Interpr√©tation g√©om√©trique:\")\nprint(f\"Rang = {rank_direct} ‚Üí A compresse ‚Ñù¬≥ en une ligne (espace 1D)\")\nprint(f\"Les 3 lignes de A sont colin√©aires (m√™me direction)\")\nprint(f\"œÉ‚ÇÅ > 0 : l'information est concentr√©e dans 1 seule direction\")\nprint(f\"œÉ‚ÇÇ, œÉ‚ÇÉ ‚âà 0 : aucune information dans les autres directions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 5.3 : SVD et Rang",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 6.2\n\nnp.random.seed(42)\n\n# G√©n√©rer 100 points 3D avec corr√©lations\nn_samples = 100\n\n# Feature 1 : al√©atoire\nfeature1 = np.random.randn(n_samples)\n\n# Feature 2 : corr√©l√©e avec feature1 + bruit\nfeature2 = feature1 + np.random.randn(n_samples) * 0.3\n\n# Feature 3 : principalement du bruit\nfeature3 = np.random.randn(n_samples) * 0.1\n\n# Construire la matrice de donn√©es\nX = np.column_stack([feature1, feature2, feature3])\n\nprint(\"Donn√©es g√©n√©r√©es:\")\nprint(f\"Shape: {X.shape}\")\nprint(f\"Feature 1 (al√©atoire): std = {feature1.std():.2f}\")\nprint(f\"Feature 2 (= Feature1 + bruit): std = {feature2.std():.2f}\")\nprint(f\"Feature 3 (petit bruit): std = {feature3.std():.2f}\\n\")\n\n# PCA\nmean = np.mean(X, axis=0)\nX_centered = X - mean\ncov = np.cov(X_centered.T)\n\neigenvalues, eigenvectors = np.linalg.eig(cov)\nidx = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\nprint(\"=\"*60)\nprint(\"Analyse en Composantes Principales\")\nprint(\"=\"*60)\n\n# Variance expliqu√©e\ntotal_variance = np.sum(eigenvalues)\nvariance_ratio = eigenvalues / total_variance\ncumulative_variance = np.cumsum(variance_ratio)\n\nprint(f\"\\nValeurs propres: {eigenvalues}\")\nprint(f\"\\nVariance expliqu√©e par chaque composante:\")\nfor i in range(3):\n    print(f\"  PC{i+1}: {variance_ratio[i]*100:.2f}% (cumulatif: {cumulative_variance[i]*100:.2f}%)\")\n\n# Scree plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Graphique 1: Variance expliqu√©e\nax1.bar(range(1, 4), variance_ratio * 100, alpha=0.7, color='steelblue')\nax1.set_xlabel('Composante Principale')\nax1.set_ylabel('Variance Expliqu√©e (%)')\nax1.set_title('Scree Plot - Variance par Composante')\nax1.set_xticks(range(1, 4))\nax1.set_xticklabels(['PC1', 'PC2', 'PC3'])\nax1.grid(True, alpha=0.3)\n\n# Graphique 2: Variance cumulative\nax2.plot(range(1, 4), cumulative_variance * 100, 'o-', \n         linewidth=2, markersize=8, color='darkgreen')\nax2.axhline(y=95, color='r', linestyle='--', alpha=0.5, label='Seuil 95%')\nax2.set_xlabel('Nombre de Composantes')\nax2.set_ylabel('Variance Cumulative (%)')\nax2.set_title('Variance Cumulative Expliqu√©e')\nax2.set_xticks(range(1, 4))\nax2.grid(True, alpha=0.3)\nax2.legend()\nax2.set_ylim([0, 105])\n\nplt.tight_layout()\nplt.show()\n\n# R√©ponse √† la question\nn_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\nprint(f\"\\n{'='*60}\")\nprint(f\"Question: Combien de composantes pour 95% de variance?\")\nprint(f\"R√©ponse: {n_components_95} composante(s)\")\nprint(f\"Variance captur√©e: {cumulative_variance[n_components_95-1]*100:.2f}%\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\nüìä Interpr√©tation:\")\nprint(f\"PC1 capture {variance_ratio[0]*100:.1f}% : direction de corr√©lation Feature1-Feature2\")\nprint(f\"PC2 capture {variance_ratio[1]*100:.1f}% : variabilit√© r√©siduelle\")\nprint(f\"PC3 capture {variance_ratio[2]*100:.1f}% : principalement du bruit\")\nprint(f\"\\nConclusion: On peut r√©duire de 3D ‚Üí 2D en gardant {cumulative_variance[1]*100:.1f}% d'info !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Solution 6.6\n\nnp.random.seed(42)\n\n# Simuler 10 actions\nn_stocks = 10\nn_days = 252  # 1 an de trading\n\nprint(\"Simulation de rendements d'actions\")\nprint(f\"  Nombre d'actions: {n_stocks}\")\nprint(f\"  P√©riode: {n_days} jours (1 an de trading)\\n\")\n\n# Cr√©er une matrice de corr√©lation r√©aliste\n# Mod√®le: toutes les actions sont corr√©l√©es au \"march√©\" + bruit idiosyncratique\nmarket_factor = np.random.randn(n_days)\nidiosyncratic_noise = np.random.randn(n_days, n_stocks) * 0.5\n\n# Diff√©rents \"betas\" pour chaque action (exposition au march√©)\nbetas = np.random.uniform(0.5, 1.5, n_stocks)\n\n# Rendements simul√©s\nreturns = np.outer(market_factor, betas) + idiosyncratic_noise\n\n# Normaliser (rendements en %)\nreturns = returns * 0.01\n\nprint(f\"Rendements g√©n√©r√©s:\")\nprint(f\"  Shape: {returns.shape}\")\nprint(f\"  Moyenne des rendements quotidiens: {returns.mean()*100:.4f}%\")\nprint(f\"  Volatilit√© moyenne: {returns.std()*100:.2f}%\\n\")\n\n# Matrice de corr√©lation\ncorr_matrix = np.corrcoef(returns.T)\nprint(\"Matrice de corr√©lation (10√ó10):\")\nplt.figure(figsize=(8, 6))\nplt.imshow(corr_matrix, cmap='RdBu', vmin=-1, vmax=1)\nplt.colorbar(label='Corr√©lation')\nplt.title('Matrice de Corr√©lation des Rendements')\nplt.xlabel('Action')\nplt.ylabel('Action')\nplt.tight_layout()\nplt.show()\n\n# PCA\nprint(\"\\n\" + \"=\"*60)\nprint(\"Analyse en Composantes Principales\")\nprint(\"=\"*60)\n\nmean = np.mean(returns, axis=0)\nreturns_centered = returns - mean\ncov = np.cov(returns_centered.T)\n\neigenvalues, eigenvectors = np.linalg.eig(cov)\nidx = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[idx].real\neigenvectors = eigenvectors[:, idx].real\n\n# Variance expliqu√©e\nvariance_ratio = eigenvalues / np.sum(eigenvalues)\ncumulative_variance = np.cumsum(variance_ratio)\n\nprint(f\"\\nVariance expliqu√©e par facteur:\")\nfor i in range(5):\n    print(f\"  Facteur {i+1}: {variance_ratio[i]*100:.2f}% (cumulatif: {cumulative_variance[i]*100:.2f}%)\")\n\n# Scree plot\nplt.figure(figsize=(10, 5))\nplt.bar(range(1, 11), variance_ratio * 100, alpha=0.7, color='steelblue')\nplt.axhline(y=variance_ratio[0]*100, color='r', linestyle='--', \n            alpha=0.5, label=f'Premier facteur ({variance_ratio[0]*100:.1f}%)')\nplt.xlabel('Facteur de Risque')\nplt.ylabel('Variance Expliqu√©e (%)')\nplt.title('D√©composition du Risque en Facteurs')\nplt.xticks(range(1, 11))\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.show()\n\n# Question: Combien de facteurs pour 80% du risque?\nn_factors_80 = np.argmax(cumulative_variance >= 0.80) + 1\nprint(f\"\\n{'='*60}\")\nprint(f\"Question: Combien de facteurs expliquent 80% du risque?\")\nprint(f\"R√©ponse: {n_factors_80} facteur(s)\")\nprint(f\"Variance captur√©e: {cumulative_variance[n_factors_80-1]*100:.2f}%\")\nprint(f\"{'='*60}\")\n\n# Loadings de la premi√®re composante (betas)\nprint(f\"\\n{'='*60}\")\nprint(\"Loadings de la premi√®re composante (PC1)\")\nprint(\"= Exposition de chaque action au facteur de march√©\")\nprint(f\"{'='*60}\")\n\nloadings_pc1 = eigenvectors[:, 0]\nstock_names = [f'Stock {i+1}' for i in range(n_stocks)]\n\n# Tri par loadings\nsorted_idx = np.argsort(np.abs(loadings_pc1))[::-1]\n\nprint(f\"\\nActions tri√©es par exposition au march√©:\")\nfor i in sorted_idx:\n    print(f\"  {stock_names[i]}: {loadings_pc1[i]:+.3f}\")\n\n# Visualisation des loadings\nplt.figure(figsize=(10, 5))\ncolors = ['green' if x > 0 else 'red' for x in loadings_pc1]\nplt.bar(range(n_stocks), loadings_pc1, color=colors, alpha=0.7)\nplt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\nplt.xlabel('Action')\nplt.ylabel('Loading (Beta du march√©)')\nplt.title('Exposition de chaque action au premier facteur de risque (PC1)')\nplt.xticks(range(n_stocks), [f'S{i+1}' for i in range(n_stocks)])\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüìä Interpr√©tation financi√®re:\")\nprint(f\"  ‚Ä¢ PC1 ({variance_ratio[0]*100:.1f}%) = Facteur de march√©\")\nprint(f\"    ‚Üí Mouvement commun √† toutes les actions\")\nprint(f\"    ‚Üí Risque syst√©matique (non-diversifiable)\")\nprint(f\"\\n  ‚Ä¢ PC2-PC{n_factors_80} = Facteurs sectoriels/styles\")\nprint(f\"    ‚Üí Expliquent les diff√©rences entre actions\")\nprint(f\"\\n  ‚Ä¢ PC{n_factors_80+1}+ = Risque idiosyncratique\")\nprint(f\"    ‚Üí Sp√©cifique √† chaque action (diversifiable)\")\nprint(f\"\\nConclusion: {n_factors_80} facteur(s) suffisent pour mod√©liser 80% du risque !\")\nprint(f\"‚Üí Utile pour gestion de portefeuille et couverture du risque\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Solution 7.5\n\nnp.random.seed(42)\n\n# Cr√©er une matrice terme-document\nn_terms = 20  # nombre de mots\nn_docs = 10   # nombre de documents\n\n# Noms de termes (simul√©s)\nterms = [f'mot_{i+1}' for i in range(n_terms)]\ndocs = [f'doc_{i+1}' for i in range(n_docs)]\n\n# Simuler 3 \"topics\" latents\n# Topic 1: mots 1-7 (ex: technologie)\n# Topic 2: mots 8-14 (ex: sport)\n# Topic 3: mots 15-20 (ex: cuisine)\n\n# Matrice terme-document avec structure th√©matique\nterm_doc = np.zeros((n_terms, n_docs))\n\n# Documents 1-3: Topic 1 (technologie)\nterm_doc[:7, :3] = np.random.poisson(5, (7, 3))\n\n# Documents 4-6: Topic 2 (sport)\nterm_doc[7:14, 3:6] = np.random.poisson(5, (7, 3))\n\n# Documents 7-10: Topic 3 (cuisine)\nterm_doc[14:, 6:] = np.random.poisson(5, (6, 4))\n\n# Ajouter du bruit\nterm_doc += np.random.poisson(0.5, (n_terms, n_docs))\n\nprint(\"Matrice Terme-Document (20 mots √ó 10 documents)\")\nprint(f\"Shape: {term_doc.shape}\")\nprint(f\"Total de mots: {term_doc.sum():.0f}\\n\")\n\n# Visualisation de la matrice\nplt.figure(figsize=(10, 8))\nplt.imshow(term_doc, cmap='YlOrRd', aspect='auto')\nplt.colorbar(label='Fr√©quence')\nplt.xlabel('Documents')\nplt.ylabel('Termes')\nplt.title('Matrice Terme-Document')\nplt.xticks(range(n_docs), docs, rotation=45)\nplt.yticks(range(n_terms), terms)\nplt.tight_layout()\nplt.show()\n\n# SVD\nU, s, Vt = np.linalg.svd(term_doc, full_matrices=False)\n\nprint(f\"{'='*60}\")\nprint(\"D√©composition SVD\")\nprint(f\"{'='*60}\")\nprint(f\"\\nValeurs singuli√®res: {s}\")\n\n# Variance expliqu√©e\nvariance = s ** 2\nvariance_ratio = variance / np.sum(variance)\n\nprint(f\"\\nVariance expliqu√©e par composante:\")\nfor i in range(min(5, len(s))):\n    print(f\"  Composante {i+1}: {variance_ratio[i]*100:.2f}%\")\n\n# R√©duction √† 3 topics\nk = 3\nU_k = U[:, :k]\ns_k = s[:k]\nVt_k = Vt[:k, :]\n\nprint(f\"\\n{'='*60}\")\nprint(f\"R√©duction √† {k} topics\")\nprint(f\"{'='*60}\")\n\n# Interpr√©tation des topics\nprint(f\"\\nInterpr√©tation des {k} topics (via U_k):\")\nfor i in range(k):\n    # Top 5 termes pour chaque topic\n    topic_terms = U_k[:, i]\n    top_indices = np.argsort(np.abs(topic_terms))[-5:][::-1]\n    \n    print(f\"\\nTopic {i+1} (œÉ={s_k[i]:.2f}):\")\n    print(f\"  Termes principaux:\")\n    for idx in top_indices:\n        print(f\"    {terms[idx]}: {topic_terms[idx]:.3f}\")\n\n# Document embeddings (projection dans l'espace des topics)\ndoc_embeddings = np.diag(s_k) @ Vt_k  # 3 √ó 10\n\nprint(f\"\\n{'='*60}\")\nprint(\"Embeddings des documents (3D ‚Üí topics)\")\nprint(f\"{'='*60}\")\n\nfor j in range(n_docs):\n    print(f\"\\n{docs[j]}:\")\n    for i in range(k):\n        print(f\"  Topic {i+1}: {doc_embeddings[i, j]:.2f}\")\n\n# Visualisation des documents dans l'espace des topics\nfig = plt.figure(figsize=(12, 5))\n\n# 2D visualization (Topics 1 vs 2)\nax1 = fig.add_subplot(121)\ncolors = ['red']*3 + ['blue']*3 + ['green']*4  # colorer par topic dominant\nax1.scatter(doc_embeddings[0, :], doc_embeddings[1, :], \n           c=colors, s=100, alpha=0.7)\nfor j in range(n_docs):\n    ax1.annotate(docs[j], (doc_embeddings[0, j], doc_embeddings[1, j]),\n                xytext=(5, 5), textcoords='offset points', fontsize=8)\nax1.set_xlabel('Topic 1')\nax1.set_ylabel('Topic 2')\nax1.set_title('Documents dans l\\'Espace des Topics (2D)')\nax1.grid(True, alpha=0.3)\n\n# 3D visualization\nax2 = fig.add_subplot(122, projection='3d')\nax2.scatter(doc_embeddings[0, :], doc_embeddings[1, :], doc_embeddings[2, :],\n           c=colors, s=100, alpha=0.7)\nfor j in range(n_docs):\n    ax2.text(doc_embeddings[0, j], doc_embeddings[1, j], doc_embeddings[2, j],\n            docs[j], fontsize=8)\nax2.set_xlabel('Topic 1')\nax2.set_ylabel('Topic 2')\nax2.set_zlabel('Topic 3')\nax2.set_title('Documents dans l\\'Espace 3D des Topics')\n\nplt.tight_layout()\nplt.show()\n\n# Reconstruction et taux de compression\nterm_doc_reconstructed = U_k @ np.diag(s_k) @ Vt_k\n\n# Erreur de reconstruction\nerror = np.linalg.norm(term_doc - term_doc_reconstructed, 'fro')\nrelative_error = error / np.linalg.norm(term_doc, 'fro')\n\nprint(f\"\\n{'='*60}\")\nprint(\"Qualit√© de la reconstruction\")\nprint(f\"{'='*60}\")\nprint(f\"Erreur relative: {relative_error*100:.2f}%\")\nprint(f\"Variance captur√©e: {np.sum(variance_ratio[:k])*100:.2f}%\")\n\n# Taux de compression\nstorage_original = n_terms * n_docs\nstorage_compressed = k * (n_terms + 1 + n_docs)  # U_k + s_k + Vt_k\ncompression_ratio = storage_compressed / storage_original\n\nprint(f\"\\nTaux de compression:\")\nprint(f\"  Stockage original: {storage_original} √©l√©ments\")\nprint(f\"  Stockage compress√©: {storage_compressed} √©l√©ments\")\nprint(f\"  Ratio: {compression_ratio*100:.1f}%\")\nprint(f\"  √âconomie: {(1-compression_ratio)*100:.1f}%\")\n\nprint(f\"\\nüìö Applications en NLP (Traitement du Langage Naturel):\")\nprint(f\"  ‚Ä¢ Latent Semantic Analysis (LSA)\")\nprint(f\"  ‚Ä¢ R√©duction de dimensionnalit√© pour textes\")\nprint(f\"  ‚Ä¢ Recherche s√©mantique de documents\")\nprint(f\"  ‚Ä¢ D√©tection de topics automatique\")\nprint(f\"\\n‚úì SVD r√©v√®le la structure th√©matique cach√©e dans les documents !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 7.5 : Compression de Texte avec SVD",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 7.4\n\nnp.random.seed(42)\n\n# Simuler les rendements de 5 actions\nn_stocks = 5\nn_days = 252  # 1 an\n\nstock_names = ['Action A', 'Action B', 'Action C', 'Action D', 'Action E']\n\n# Rendements quotidiens (en %)\nmean_returns = np.array([0.05, 0.08, 0.06, 0.10, 0.04]) / 252  # rendements annualis√©s ‚Üí quotidiens\nvolatilities = np.array([0.15, 0.25, 0.18, 0.30, 0.12]) / np.sqrt(252)  # volatilit√© quotidienne\n\n# Matrice de corr√©lation\ncorr = np.array([\n    [1.0, 0.6, 0.5, 0.3, 0.4],\n    [0.6, 1.0, 0.4, 0.5, 0.3],\n    [0.5, 0.4, 1.0, 0.6, 0.5],\n    [0.3, 0.5, 0.6, 1.0, 0.4],\n    [0.4, 0.3, 0.5, 0.4, 1.0]\n])\n\n# Matrice de covariance: Œ£ = D¬∑R¬∑D o√π D = diag(volatilit√©s)\nD = np.diag(volatilities)\nSigma = D @ corr @ D\n\nprint(\"Simulation de portefeuille:\")\nprint(f\"  Nombre d'actions: {n_stocks}\")\nprint(f\"  P√©riode: {n_days} jours\\n\")\n\nprint(\"Rendements annualis√©s attendus:\")\nfor i, name in enumerate(stock_names):\n    print(f\"  {name}: {mean_returns[i]*252*100:.2f}% (vol: {volatilities[i]*np.sqrt(252)*100:.1f}%)\")\n\nprint(f\"\\nMatrice de covariance (quotidienne):\")\nprint(Sigma)\n\n# Portefeuille √©quipond√©r√©\nw = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n\nprint(f\"\\n{'='*60}\")\nprint(\"Analyse du portefeuille √©quipond√©r√©\")\nprint(f\"{'='*60}\")\nprint(f\"\\nPoids: {w}\")\n\n# Rendement du portefeuille\nportfolio_return = w @ mean_returns\nprint(f\"\\nRendement quotidien: {portfolio_return*100:.4f}%\")\nprint(f\"Rendement annualis√©: {portfolio_return*252*100:.2f}%\")\n\n# Risque du portefeuille: œÉ‚Çö = ‚àö(w^T Œ£ w)\nportfolio_variance = w @ Sigma @ w\nportfolio_risk = np.sqrt(portfolio_variance)\n\nprint(f\"\\nRisque quotidien: {portfolio_risk*100:.4f}%\")\nprint(f\"Risque annualis√©: {portfolio_risk*np.sqrt(252)*100:.2f}%\")\n\n# Ratio de Sharpe (simplifi√©, sans taux sans risque)\nsharpe_ratio = portfolio_return / portfolio_risk\nprint(f\"\\nRatio de Sharpe quotidien: {sharpe_ratio:.4f}\")\nprint(f\"Ratio de Sharpe annualis√©: {sharpe_ratio*np.sqrt(252):.4f}\")\n\n# Analyse via valeurs propres\nprint(f\"\\n{'='*60}\")\nprint(\"D√©composition du risque via valeurs propres\")\nprint(f\"{'='*60}\")\n\neigenvalues, eigenvectors = np.linalg.eig(Sigma)\nidx = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[idx].real\neigenvectors = eigenvectors[:, idx].real\n\nprint(f\"\\nValeurs propres de Œ£:\")\nfor i in range(n_stocks):\n    variance_contrib = eigenvalues[i] / np.sum(eigenvalues)\n    print(f\"  Œª{i+1} = {eigenvalues[i]:.6f} ({variance_contrib*100:.1f}% de la variance)\")\n\n# Direction de risque maximum (vecteur propre de Œª_max)\nmax_risk_direction = eigenvectors[:, 0]\nprint(f\"\\nDirection de risque MAXIMUM (PC1):\")\nprint(f\"  Vecteur: {max_risk_direction}\")\nprint(f\"  Interpr√©tation: Composition du portefeuille le plus risqu√©\")\n\n# Direction de risque minimum (vecteur propre de Œª_min)\nmin_risk_direction = eigenvectors[:, -1]\nprint(f\"\\nDirection de risque MINIMUM (PC{n_stocks}):\")\nprint(f\"  Vecteur: {min_risk_direction}\")\nprint(f\"  Interpr√©tation: Composition du portefeuille le moins risqu√©\")\n\n# Visualisation\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# 1. Contributions au risque\nrisk_contributions = (w * (Sigma @ w)) / portfolio_variance\naxes[0].bar(stock_names, risk_contributions * 100, color='steelblue', alpha=0.7)\naxes[0].set_ylabel('Contribution au Risque (%)')\naxes[0].set_title('Contribution de Chaque Action au Risque Total')\naxes[0].tick_params(axis='x', rotation=45)\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# 2. Efficient frontier (simplifi√©)\n# G√©n√©rer des portefeuilles al√©atoires\nn_portfolios = 1000\nreturns = []\nrisks = []\n\nfor _ in range(n_portfolios):\n    # Poids al√©atoires qui somment √† 1\n    weights = np.random.random(n_stocks)\n    weights /= weights.sum()\n    \n    ret = weights @ mean_returns\n    risk = np.sqrt(weights @ Sigma @ weights)\n    \n    returns.append(ret * 252 * 100)  # annualis√©\n    risks.append(risk * np.sqrt(252) * 100)  # annualis√©\n\naxes[1].scatter(risks, returns, alpha=0.3, s=10, c='gray')\n# Portefeuille √©quipond√©r√©\naxes[1].scatter(portfolio_risk*np.sqrt(252)*100, portfolio_return*252*100, \n               color='red', s=200, marker='*', label='√âquipond√©r√©', zorder=5)\naxes[1].set_xlabel('Risque Annualis√© (%)')\naxes[1].set_ylabel('Rendement Annualis√© (%)')\naxes[1].set_title('Fronti√®re Efficiente (simul√©e)')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# 3. Directions de risque\naxes[2].bar(stock_names, max_risk_direction, alpha=0.6, label='Risque MAX (PC1)', color='red')\naxes[2].bar(stock_names, min_risk_direction, alpha=0.6, label='Risque MIN (PC5)', color='green')\naxes[2].set_ylabel('Poids')\naxes[2].set_title('Directions de Risque Extr√™mes')\naxes[2].tick_params(axis='x', rotation=45)\naxes[2].legend()\naxes[2].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüìä Interpr√©tation financi√®re:\")\nprint(f\"  ‚Ä¢ Les valeurs propres identifient les sources de risque\")\nprint(f\"  ‚Ä¢ PC1 ({eigenvalues[0]/np.sum(eigenvalues)*100:.1f}%) = facteur de risque dominant\")\nprint(f\"  ‚Ä¢ Les vecteurs propres donnent les compositions extr√™mes\")\nprint(f\"  ‚Ä¢ Utile pour: hedging, diversification, gestion du risque\")\nprint(f\"\\n‚úì L'analyse spectrale r√©v√®le la structure du risque du portefeuille !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 7.4 : Portfolio Optimization Simple",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 7.3\n\nnp.random.seed(42)\n\n# G√©n√©rer 100 points 3D normaux (distribution gaussienne)\nn_normal = 100\nnormal_points = np.random.randn(n_normal, 3)\n\n# Ajouter une corr√©lation entre dimensions\nnormal_points[:, 1] = normal_points[:, 0] * 0.8 + np.random.randn(n_normal) * 0.3\nnormal_points[:, 2] = normal_points[:, 0] * 0.5 + np.random.randn(n_normal) * 0.4\n\n# G√©n√©rer 10 anomalies (points tr√®s √©loign√©s)\nn_anomalies = 10\nanomalies = np.random.randn(n_anomalies, 3) * 4 + np.array([5, 5, 5])  # √©loign√©s du centre\n\n# Combiner\nX = np.vstack([normal_points, anomalies])\nlabels = np.array([0] * n_normal + [1] * n_anomalies)  # 0=normal, 1=anomalie\n\nprint(f\"Dataset:\")\nprint(f\"  Points normaux: {n_normal}\")\nprint(f\"  Anomalies: {n_anomalies}\")\nprint(f\"  Total: {len(X)}\\n\")\n\n# PCA avec 2 composantes\nmean = np.mean(X, axis=0)\nX_centered = X - mean\ncov = np.cov(X_centered.T)\n\neigenvalues, eigenvectors = np.linalg.eig(cov)\nidx = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[idx].real\neigenvectors = eigenvectors[:, idx].real\n\n# Projection sur 2 composantes\nn_components = 2\ncomponents = eigenvectors[:, :n_components]\nX_pca = X_centered @ components\n\n# Reconstruction\nX_reconstructed = X_pca @ components.T + mean\n\n# Erreur de reconstruction pour chaque point\nreconstruction_errors = np.sum((X - X_reconstructed) ** 2, axis=1)\n\nprint(\"=\"*60)\nprint(\"Analyse des erreurs de reconstruction\")\nprint(\"=\"*60)\n\n# Statistiques par groupe\nerrors_normal = reconstruction_errors[labels == 0]\nerrors_anomalies = reconstruction_errors[labels == 1]\n\nprint(f\"\\nPoints normaux:\")\nprint(f\"  Erreur moyenne: {errors_normal.mean():.4f}\")\nprint(f\"  Erreur std: {errors_normal.std():.4f}\")\nprint(f\"  Erreur max: {errors_normal.max():.4f}\")\n\nprint(f\"\\nAnomalies:\")\nprint(f\"  Erreur moyenne: {errors_anomalies.mean():.4f}\")\nprint(f\"  Erreur std: {errors_anomalies.std():.4f}\")\nprint(f\"  Erreur min: {errors_anomalies.min():.4f}\")\n\n# D√©finir un seuil de d√©tection\nthreshold = errors_normal.mean() + 2 * errors_normal.std()\nprint(f\"\\nSeuil de d√©tection (moyenne + 2œÉ): {threshold:.4f}\")\n\n# Pr√©dictions\npredicted_anomalies = reconstruction_errors > threshold\ntrue_positives = np.sum((predicted_anomalies == 1) & (labels == 1))\nfalse_positives = np.sum((predicted_anomalies == 1) & (labels == 0))\ntrue_negatives = np.sum((predicted_anomalies == 0) & (labels == 0))\nfalse_negatives = np.sum((predicted_anomalies == 0) & (labels == 1))\n\nprint(f\"\\nPerformance de d√©tection:\")\nprint(f\"  Vrais positifs: {true_positives}/{n_anomalies}\")\nprint(f\"  Faux positifs: {false_positives}/{n_normal}\")\nprint(f\"  Pr√©cision: {true_positives/(true_positives+false_positives)*100:.1f}%\")\nprint(f\"  Rappel: {true_positives/n_anomalies*100:.1f}%\")\n\n# Visualisation en 2D (espace PCA)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Graphique 1: Points dans l'espace PCA\nax1.scatter(X_pca[labels==0, 0], X_pca[labels==0, 1], \n           c='blue', alpha=0.6, s=50, label='Normal')\nax1.scatter(X_pca[labels==1, 0], X_pca[labels==1, 1], \n           c='red', alpha=0.8, s=100, marker='X', label='Anomalie')\nax1.set_xlabel('PC1')\nax1.set_ylabel('PC2')\nax1.set_title('Points dans l\\'Espace PCA R√©duit (2D)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Graphique 2: Erreurs de reconstruction\ncolors = ['red' if l == 1 else 'blue' for l in labels]\nax2.scatter(range(len(X)), reconstruction_errors, c=colors, alpha=0.6, s=50)\nax2.axhline(y=threshold, color='orange', linestyle='--', linewidth=2, \n           label=f'Seuil = {threshold:.2f}')\nax2.set_xlabel('Index du Point')\nax2.set_ylabel('Erreur de Reconstruction')\nax2.set_title('Erreur de Reconstruction par Point')\nax2.legend(['Seuil', 'Normal', 'Anomalie'])\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Distribution des erreurs\nplt.figure(figsize=(10, 5))\nplt.hist(errors_normal, bins=20, alpha=0.6, label='Normal', color='blue')\nplt.hist(errors_anomalies, bins=20, alpha=0.6, label='Anomalies', color='red')\nplt.axvline(x=threshold, color='orange', linestyle='--', linewidth=2, label='Seuil')\nplt.xlabel('Erreur de Reconstruction')\nplt.ylabel('Fr√©quence')\nplt.title('Distribution des Erreurs de Reconstruction')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\\n‚úì Les anomalies ont des erreurs de reconstruction plus √©lev√©es !\")\nprint(\"‚úì PCA capte la structure normale ‚Üí points hors structure = anomalies\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 7.3 : D√©tection d'Anomalies avec PCA",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 7.2\n\nnp.random.seed(42)\n\n# Simuler 20 \"visages\" 10√ó10 corr√©l√©s\nn_faces = 20\nface_size = 10\n\n# Cr√©er des patterns de base (eigenfaces th√©oriques)\npattern1 = np.outer(np.linspace(0, 1, face_size), np.ones(face_size))  # gradient vertical\npattern2 = np.outer(np.ones(face_size), np.linspace(0, 1, face_size))  # gradient horizontal\npattern3 = np.random.randn(face_size, face_size) * 0.2  # texture\n\n# G√©n√©rer des visages comme combinaisons de patterns + bruit\nfaces = []\nfor i in range(n_faces):\n    w1, w2, w3 = np.random.randn(3)\n    face = w1 * pattern1 + w2 * pattern2 + w3 * pattern3\n    face += np.random.randn(face_size, face_size) * 0.1  # bruit\n    faces.append(face.flatten())\n\nX = np.array(faces)  # 20 √ó 100\n\nprint(\"Dataset de visages simul√©s:\")\nprint(f\"  Nombre de visages: {X.shape[0]}\")\nprint(f\"  Dimensions: {X.shape[1]} (images 10√ó10 aplaties)\\n\")\n\n# PCA\nmean_face = np.mean(X, axis=0)\nX_centered = X - mean_face\n\n# SVD\nU, s, Vt = np.linalg.svd(X_centered, full_matrices=False)\neigenfaces = Vt.T  # 100 √ó 20\n\n# Variance expliqu√©e\nvariance = (s ** 2) / (X.shape[0] - 1)\nvariance_ratio = variance / np.sum(variance)\n\nprint(\"Variance expliqu√©e par les 5 premi√®res eigenfaces:\")\nfor i in range(5):\n    print(f\"  Eigenface {i+1}: {variance_ratio[i]*100:.2f}%\")\n\n# Visualisation des eigenfaces\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\naxes = axes.flatten()\n\n# Visage moyen\naxes[0].imshow(mean_face.reshape(face_size, face_size), cmap='gray')\naxes[0].set_title('Visage Moyen', fontsize=10)\naxes[0].axis('off')\n\n# 5 premi√®res eigenfaces\nfor i in range(5):\n    eigenface = eigenfaces[:, i].reshape(face_size, face_size)\n    axes[i+1].imshow(eigenface, cmap='gray')\n    axes[i+1].set_title(f'Eigenface {i+1}\\n({variance_ratio[i]*100:.1f}%)', fontsize=9)\n    axes[i+1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Reconstruction d'un visage avec diff√©rents nombres de composantes\nface_idx = 0\noriginal_face = X[face_idx].reshape(face_size, face_size)\n\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\naxes = axes.flatten()\n\n# Original\naxes[0].imshow(original_face, cmap='gray')\naxes[0].set_title('Original', fontsize=10)\naxes[0].axis('off')\n\n# Reconstructions\nn_components_list = [1, 2, 3, 5, 10]\nfor idx, n_comp in enumerate(n_components_list, start=1):\n    # Projection\n    X_pca = X_centered[face_idx] @ eigenfaces[:, :n_comp]\n    \n    # Reconstruction\n    X_reconstructed = X_pca @ eigenfaces[:, :n_comp].T + mean_face\n    face_reconstructed = X_reconstructed.reshape(face_size, face_size)\n    \n    # Erreur\n    mse = np.mean((X[face_idx] - X_reconstructed) ** 2)\n    var_explained = np.sum(variance_ratio[:n_comp])\n    \n    axes[idx].imshow(face_reconstructed, cmap='gray')\n    axes[idx].set_title(\n        f'{n_comp} eigenfaces\\n'\n        f'MSE: {mse:.3f}\\n'\n        f'Var: {var_explained*100:.0f}%',\n        fontsize=9\n    )\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Analyse de la qualit√© de reconstruction\nprint(\"\\n\" + \"=\"*60)\nprint(\"Qualit√© de reconstruction (moyenne sur tous les visages)\")\nprint(\"=\"*60)\n\nfor n_comp in [1, 2, 3, 5, 10, 20]:\n    X_pca = X_centered @ eigenfaces[:, :n_comp]\n    X_reconstructed = X_pca @ eigenfaces[:, :n_comp].T + mean_face\n    \n    mse = np.mean((X - X_reconstructed) ** 2)\n    var_explained = np.sum(variance_ratio[:n_comp])\n    \n    print(f\"\\n{n_comp:2d} eigenfaces:\")\n    print(f\"  MSE: {mse:.4f}\")\n    print(f\"  Variance expliqu√©e: {var_explained*100:.1f}%\")\n\nprint(\"\\n‚úì Les eigenfaces capturent les variations principales entre visages\")\nprint(\"‚úì Application: reconnaissance faciale, compression d'images\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 7.2 : Eigenfaces",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 7.1\n\n# Cr√©er une matrice utilisateurs √ó films\n# 0 = non not√©, 1-5 = notes\nnp.random.seed(42)\n\nusers = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']\nmovies = ['Action1', 'Comedy1', 'Drama1', 'Action2', 'Comedy2', 'Drama2', 'Action3', 'Comedy3']\n\n# Matrice avec notes (0 = manquant)\nratings = np.array([\n    [5, 1, 0, 4, 2, 0, 5, 1],  # Alice aime l'action\n    [1, 5, 0, 2, 5, 1, 1, 4],  # Bob aime les com√©dies\n    [0, 1, 5, 1, 0, 4, 0, 2],  # Charlie aime le drame\n    [4, 0, 1, 5, 1, 0, 4, 0],  # Diana aime l'action\n    [2, 4, 0, 1, 5, 1, 2, 5],  # Eve aime les com√©dies\n], dtype=float)\n\nprint(\"Matrice de notes (utilisateurs √ó films):\")\nprint(\"0 = non not√©, 1-5 = notes\\n\")\nfor i, user in enumerate(users):\n    print(f\"{user:8s}: {ratings[i]}\")\n\n# Masquer quelques notes pour tester\ntest_positions = [(0, 2), (1, 2), (2, 1), (3, 1), (4, 2)]  # positions √† pr√©dire\ntrue_values = {}\nfor i, j in test_positions:\n    true_values[(i, j)] = ratings[i, j]\n    ratings[i, j] = 0\n\nprint(f\"\\n{len(test_positions)} notes masqu√©es pour test...\")\n\n# Remplacer les 0 par la moyenne utilisateur (pour SVD)\nratings_filled = ratings.copy()\nfor i in range(len(users)):\n    user_ratings = ratings[i, ratings[i] > 0]\n    if len(user_ratings) > 0:\n        mean_rating = user_ratings.mean()\n        ratings_filled[i, ratings_filled[i] == 0] = mean_rating\n\n# SVD avec rang 2\nU, s, Vt = np.linalg.svd(ratings_filled, full_matrices=False)\n\n# Approximation rang 2\nk = 2\nU_k = U[:, :k]\ns_k = s[:k]\nVt_k = Vt[:k, :]\n\nratings_reconstructed = U_k @ np.diag(s_k) @ Vt_k\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Reconstruction avec SVD (rang {k})\")\nprint(f\"{'='*60}\")\n\nprint(\"\\nMatrice reconstruite:\")\nfor i, user in enumerate(users):\n    print(f\"{user:8s}: {ratings_reconstructed[i]}\")\n\n# Pr√©dictions pour les positions masqu√©es\nprint(f\"\\n{'='*60}\")\nprint(\"Pr√©dictions des notes manquantes\")\nprint(f\"{'='*60}\")\n\nerrors = []\nfor i, j in test_positions:\n    predicted = ratings_reconstructed[i, j]\n    actual = true_values[(i, j)]\n    error = abs(predicted - actual)\n    errors.append(error)\n    \n    print(f\"{users[i]:8s} √ó {movies[j]:8s}:\")\n    print(f\"  Pr√©diction: {predicted:.2f}\")\n    print(f\"  Vraie note: {actual:.0f}\")\n    print(f\"  Erreur: {error:.2f}\\n\")\n\nmae = np.mean(errors)\nprint(f\"Erreur Absolue Moyenne (MAE): {mae:.2f}\")\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Matrice originale (avec masquage)\nim1 = axes[0].imshow(ratings, cmap='YlOrRd', vmin=0, vmax=5)\naxes[0].set_title('Matrice Originale (avec valeurs masqu√©es)')\naxes[0].set_xlabel('Films')\naxes[0].set_ylabel('Utilisateurs')\naxes[0].set_xticks(range(len(movies)))\naxes[0].set_xticklabels(movies, rotation=45, ha='right')\naxes[0].set_yticks(range(len(users)))\naxes[0].set_yticklabels(users)\nplt.colorbar(im1, ax=axes[0], label='Note')\n\n# Marquer les positions masqu√©es\nfor i, j in test_positions:\n    axes[0].add_patch(plt.Rectangle((j-0.4, i-0.4), 0.8, 0.8, \n                                     fill=False, edgecolor='blue', linewidth=2))\n\n# Matrice reconstruite\nim2 = axes[1].imshow(ratings_reconstructed, cmap='YlOrRd', vmin=0, vmax=5)\naxes[1].set_title('Matrice Reconstruite (SVD rang 2)')\naxes[1].set_xlabel('Films')\naxes[1].set_ylabel('Utilisateurs')\naxes[1].set_xticks(range(len(movies)))\naxes[1].set_xticklabels(movies, rotation=45, ha='right')\naxes[1].set_yticks(range(len(users)))\naxes[1].set_yticklabels(users)\nplt.colorbar(im2, ax=axes[1], label='Note')\n\n# Marquer les pr√©dictions\nfor i, j in test_positions:\n    axes[1].add_patch(plt.Rectangle((j-0.4, i-0.4), 0.8, 0.8, \n                                     fill=False, edgecolor='green', linewidth=2))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n‚úì SVD capture les pr√©f√©rences latentes (action vs com√©die vs drame)\")\nprint(\"‚úì Rang 2 = 2 facteurs cach√©s qui expliquent les go√ªts\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 7.1 : Syst√®mes de Recommandation Simplifi√©",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 7Ô∏è‚É£ Section 7 : Applications Int√©gr√©es - Solutions",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 6.6 : PCA sur Donn√©es Financi√®res",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 6.5\n\nfrom sklearn.datasets import load_digits\n\n# Charger le dataset\ndigits = load_digits()\nX = digits.data  # 1797 images √ó 64 pixels (8√ó8)\ny = digits.target\n\nprint(\"Dataset Digits:\")\nprint(f\"  Nombre d'images: {X.shape[0]}\")\nprint(f\"  Dimensions originales: {X.shape[1]} (images 8√ó8 aplaties)\")\nprint(f\"  Classes: {len(np.unique(y))} (chiffres 0-9)\\n\")\n\n# PCA\nmean = np.mean(X, axis=0)\nX_centered = X - mean\n\n# Via SVD\nU, s, Vt = np.linalg.svd(X_centered, full_matrices=False)\ncomponents = Vt.T  # Composantes principales\n\n# Variance expliqu√©e\nvariance = (s ** 2) / (X.shape[0] - 1)\nvariance_ratio = variance / np.sum(variance)\n\nprint(\"Variance expliqu√©e (premi√®res composantes):\")\nfor n_comp in [2, 5, 10, 20]:\n    var_cum = np.sum(variance_ratio[:n_comp])\n    print(f\"  {n_comp:2d} composantes: {var_cum*100:.2f}%\")\n\n# Nombres de composantes √† tester\nn_components_list = [2, 5, 10, 20]\n\n# Choisir un chiffre pour visualisation\nidx = 0  # Premier chiffre du dataset\noriginal_digit = X[idx].reshape(8, 8)\n\nprint(f\"\\nChiffre s√©lectionn√©: {y[idx]}\")\n\n# Visualisation\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\naxes = axes.flatten()\n\n# Original\naxes[0].imshow(original_digit, cmap='gray')\naxes[0].set_title(f'Original\\n64 dimensions', fontsize=10)\naxes[0].axis('off')\n\n# Reconstructions\nfor i, n_comp in enumerate(n_components_list, start=1):\n    # Projection dans l'espace r√©duit\n    X_pca = X_centered[idx] @ components[:, :n_comp]\n    \n    # Reconstruction\n    X_reconstructed = X_pca @ components[:, :n_comp].T + mean\n    X_reconstructed_img = X_reconstructed.reshape(8, 8)\n    \n    # Erreur\n    mse = np.mean((X[idx] - X_reconstructed) ** 2)\n    var_explained = np.sum(variance_ratio[:n_comp])\n    \n    # Affichage\n    axes[i].imshow(X_reconstructed_img, cmap='gray')\n    axes[i].set_title(\n        f'{n_comp} composantes\\n'\n        f'MSE: {mse:.2f}\\n'\n        f'Var: {var_explained*100:.1f}%',\n        fontsize=9\n    )\n    axes[i].axis('off')\n\n# Cacher le dernier subplot\naxes[5].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Analyse quantitative sur tous les chiffres\nprint(\"\\n\" + \"=\"*60)\nprint(\"Analyse quantitative (moyenne sur tous les chiffres)\")\nprint(\"=\"*60)\n\nfor n_comp in n_components_list:\n    # Projection\n    X_pca = X_centered @ components[:, :n_comp]\n    \n    # Reconstruction\n    X_reconstructed = X_pca @ components[:, :n_comp].T + mean\n    \n    # Erreur moyenne\n    mse = np.mean((X - X_reconstructed) ** 2)\n    \n    # Variance expliqu√©e\n    var_explained = np.sum(variance_ratio[:n_comp])\n    \n    # Taux de compression\n    compression = n_comp / 64\n    \n    print(f\"\\n{n_comp} composantes:\")\n    print(f\"  MSE moyenne: {mse:.2f}\")\n    print(f\"  Variance expliqu√©e: {var_explained*100:.2f}%\")\n    print(f\"  Taux de compression: {compression*100:.1f}% ({n_comp}/64)\")\n    print(f\"  √âconomie de stockage: {(1-compression)*100:.1f}%\")\n\n# Graphique MSE vs nombre de composantes\nmse_values = []\nfor n_comp in range(1, 65):\n    X_pca = X_centered @ components[:, :n_comp]\n    X_reconstructed = X_pca @ components[:, :n_comp].T + mean\n    mse = np.mean((X - X_reconstructed) ** 2)\n    mse_values.append(mse)\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 65), mse_values, linewidth=2)\nplt.xlabel('Nombre de Composantes')\nplt.ylabel('MSE Moyenne')\nplt.title('Erreur de Reconstruction vs Nombre de Composantes')\nplt.grid(True, alpha=0.3)\nplt.axvline(x=20, color='r', linestyle='--', alpha=0.5, label='20 composantes')\nplt.legend()\nplt.show()\n\nprint(\"\\n‚úì Avec 20 composantes (31% des donn√©es), on reconstruit fid√®lement les chiffres !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 6.5 : Reconstruction avec PCA",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 6.4\n\nnp.random.seed(123)\nX = np.random.randn(50, 4)\n\nprint(\"Donn√©es: 50 √©chantillons √ó 4 features\")\nprint(f\"Shape: {X.shape}\\n\")\n\n# Centrer les donn√©es\nmean = np.mean(X, axis=0)\nX_centered = X - mean\n\nprint(\"=\"*60)\nprint(\"M√©thode 1: PCA classique (via covariance)\")\nprint(\"=\"*60)\n\n# Matrice de covariance\ncov = np.cov(X_centered.T)\nprint(f\"\\nMatrice de covariance (4√ó4):\\n{cov}\\n\")\n\n# Valeurs propres et vecteurs propres\neigenvalues_pca, eigenvectors_pca = np.linalg.eig(cov)\n\n# Tri d√©croissant\nidx = np.argsort(eigenvalues_pca)[::-1]\neigenvalues_pca = eigenvalues_pca[idx]\neigenvectors_pca = eigenvectors_pca[:, idx]\n\nprint(f\"Valeurs propres (PCA):\")\nprint(eigenvalues_pca)\nprint(f\"\\nVecteurs propres (PCA) - premi√®re composante:\")\nprint(eigenvectors_pca[:, 0])\n\n# Variance expliqu√©e\nvariance_pca = eigenvalues_pca / np.sum(eigenvalues_pca)\nprint(f\"\\nVariance expliqu√©e (PCA):\")\nfor i in range(4):\n    print(f\"  PC{i+1}: {variance_pca[i]*100:.2f}%\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"M√©thode 2: PCA via SVD\")\nprint(\"=\"*60)\n\n# SVD de X_centered\nU, s, Vt = np.linalg.svd(X_centered, full_matrices=False)\n\nprint(f\"\\nValeurs singuli√®res:\")\nprint(s)\n\n# Les composantes principales sont dans V (colonnes de Vt.T)\neigenvectors_svd = Vt.T\nprint(f\"\\nVecteurs propres (SVD) - premi√®re composante:\")\nprint(eigenvectors_svd[:, 0])\n\n# Variances = s¬≤ / (n-1)\nn = X_centered.shape[0]\neigenvalues_svd = (s ** 2) / (n - 1)\nprint(f\"\\nValeurs propres (SVD): Œª = œÉ¬≤ / (n-1)\")\nprint(eigenvalues_svd)\n\n# Variance expliqu√©e\nvariance_svd = eigenvalues_svd / np.sum(eigenvalues_svd)\nprint(f\"\\nVariance expliqu√©e (SVD):\")\nfor i in range(4):\n    print(f\"  PC{i+1}: {variance_svd[i]*100:.2f}%\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Comparaison des deux m√©thodes\")\nprint(\"=\"*60)\n\n# Comparaison des valeurs propres\nprint(f\"\\nValeurs propres:\")\nprint(f\"  PCA: {eigenvalues_pca}\")\nprint(f\"  SVD: {eigenvalues_svd}\")\nprint(f\"  Match? {np.allclose(eigenvalues_pca, eigenvalues_svd)} ‚úì\")\n\n# Comparaison des composantes principales\n# Note: les vecteurs propres peuvent diff√©rer par un signe\nprint(f\"\\nPremi√®re composante principale:\")\nprint(f\"  PCA: {eigenvectors_pca[:, 0]}\")\nprint(f\"  SVD: {eigenvectors_svd[:, 0]}\")\n# V√©rifier √©galit√© ou opposition\nmatch_direct = np.allclose(eigenvectors_pca[:, 0], eigenvectors_svd[:, 0])\nmatch_opposite = np.allclose(eigenvectors_pca[:, 0], -eigenvectors_svd[:, 0])\nprint(f\"  Match (m√™me direction ou oppos√©e)? {match_direct or match_opposite} ‚úì\")\n\nprint(f\"\\nVariance expliqu√©e:\")\nprint(f\"  PCA: {variance_pca}\")\nprint(f\"  SVD: {variance_svd}\")\nprint(f\"  Match? {np.allclose(variance_pca, variance_svd)} ‚úì\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Conclusion\")\nprint(\"=\"*60)\nprint(\"PCA et SVD donnent les M√äMES r√©sultats !\")\nprint(\"\\nAvantages de SVD:\")\nprint(\"  ‚Ä¢ Plus stable num√©riquement\")\nprint(\"  ‚Ä¢ Plus rapide (pas besoin de calculer la covariance)\")\nprint(\"  ‚Ä¢ Fonctionne m√™me si n < p (plus d'√©chantillons que de features)\")\nprint(\"\\nFormule cl√©: Pour donn√©es centr√©es X,\")\nprint(\"  ‚Ä¢ Cov(X) = X·µÄX / (n-1)\")\nprint(\"  ‚Ä¢ SVD: X = U¬∑Œ£¬∑V·µÄ\")\nprint(\"  ‚Ä¢ Alors: Cov(X) = V¬∑(Œ£¬≤/(n-1))¬∑V·µÄ\")\nprint(\"  ‚Üí Les colonnes de V sont les composantes principales\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 6.4 : PCA vs SVD",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 6.2 : Variance Expliqu√©e",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 4.5 : Matrices Sym√©triques",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 4.4\n\nA = np.array([[0.8, 0.2], [0.3, 0.7]])\n\nprint(\"Calcul de A‚Åµ‚Å∞ par diagonalisation\\n\")\nprint(f\"Matrice A (matrice de transition):\\n{A}\\n\")\n\n# M√©thode 1 : Calcul direct (inefficace pour grands n)\nprint(\"M√©thode 1 : Calcul direct\")\nimport time\nstart = time.time()\nA_50_direct = np.linalg.matrix_power(A, 50)\ntime_direct = time.time() - start\nprint(f\"Temps: {time_direct*1000:.2f}ms\\n\")\n\n# M√©thode 2 : Via diagonalisation (efficace !)\nprint(\"M√©thode 2 : Via diagonalisation (A = P¬∑D¬∑P‚Åª¬π)\")\nstart = time.time()\n\n# Diagonalisation\neigenvalues, P = np.linalg.eig(A)\nD = np.diag(eigenvalues)\nP_inv = np.linalg.inv(P)\n\nprint(f\"Valeurs propres: {eigenvalues}\")\nprint(f\"\\nSi A = P¬∑D¬∑P‚Åª¬π, alors A‚Åµ‚Å∞ = P¬∑D‚Åµ‚Å∞¬∑P‚Åª¬π\")\nprint(f\"Et D‚Åµ‚Å∞ = diag(Œª‚ÇÅ‚Åµ‚Å∞, Œª‚ÇÇ‚Åµ‚Å∞)\\n\")\n\n# Calcul de D^50\nD_50 = np.diag(eigenvalues ** 50)\nA_50_eigen = P @ D_50 @ P_inv\n\ntime_eigen = time.time() - start\nprint(f\"Temps: {time_eigen*1000:.2f}ms\")\nprint(f\"Acc√©l√©ration: {time_direct/time_eigen:.1f}x\\n\")\n\nprint(f\"A‚Åµ‚Å∞ =\\n{A_50_eigen}\\n\")\n\n# V√©rification\nprint(f\"Diff√©rence entre les deux m√©thodes:\")\nprint(f\"||A‚Åµ‚Å∞_direct - A‚Åµ‚Å∞_eigen|| = {np.linalg.norm(A_50_direct - A_50_eigen):.2e}\")\nprint(f\"‚úì Les deux m√©thodes donnent le m√™me r√©sultat\\n\")\n\n# Interpr√©tation (matrice de transition/cha√Æne de Markov)\nprint(\"üìä Interpr√©tation (Cha√Æne de Markov):\")\nprint(\"A est une matrice stochastique (chaque colonne somme √† 1)\")\nprint(f\"Œª‚ÇÅ = {eigenvalues[0]:.4f} (proche de 1)\")\nprint(f\"Œª‚ÇÇ = {eigenvalues[1]:.4f} (< 1)\")\nprint(f\"\\nApr√®s 50 it√©rations, le syst√®me converge vers un √©tat stable:\")\nprint(\"Chaque colonne de A‚Åµ‚Å∞ ‚âà m√™me vecteur = distribution stationnaire\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 4.4 : Puissance Matricielle",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 5.5\n\n# Cr√©er une \"image\" 20√ó20 avec un motif (gradient)\nn = 20\nx = np.linspace(0, 1, n)\ny = np.linspace(0, 1, n)\nX, Y = np.meshgrid(x, y)\n\n# Motif: Gradient circulaire\nimage = np.sqrt((X - 0.5)**2 + (Y - 0.5)**2)\nimage = 1 - image / image.max()  # Inverser pour avoir du blanc au centre\n\nprint(\"Image originale (20√ó20):\")\nprint(f\"Taille: {image.shape}\")\nprint(f\"Valeurs min/max: [{image.min():.2f}, {image.max():.2f}]\\n\")\n\n# SVD\nU, s, Vt = np.linalg.svd(image, full_matrices=False)\nprint(f\"Valeurs singuli√®res (top 5): {s[:5]}\\n\")\n\n# Rangs √† tester\nranks = [1, 3, 5, 10, 20]\n\n# Visualisation\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\n# Image originale\naxes[0].imshow(image, cmap='gray')\naxes[0].set_title('Original (rang 20)', fontsize=12)\naxes[0].axis('off')\n\n# Reconstructions\nfor idx, k in enumerate(ranks[:-1], start=1):\n    # Approximation rang k\n    U_k = U[:, :k]\n    s_k = s[:k]\n    Vt_k = Vt[:k, :]\n    \n    image_k = U_k @ np.diag(s_k) @ Vt_k\n    \n    # Erreur\n    error = np.linalg.norm(image - image_k, 'fro')\n    error_rel = error / np.linalg.norm(image, 'fro')\n    \n    # Taux de compression\n    # Original: 20√ó20 = 400 √©l√©ments\n    # Rang k: k√ó20 (U_k) + k (s_k) + k√ó20 (Vt_k) = k√ó41\n    storage_original = 20 * 20\n    storage_compressed = k * (20 + 1 + 20)\n    compression_ratio = storage_compressed / storage_original\n    \n    # Affichage\n    axes[idx].imshow(image_k, cmap='gray')\n    axes[idx].set_title(\n        f'Rang {k}\\n'\n        f'Erreur: {error_rel*100:.1f}%\\n'\n        f'Stockage: {compression_ratio*100:.0f}%',\n        fontsize=10\n    )\n    axes[idx].axis('off')\n    \n    print(f\"Rang {k:2d}:\")\n    print(f\"  Erreur relative: {error_rel*100:.2f}%\")\n    print(f\"  Taux de stockage: {compression_ratio*100:.1f}% ({storage_compressed}/{storage_original})\")\n    print(f\"  √âconomie: {(1-compression_ratio)*100:.1f}%\\n\")\n\n# Cacher le dernier subplot\naxes[5].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Graphique de l'erreur vs rang\nerrors = []\nfor k in range(1, 21):\n    image_k = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n    error_rel = np.linalg.norm(image - image_k, 'fro') / np.linalg.norm(image, 'fro')\n    errors.append(error_rel * 100)\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 21), errors, 'o-', linewidth=2, markersize=6)\nplt.xlabel('Rang k')\nplt.ylabel('Erreur Relative (%)')\nplt.title('Erreur de Reconstruction vs Rang')\nplt.grid(True, alpha=0.3)\nplt.axhline(y=5, color='r', linestyle='--', alpha=0.5, label='Seuil 5%')\nplt.legend()\nplt.show()\n\nprint(\"\\n‚úì Avec rang 5, l'erreur est < 5% mais on √©conomise 49% de stockage !\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 5.5 : Compression d'Image Simple",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 4.2 : Propri√©t√©s des Valeurs Propres",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Solution 3.4\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[1, 1], [1, 1]])\n\n# Calcul des rangs\nrank_A = np.linalg.matrix_rank(A)\nrank_B = np.linalg.matrix_rank(B)\nAB = A @ B\nrank_AB = np.linalg.matrix_rank(AB)\n\nprint(f\"Matrice A (2√ó2):\\n{A}\\n\")\nprint(f\"Matrice B (2√ó2):\\n{B}\\n\")\nprint(f\"Produit AB:\\n{AB}\\n\")\n\nprint(f\"Rang(A) = {rank_A}\")\nprint(f\"Rang(B) = {rank_B}\")\nprint(f\"Rang(AB) = {rank_AB}\\n\")\n\n# V√©rification de l'in√©galit√©\nmin_rank = min(rank_A, rank_B)\nprint(f\"Th√©or√®me: rang(AB) ‚â§ min(rang(A), rang(B))\")\nprint(f\"V√©rification: {rank_AB} ‚â§ {min_rank} ‚Üí {rank_AB <= min_rank} ‚úì\\n\")\n\n# Explication intuitive\nprint(\"üìä Interpr√©tation:\")\nprint(\"Le produit AB ne peut pas avoir plus d'information\")\nprint(\"que n'en contient A ou B individuellement.\")\nprint(f\"\\nIci: B a rang {rank_B} (1 seule direction ind√©pendante)\")\nprint(f\"Donc AB ne peut avoir au plus que rang {rank_B}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 3.4 : Rang d'un Produit Matriciel",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Section 4 : Valeurs Propres - Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.1 : Calcul des Valeurs Propres\n",
    "\n",
    "A = np.array([[3, 1], [1, 3]])\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Matrice A:\")\n",
    "print(A)\n",
    "print(f\"\\nValeurs propres: {eigenvalues}\")\n",
    "print(f\"\\nVecteurs propres:\")\n",
    "for i in range(len(eigenvalues)):\n",
    "    v = eigenvectors[:, i]\n",
    "    Œª = eigenvalues[i]\n",
    "    print(f\"\\nŒª{i+1} = {Œª:.4f}, v{i+1} = {v}\")\n",
    "    \n",
    "    # V√©rification\n",
    "    Av = A @ v\n",
    "    Œªv = Œª * v\n",
    "    print(f\"A*v = {Av}\")\n",
    "    print(f\"Œª*v = {Œªv}\")\n",
    "    print(f\"‚úì Match: {np.allclose(Av, Œªv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4.3 : Diagonalisation\n",
    "\n",
    "A = np.array([[2, 1], [1, 2]])\n",
    "Œª, P = np.linalg.eig(A)\n",
    "D = np.diag(Œª)\n",
    "\n",
    "print(\"Diagonalisation de A:\")\n",
    "print(f\"\\nP (vecteurs propres):\\n{P}\")\n",
    "print(f\"\\nD (valeurs propres):\\n{D}\")\n",
    "\n",
    "# Reconstruction\n",
    "A_reconstructed = P @ D @ np.linalg.inv(P)\n",
    "print(f\"\\nP @ D @ P‚Åª¬π =\\n{A_reconstructed}\")\n",
    "print(f\"\\nA original =\\n{A}\")\n",
    "print(f\"\\n‚úì A = P @ D @ P‚Åª¬π : {np.allclose(A, A_reconstructed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Section 5 : SVD - Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.1 : SVD Basique\n",
    "\n",
    "A = np.array([[3, 1, 1], [1, 3, 1]])\n",
    "U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "print(\"Matrice A:\")\n",
    "print(A)\n",
    "print(f\"\\nU (vecteurs singuliers gauches):\\n{U}\")\n",
    "print(f\"\\nValeurs singuli√®res: {s}\")\n",
    "print(f\"\\nV·µÄ (vecteurs singuliers droits transpos√©s):\\n{Vt}\")\n",
    "\n",
    "# Reconstruction\n",
    "Sigma = np.zeros((U.shape[1], Vt.shape[0]))\n",
    "Sigma[:len(s), :len(s)] = np.diag(s)\n",
    "A_reconstructed = U @ Sigma @ Vt\n",
    "\n",
    "print(f\"\\nReconstruction U @ Œ£ @ V·µÄ:\\n{A_reconstructed}\")\n",
    "print(f\"\\n‚úì Match: {np.allclose(A, A_reconstructed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution 5.2 : Approximation de Rang k\n\nnp.random.seed(42)\nA = np.random.randn(10, 10)\nU, s, Vt = np.linalg.svd(A)\n\nranks = [1, 2, 3, 5, 10]\nerrors = []\nnorm_A = np.linalg.norm(A, 'fro')\n\nfor k in ranks:\n    # Approximation rang k\n    Sigma_k = np.zeros_like(A)\n    Sigma_k[:k, :k] = np.diag(s[:k])\n    A_k = U @ Sigma_k @ Vt\n    \n    # Erreur relative\n    error = np.linalg.norm(A - A_k, 'fro') / norm_A\n    errors.append(error)\n    print(f\"Rang {k:2d}: Erreur relative = {error*100:.2f}%\")\n\n# Graphique\nplt.figure(figsize=(8, 5))\nplt.plot(ranks, errors, 'o-', linewidth=2, markersize=8)\nplt.xlabel('Rang k')\nplt.ylabel('Erreur Relative')\nplt.title('Erreur d\\'Approximation vs Rang')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Solution 5.6 : Approximation de Rang Faible\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Solution 5.6 : Approximation de Rang Faible\")\nprint(\"=\"*60 + \"\\n\")\n\nnp.random.seed(42)\nA = np.random.randn(5, 5)\nU, s, Vt = np.linalg.svd(A)\n\nprint(f\"Matrice originale (5√ó5):\\n{A}\\n\")\nprint(f\"Valeurs singuli√®res: {s}\\n\")\n\nfor k in [1, 2, 3]:\n    # Construire Œ£_k\n    Sigma_k = np.zeros((5, 5))\n    Sigma_k[:k, :k] = np.diag(s[:k])\n    \n    # Approximation\n    A_k = U @ Sigma_k @ Vt\n    \n    # Erreur\n    error = np.linalg.norm(A - A_k, 'fro')\n    error_rel = error / np.linalg.norm(A, 'fro')\n    \n    print(f\"Rang {k}:\")\n    print(f\"  Erreur Frobenius = {error:.4f}\")\n    print(f\"  Erreur relative = {error_rel*100:.2f}%\")\n    print(f\"  Taux de compression = {(k*5 + k*5 + k)/(5*5)}%\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Section 6 : PCA - Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 6.1 : PCA √† la Main\n",
    "\n",
    "X = np.array([[2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n",
    "\n",
    "# √âtape 1 : Centrer\n",
    "mean = np.mean(X, axis=0)\n",
    "X_centered = X - mean\n",
    "print(f\"Moyenne: {mean}\")\n",
    "print(f\"Donn√©es centr√©es:\\n{X_centered}\")\n",
    "\n",
    "# √âtape 2 : Covariance\n",
    "cov = np.cov(X_centered.T)\n",
    "print(f\"\\nMatrice de covariance:\\n{cov}\")\n",
    "\n",
    "# √âtape 3 : Eigen\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"\\nValeurs propres: {eigenvalues}\")\n",
    "print(f\"Vecteurs propres:\\n{eigenvectors}\")\n",
    "\n",
    "# √âtape 4 : Projection sur PC1\n",
    "pc1 = eigenvectors[:, 0]\n",
    "X_pca = X_centered @ pc1\n",
    "print(f\"\\nProjection sur PC1: {X_pca}\")\n",
    "\n",
    "# Variance expliqu√©e\n",
    "variance_explained = eigenvalues[0] / np.sum(eigenvalues)\n",
    "print(f\"\\nVariance expliqu√©e par PC1: {variance_explained*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 6.3 : PCA pour Visualisation\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# G√©n√©rer des clusters en 5D\n",
    "X, y = make_blobs(n_samples=300, n_features=5, centers=3, random_state=42)\n",
    "\n",
    "# PCA\n",
    "mean = np.mean(X, axis=0)\n",
    "X_centered = X - mean\n",
    "cov = np.cov(X_centered.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# Projection sur 2 composantes\n",
    "components = eigenvectors[:, :2]\n",
    "X_pca = X_centered @ components\n",
    "\n",
    "# Variance expliqu√©e\n",
    "variance_ratio = eigenvalues[:2] / np.sum(eigenvalues)\n",
    "print(f\"Variance expliqu√©e:\")\n",
    "print(f\"  PC1: {variance_ratio[0]*100:.2f}%\")\n",
    "print(f\"  PC2: {variance_ratio[1]*100:.2f}%\")\n",
    "print(f\"  Total: {np.sum(variance_ratio)*100:.2f}%\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(3):\n",
    "    plt.scatter(X_pca[y==i, 0], X_pca[y==i, 1], \n",
    "                label=f'Cluster {i}', alpha=0.6, s=50)\n",
    "plt.xlabel(f'PC1 ({variance_ratio[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({variance_ratio[1]*100:.1f}%)')\n",
    "plt.title('Clusters visualis√©s apr√®s PCA (5D ‚Üí 2D)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Les 3 clusters sont bien s√©par√©s m√™me en 2D !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Toutes les Solutions Compl√®tes !\n",
    "\n",
    "Tu as maintenant toutes les solutions des exercices d'alg√®bre lin√©aire avanc√©e.\n",
    "\n",
    "### üí° Points Cl√©s √† Retenir :\n",
    "\n",
    "1. **Rang** = Mesure de l'information ind√©pendante\n",
    "2. **Eigenvalues** = Directions importantes d'une transformation\n",
    "3. **SVD** = D√©composition universelle pour toutes matrices\n",
    "4. **PCA** = R√©duction de dimension bas√©e sur la variance\n",
    "\n",
    "### üöÄ Prochaine √âtape :\n",
    "\n",
    "**Projet Final** : `projet_04_compression_svd.ipynb`\n",
    "\n",
    "Applique la SVD pour compresser de vraies images ! üé®\n",
    "\n",
    "---\n",
    "\n",
    "**Continue √† pratiquer et exp√©rimenter ! C'est comme √ßa qu'on devient expert ! üí™**"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Solution 5.4 : Pseudo-Inverse avec SVD\n\nA = np.array([[1, 2], [2, 4]])  # Singuli√®re (colonne 2 = 2*colonne 1)\n\nprint(f\"Matrice A:\\n{A}\\n\")\nprint(f\"Det(A) = {np.linalg.det(A):.6f} (singuli√®re)\\n\")\n\n# SVD\nU, s, Vt = np.linalg.svd(A, full_matrices=False)\n\nprint(f\"Valeurs singuli√®res: {s}\")\nprint(f\"Note: œÉ‚ÇÇ ‚âà 0 (pr√®s de singularit√©)\\n\")\n\n# Pseudo-inverse manuelle\nthreshold = 1e-10\nSigma_plus = np.zeros((Vt.shape[0], U.shape[1]))\nfor i in range(len(s)):\n    if s[i] > threshold:\n        Sigma_plus[i, i] = 1 / s[i]\n\nA_pinv_manual = Vt.T @ Sigma_plus @ U.T\n\n# Comparaison avec np.linalg.pinv()\nA_pinv_numpy = np.linalg.pinv(A)\n\nprint(f\"A‚Å∫ (calcul manuel):\\n{A_pinv_manual}\\n\")\nprint(f\"A‚Å∫ (np.linalg.pinv()):\\n{A_pinv_numpy}\\n\")\nprint(f\"Match: {np.allclose(A_pinv_manual, A_pinv_numpy)}\\n\")\n\n# Propri√©t√©s int√©ressantes de la pseudo-inverse\nprint(\"Propri√©t√©s de la pseudo-inverse:\")\nprint(f\"A @ A‚Å∫ @ A ‚âà A: {np.allclose(A @ A_pinv_numpy @ A, A)}\")\nprint(f\"A‚Å∫ @ A @ A‚Å∫ ‚âà A‚Å∫: {np.allclose(A_pinv_numpy @ A @ A_pinv_numpy, A_pinv_numpy)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Solution 5.4 : Pseudo-Inverse avec SVD\n\n**Note Conceptuelle**: La pseudo-inverse de Moore-Penrose $A^+$ g√©n√©ralise l'inverse pour:\n- Matrices non-carr√©es (m ‚â† n)\n- Matrices singuli√®res (det(A) = 0)\n\nElle se calcule via SVD: $A^+ = V \\cdot \\Sigma^+ \\cdot U^T$ o√π $\\Sigma^+$ inverse les valeurs singuli√®res non-nulles.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}