{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Exercices - Calcul DiffÃ©rentiel\n",
    "\n",
    "Bienvenue dans les exercices de calcul diffÃ©rentiel ! ðŸŽ¯\n",
    "\n",
    "## ðŸ“ Instructions GÃ©nÃ©rales\n",
    "\n",
    "- **Ã‰crivez votre code Python** dans les cellules vides prÃ©vues\n",
    "- **Utilisez `print()`** pour afficher vos rÃ©sultats\n",
    "- **ExÃ©cutez avec `Shift + Enter`** pour vÃ©rifier\n",
    "- **Utilisez SymPy** pour le calcul symbolique : `import sympy as sp`\n",
    "- **Utilisez NumPy** pour le calcul numÃ©rique : `import numpy as np`\n",
    "\n",
    "## ðŸŽ¯ Objectifs\n",
    "\n",
    "Ã€ la fin de ces exercices, vous serez capable de :\n",
    "- âœ… Calculer des limites\n",
    "- âœ… Calculer des dÃ©rivÃ©es avec les rÃ¨gles de dÃ©rivation\n",
    "- âœ… Calculer des dÃ©rivÃ©es partielles\n",
    "- âœ… Calculer et interprÃ©ter le gradient\n",
    "- âœ… Appliquer la rÃ¨gle de la chaÃ®ne\n",
    "- âœ… Trouver des minima/maxima par optimisation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Section 1 : Limites (Ex 1.1-1.5)\n",
    "\n",
    "Comprendre les limites et la continuitÃ©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.1 : Limites Simples\n",
    "\n",
    "**Consigne :** Calculez les limites suivantes avec SymPy :\n",
    "\n",
    "1. $\\lim_{x \\to 3} (2x + 5)$\n",
    "2. $\\lim_{x \\to 2} (x^2 - 4)$\n",
    "3. $\\lim_{x \\to 0} \\frac{\\sin(x)}{x}$\n",
    "4. $\\lim_{x \\to \\infty} \\frac{1}{x}$\n",
    "\n",
    "Utilisez `sp.limit(f, x, a)` oÃ¹ `a` est la valeur vers laquelle tend `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1.1 : Ã‰crivez votre code ici\n",
    "import sympy as sp\n",
    "\n",
    "# INDICE : import sympy as sp\n",
    "# x = sp.Symbol('x')\n",
    "# resultat = sp.limit(expression, x, valeur)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.2 : Limites LatÃ©rales\n",
    "\n",
    "**Consigne :** Pour $f(x) = \\frac{|x|}{x}$, calculez :\n",
    "\n",
    "1. $\\lim_{x \\to 0^+} f(x)$ (limite Ã  droite)\n",
    "2. $\\lim_{x \\to 0^-} f(x)$ (limite Ã  gauche)\n",
    "3. La fonction est-elle continue en $x = 0$ ?\n",
    "\n",
    "Utilisez `sp.limit(f, x, 0, '+')` pour la limite Ã  droite et `'-'` pour la limite Ã  gauche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1.2 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : f(x) = |x| / x = Abs(x) / x en SymPy\n",
    "# limite_droite = sp.limit(f, x, 0, '+')\n",
    "# limite_gauche = sp.limit(f, x, 0, '-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.3 : ContinuitÃ©\n",
    "\n",
    "**Consigne :** Pour chaque fonction, dÃ©terminez si elle est continue en $x = 0$ :\n",
    "\n",
    "1. $f(x) = x^2$\n",
    "2. $g(x) = \\frac{1}{x}$\n",
    "3. $h(x) = \\begin{cases} x & \\text{si } x \\neq 0 \\\\ 1 & \\text{si } x = 0 \\end{cases}$\n",
    "\n",
    "Une fonction est continue en $a$ si $\\lim_{x \\to a} f(x) = f(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1.3 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Calculez la limite et comparez avec f(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.4 : Approximation NumÃ©rique de Limite\n",
    "\n",
    "**Consigne :** Pour $f(x) = \\frac{e^x - 1}{x}$, approchez numÃ©riquement $\\lim_{x \\to 0} f(x)$ en calculant $f(x)$ pour $x = 0.1, 0.01, 0.001, 0.0001$.\n",
    "\n",
    "Comparez avec le rÃ©sultat symbolique de SymPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1.4 : Ã‰crivez votre code ici\n",
    "import numpy as np\n",
    "\n",
    "# INDICE : DÃ©finissez f(x) = (e^x - 1) / x\n",
    "# Calculez pour x dÃ©croissant vers 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.5 : Limite et Forme IndÃ©terminÃ©e\n",
    "\n",
    "**Consigne :** Calculez $\\lim_{x \\to 0} \\frac{x^2 - 4x}{x}$ :\n",
    "\n",
    "1. Directement (vous obtiendrez 0/0)\n",
    "2. En simplifiant d'abord algÃ©briquement\n",
    "3. Avec SymPy (qui simplifie automatiquement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 1.5 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Simplifiez (xÂ² - 4x) / x = x - 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“ˆ Section 2 : DÃ©rivÃ©es Basiques (Ex 2.1-2.6)\n",
    "\n",
    "Calculer des dÃ©rivÃ©es avec la dÃ©finition et les propriÃ©tÃ©s de base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.1 : DÃ©rivÃ©e avec la DÃ©finition\n",
    "\n",
    "**Consigne :** Calculez la dÃ©rivÃ©e de $f(x) = x^2$ en utilisant la **dÃ©finition** :\n",
    "\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n",
    "\n",
    "1. DÃ©veloppez $(x + h)^2$\n",
    "2. Calculez $f(x + h) - f(x)$\n",
    "3. Divisez par $h$\n",
    "4. Prenez la limite quand $h \\to 0$\n",
    "\n",
    "Utilisez SymPy pour vÃ©rifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2.1 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : \n",
    "# h = sp.Symbol('h')\n",
    "# f = x**2\n",
    "# diff_quotient = (f.subs(x, x+h) - f) / h\n",
    "# derivee = sp.limit(diff_quotient, h, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.2 : DÃ©rivÃ©es de PolynÃ´mes\n",
    "\n",
    "**Consigne :** Calculez les dÃ©rivÃ©es des fonctions suivantes avec SymPy :\n",
    "\n",
    "1. $f(x) = 3x^4 - 2x^3 + 5x - 7$\n",
    "2. $g(x) = x^5 + 6x^2 - 9$\n",
    "3. $h(x) = (2x + 1)^3$ (dÃ©veloppez d'abord ou utilisez chain rule)\n",
    "4. $k(x) = \\frac{x^3}{3} - \\frac{x^2}{2} + x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2.2 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : sp.diff(fonction, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.3 : Pente de la Tangente\n",
    "\n",
    "**Consigne :** Pour $f(x) = x^3 - 3x$, trouvez :\n",
    "\n",
    "1. La pente de la tangente en $x = 1$\n",
    "2. La pente de la tangente en $x = -2$\n",
    "3. L'Ã©quation de la tangente en $x = 1$ (forme $y = mx + b$)\n",
    "\n",
    "Rappel : Pente = $f'(x)$, Ã©quation tangente : $y - f(a) = f'(a)(x - a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2.3 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Calculez f'(x), puis Ã©valuez en x=1 et x=-2\n",
    "# Ã‰quation tangente : y = f'(a)*(x - a) + f(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.4 : Visualisation de DÃ©rivÃ©e\n",
    "\n",
    "**Consigne :** Pour $f(x) = \\sin(x)$ :\n",
    "\n",
    "1. Tracez $f(x)$ et $f'(x)$ sur le mÃªme graphique pour $x \\in [-2\\pi, 2\\pi]$\n",
    "2. Observez : oÃ¹ $f'(x) = 0$, que se passe-t-il pour $f(x)$ ?\n",
    "3. OÃ¹ $f'(x) > 0$, comment Ã©volue $f(x)$ ?\n",
    "4. OÃ¹ $f'(x) < 0$, comment Ã©volue $f(x)$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2.4 : Ã‰crivez votre code ici\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# INDICE : f'(sin(x)) = cos(x)\n",
    "# Tracez sin(x) et cos(x) ensemble\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.5 : DÃ©rivÃ©e NumÃ©rique\n",
    "\n",
    "**Consigne :** ImplÃ©mentez une fonction `derivee_numerique(f, x, h)` qui calcule :\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x + h) - f(x)}{h}$$\n",
    "\n",
    "Testez avec $f(x) = e^x$ en $x = 1$ et comparez avec la vraie dÃ©rivÃ©e ($e^1 \\approx 2.718$).\n",
    "\n",
    "ExpÃ©rimentez avec diffÃ©rentes valeurs de $h$ : $0.1, 0.01, 0.001, 0.0001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2.5 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE :\n",
    "# def derivee_numerique(f, x, h=1e-5):\n",
    "#     return (f(x + h) - f(x)) / h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.6 : Gradient Descent Simple\n",
    "\n",
    "**Consigne :** Minimisez $f(x) = (x - 5)^2$ avec Gradient Descent :\n",
    "\n",
    "1. Calculez $f'(x)$\n",
    "2. ImplÃ©mentez l'algorithme : $x_{new} = x_{old} - \\alpha f'(x_{old})$\n",
    "3. Partez de $x = 0$, utilisez $\\alpha = 0.1$\n",
    "4. Faites 20 itÃ©rations\n",
    "5. Affichez la trajectoire\n",
    "\n",
    "Le minimum devrait Ãªtre en $x = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2.6 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : f'(x) = 2(x - 5)\n",
    "# Boucle : x = x - alpha * f_prime(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âš¡ Section 3 : RÃ¨gles de DÃ©rivation (Ex 3.1-3.6)\n",
    "\n",
    "Appliquer les rÃ¨gles : somme, produit, quotient, chaÃ®ne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.1 : RÃ¨gle de la Somme\n",
    "\n",
    "**Consigne :** Appliquez la rÃ¨gle de la somme pour calculer les dÃ©rivÃ©es :\n",
    "\n",
    "1. $f(x) = x^3 + x^2 + x + 1$\n",
    "2. $g(x) = e^x + \\ln(x)$\n",
    "3. $h(x) = \\sin(x) + \\cos(x)$\n",
    "4. $k(x) = 5x^4 - 3x^2 + 7$\n",
    "\n",
    "VÃ©rifiez avec SymPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3.1 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : (f + g)' = f' + g'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.2 : RÃ¨gle du Produit\n",
    "\n",
    "**Consigne :** Appliquez la rÃ¨gle du produit $(fg)' = f'g + fg'$ :\n",
    "\n",
    "1. $f(x) = x^2 \\cdot e^x$\n",
    "2. $g(x) = (x + 1)(x^2 - 3)$\n",
    "3. $h(x) = x \\ln(x)$\n",
    "4. $k(x) = \\sin(x) \\cos(x)$\n",
    "\n",
    "Calculez Ã  la main puis vÃ©rifiez avec SymPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3.2 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : (f*g)' = f'*g + f*g'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.3 : RÃ¨gle du Quotient\n",
    "\n",
    "**Consigne :** Appliquez la rÃ¨gle du quotient $\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}$ :\n",
    "\n",
    "1. $f(x) = \\frac{x^2}{x + 1}$\n",
    "2. $g(x) = \\frac{1}{x^2 + 1}$\n",
    "3. $h(x) = \\frac{e^x}{x}$\n",
    "4. $k(x) = \\frac{\\sin(x)}{x}$\n",
    "\n",
    "Calculez Ã  la main puis vÃ©rifiez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3.3 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : (f/g)' = (f'*g - f*g') / gÂ²\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.4 : RÃ¨gle de la Puissance\n",
    "\n",
    "**Consigne :** Appliquez $(x^n)' = nx^{n-1}$ :\n",
    "\n",
    "1. $f(x) = x^{10}$\n",
    "2. $g(x) = x^{-3}$\n",
    "3. $h(x) = \\sqrt{x} = x^{1/2}$\n",
    "4. $k(x) = \\frac{1}{\\sqrt{x}} = x^{-1/2}$\n",
    "5. $m(x) = x^{\\pi}$ (exposant irrationnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3.4 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : d/dx[x^n] = n*x^(n-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.5 : Combiner les RÃ¨gles\n",
    "\n",
    "**Consigne :** Calculez les dÃ©rivÃ©es (combinez plusieurs rÃ¨gles) :\n",
    "\n",
    "1. $f(x) = \\frac{x^2 + 1}{x - 1}$\n",
    "2. $g(x) = (x^2 + 3x)(2x - 5)$\n",
    "3. $h(x) = \\frac{x e^x}{x^2 + 1}$\n",
    "4. $k(x) = (x + 1)^2 (x - 1)^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3.5 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Identifiez quelle rÃ¨gle appliquer (produit/quotient)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.6 : DÃ©rivÃ©es de Fonctions d'Activation\n",
    "\n",
    "**Consigne :** Calculez les dÃ©rivÃ©es des fonctions d'activation ML :\n",
    "\n",
    "1. **Sigmoid** : $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ â†’ Montrez que $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$\n",
    "2. **Tanh** : $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ â†’ Montrez que $\\tanh'(x) = 1 - \\tanh^2(x)$\n",
    "3. **Softplus** : $f(x) = \\ln(1 + e^x)$ â†’ Calculez $f'(x)$\n",
    "\n",
    "Utilisez SymPy pour simplifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 3.6 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Utilisez sp.simplify() pour simplifier les rÃ©sultats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ§® Section 4 : DÃ©rivÃ©es Partielles (Ex 4.1-4.5)\n",
    "\n",
    "DÃ©river des fonctions Ã  plusieurs variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.1 : DÃ©rivÃ©es Partielles Simples\n",
    "\n",
    "**Consigne :** Pour $f(x, y) = x^2 y + 3xy^2$, calculez :\n",
    "\n",
    "1. $\\frac{\\partial f}{\\partial x}$\n",
    "2. $\\frac{\\partial f}{\\partial y}$\n",
    "3. Ã‰valuez les deux en $(x=2, y=3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4.1 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : \n",
    "# x, y = sp.symbols('x y')\n",
    "# df_dx = sp.diff(f, x)\n",
    "# df_dy = sp.diff(f, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.2 : Fonction Ã  Trois Variables\n",
    "\n",
    "**Consigne :** Pour $f(x, y, z) = x^2 + y^2 + z^2 + xy + yz$, calculez :\n",
    "\n",
    "1. $\\frac{\\partial f}{\\partial x}$\n",
    "2. $\\frac{\\partial f}{\\partial y}$\n",
    "3. $\\frac{\\partial f}{\\partial z}$\n",
    "4. Le gradient $\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\\\ \\frac{\\partial f}{\\partial z} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4.2 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : x, y, z = sp.symbols('x y z')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.3 : DÃ©rivÃ©es Partielles d'Ordre 2\n",
    "\n",
    "**Consigne :** Pour $f(x, y) = x^3 y^2$, calculez :\n",
    "\n",
    "1. $\\frac{\\partial^2 f}{\\partial x^2}$ (dÃ©rivÃ©e seconde par rapport Ã  $x$)\n",
    "2. $\\frac{\\partial^2 f}{\\partial y^2}$ (dÃ©rivÃ©e seconde par rapport Ã  $y$)\n",
    "3. $\\frac{\\partial^2 f}{\\partial x \\partial y}$ (dÃ©rivÃ©e mixte)\n",
    "4. $\\frac{\\partial^2 f}{\\partial y \\partial x}$ (dÃ©rivÃ©e mixte inversÃ©e)\n",
    "5. VÃ©rifiez que $\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x}$ (thÃ©orÃ¨me de Schwarz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4.3 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : sp.diff(f, x, 2) pour dÃ©rivÃ©e seconde\n",
    "# sp.diff(f, x, y) pour dÃ©rivÃ©e mixte\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.4 : Visualisation de Surface 3D\n",
    "\n",
    "**Consigne :** Pour $f(x, y) = x^2 - y^2$ :\n",
    "\n",
    "1. CrÃ©ez une surface 3D de cette fonction\n",
    "2. Calculez $\\frac{\\partial f}{\\partial x}$ et $\\frac{\\partial f}{\\partial y}$\n",
    "3. En $(1, 1)$, quelle est la pente dans la direction $x$ ? Dans la direction $y$ ?\n",
    "4. CrÃ©ez un contour plot 2D avec des vecteurs de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4.4 : Ã‰crivez votre code ici\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# INDICE : Utilisez meshgrid pour crÃ©er la grille\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4.5 : Application ML - Loss Function\n",
    "\n",
    "**Consigne :** Soit la loss function (erreur quadratique) :\n",
    "\n",
    "$$L(w, b) = (y - (wx + b))^2$$\n",
    "\n",
    "Avec $x = 2$, $y = 5$ (donnÃ©es fixes), calculez :\n",
    "\n",
    "1. $\\frac{\\partial L}{\\partial w}$\n",
    "2. $\\frac{\\partial L}{\\partial b}$\n",
    "3. Ã‰valuez en $(w=1, b=0)$\n",
    "4. Ces gradients indiquent-ils d'augmenter ou diminuer $w$ et $b$ pour minimiser $L$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 4.5 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : L = (y - (w*x + b))Â²\n",
    "# Substituez x=2, y=5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Section 5 : Gradient (Ex 5.1-5.6)\n",
    "\n",
    "Calculer et interprÃ©ter le gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.1 : Calculer le Gradient\n",
    "\n",
    "**Consigne :** Pour chaque fonction, calculez le gradient $\\nabla f$ :\n",
    "\n",
    "1. $f(x, y) = x^2 + y^2$\n",
    "2. $g(x, y) = xy$\n",
    "3. $h(x, y, z) = x^2 + 2y^2 + 3z^2$\n",
    "4. $k(x, y) = e^{x+y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5.1 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Gradient = vecteur des dÃ©rivÃ©es partielles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.2 : Norme et Direction du Gradient\n",
    "\n",
    "**Consigne :** Pour $f(x, y) = x^2 + 2y^2$, en $(x=1, y=1)$ :\n",
    "\n",
    "1. Calculez $\\nabla f(1, 1)$\n",
    "2. Calculez la norme $||\\nabla f||$ (longueur du vecteur)\n",
    "3. Normalisez le gradient : $\\frac{\\nabla f}{||\\nabla f||}$ (vecteur unitaire)\n",
    "4. Cette direction indique la montÃ©e la plus rapide. Quelle direction pour la descente ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5.2 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : norme = np.linalg.norm(gradient)\n",
    "# direction descente = -gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.3 : Gradient Descent 2D\n",
    "\n",
    "**Consigne :** Minimisez $f(x, y) = (x - 3)^2 + (y + 2)^2$ avec Gradient Descent :\n",
    "\n",
    "1. Calculez $\\nabla f$\n",
    "2. Partez de $(x, y) = (0, 0)$\n",
    "3. Utilisez $\\alpha = 0.1$\n",
    "4. Faites 30 itÃ©rations\n",
    "5. Tracez la trajectoire sur un contour plot\n",
    "\n",
    "Le minimum devrait Ãªtre en $(3, -2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5.3 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : \n",
    "# gradient = np.array([df_dx, df_dy])\n",
    "# position = position - alpha * gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.4 : Champ de Gradients\n",
    "\n",
    "**Consigne :** Pour $f(x, y) = \\frac{1}{2}(x^2 + y^2)$ :\n",
    "\n",
    "1. CrÃ©ez une grille de points dans $[-3, 3] \\times [-3, 3]$\n",
    "2. Calculez le gradient en chaque point\n",
    "3. Tracez un champ de vecteurs (quiver plot)\n",
    "4. Ajoutez des contours de $f$\n",
    "5. Observez : les gradients sont-ils perpendiculaires aux contours ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5.4 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : plt.quiver(X, Y, U, V) pour champ de vecteurs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.5 : Gradient et Directions DÃ©rivÃ©es\n",
    "\n",
    "**Consigne :** Pour $f(x, y) = x^2 + xy + y^2$ en $(1, 2)$ :\n",
    "\n",
    "1. Calculez $\\nabla f(1, 2)$\n",
    "2. Calculez la dÃ©rivÃ©e directionnelle dans la direction $\\mathbf{u} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ (axe $x$)\n",
    "3. Calculez la dÃ©rivÃ©e directionnelle dans la direction $\\mathbf{u} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ (axe $y$)\n",
    "4. Calculez la dÃ©rivÃ©e directionnelle dans la direction $\\mathbf{u} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ (diagonale)\n",
    "\n",
    "Formule : $D_{\\mathbf{u}}f = \\nabla f \\cdot \\mathbf{u}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5.5 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : DÃ©rivÃ©e directionnelle = produit scalaire gradient Â· direction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5.6 : Application - RÃ©gression LinÃ©aire\n",
    "\n",
    "**Consigne :** DonnÃ©es : $(x_1=1, y_1=2)$, $(x_2=2, y_2=4)$, $(x_3=3, y_3=5)$\n",
    "\n",
    "ModÃ¨le : $\\hat{y} = wx + b$\n",
    "\n",
    "Loss (MSE) : $L(w, b) = \\frac{1}{3}\\sum_{i=1}^{3}(y_i - (wx_i + b))^2$\n",
    "\n",
    "1. Calculez $\\nabla L = \\begin{bmatrix} \\frac{\\partial L}{\\partial w} \\\\ \\frac{\\partial L}{\\partial b} \\end{bmatrix}$\n",
    "2. Minimisez avec Gradient Descent (partez de $w=0, b=0$)\n",
    "3. Tracez les donnÃ©es et la droite finale\n",
    "4. Comparez avec la solution analytique (rÃ©gression linÃ©aire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 5.6 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : \n",
    "# dL_dw = (2/3) * sum((y_i - (w*x_i + b)) * (-x_i))\n",
    "# dL_db = (2/3) * sum((y_i - (w*x_i + b)) * (-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”— Section 6 : RÃ¨gle de la ChaÃ®ne (Ex 6.1-6.6)\n",
    "\n",
    "DÃ©river des fonctions composÃ©es - clÃ© de la backpropagation !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.1 : Chain Rule Simple\n",
    "\n",
    "**Consigne :** Calculez les dÃ©rivÃ©es avec la rÃ¨gle de la chaÃ®ne :\n",
    "\n",
    "1. $h(x) = (x^2 + 1)^5$\n",
    "2. $h(x) = e^{x^2}$\n",
    "3. $h(x) = \\sin(3x)$\n",
    "4. $h(x) = \\ln(x^2 + 1)$\n",
    "\n",
    "MÃ©thode : Posez $u = g(x)$ (fonction intÃ©rieure), puis $h = f(u)$, et $\\frac{dh}{dx} = \\frac{df}{du} \\cdot \\frac{du}{dx}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 6.1 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Identifiez fonction externe et interne\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.2 : Chain Rule MultivariÃ©e\n",
    "\n",
    "**Consigne :** Soit $z = f(x, y) = x^2 + y^2$ oÃ¹ $x = t^2$ et $y = t^3$.\n",
    "\n",
    "Calculez $\\frac{dz}{dt}$ avec la chain rule :\n",
    "\n",
    "$$\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial z}{\\partial y} \\frac{dy}{dt}$$\n",
    "\n",
    "1. Calculez $\\frac{\\partial z}{\\partial x}$ et $\\frac{\\partial z}{\\partial y}$\n",
    "2. Calculez $\\frac{dx}{dt}$ et $\\frac{dy}{dt}$\n",
    "3. Combinez avec la formule\n",
    "4. VÃ©rifiez en substituant directement $x(t)$ et $y(t)$ dans $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 6.2 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : x, y, t = sp.symbols('x y t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.3 : Backpropagation 1 Couche\n",
    "\n",
    "**Consigne :** RÃ©seau simple : $x \\xrightarrow{w} z = wx \\xrightarrow{\\sigma} a = \\sigma(z) \\rightarrow L = (a - y)^2$\n",
    "\n",
    "Avec $x = 3$, $y = 0.8$ (cible), $w = 0.5$, $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ (sigmoid)\n",
    "\n",
    "Calculez $\\frac{dL}{dw}$ avec la chain rule :\n",
    "\n",
    "$$\\frac{dL}{dw} = \\frac{dL}{da} \\cdot \\frac{da}{dz} \\cdot \\frac{dz}{dw}$$\n",
    "\n",
    "1. Forward pass : calculez $z$, $a$, $L$\n",
    "2. Backward pass : calculez chaque dÃ©rivÃ©e dans la chaÃ®ne\n",
    "3. Mettez Ã  jour $w$ avec Gradient Descent ($\\alpha = 0.5$)\n",
    "4. La loss a-t-elle diminuÃ© ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 6.3 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : \n",
    "# dL_da = 2(a - y)\n",
    "# da_dz = a(1 - a)  [dÃ©rivÃ©e sigmoid]\n",
    "# dz_dw = x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.4 : Backpropagation 2 Couches\n",
    "\n",
    "**Consigne :** RÃ©seau Ã  2 couches :\n",
    "\n",
    "$$x \\xrightarrow{w_1} z_1 \\xrightarrow{\\sigma} a_1 \\xrightarrow{w_2} z_2 \\xrightarrow{\\sigma} a_2 \\rightarrow L = (a_2 - y)^2$$\n",
    "\n",
    "Avec $x = 2$, $y = 0.9$, $w_1 = 0.4$, $w_2 = 0.6$\n",
    "\n",
    "Calculez $\\frac{dL}{dw_1}$ et $\\frac{dL}{dw_2}$ :\n",
    "\n",
    "1. Forward : $z_1, a_1, z_2, a_2, L$\n",
    "2. Backward : calculez tous les gradients\n",
    "3. Mettez Ã  jour les deux poids\n",
    "4. RÃ©pÃ©tez 10 fois et tracez l'Ã©volution de $L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 6.4 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Commencez par la sortie et propagez vers l'entrÃ©e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.5 : Chain Rule avec ReLU\n",
    "\n",
    "**Consigne :** RÃ©seau avec ReLU : $x \\xrightarrow{w} z = wx \\xrightarrow{\\text{ReLU}} a = \\max(0, z) \\rightarrow L = a^2$\n",
    "\n",
    "Avec $x = -2$, $w = 0.5$\n",
    "\n",
    "1. Forward pass : Calculez $z$, $a$, $L$\n",
    "2. Backward pass : Calculez $\\frac{dL}{dw}$\n",
    "3. Attention : $\\frac{da}{dz} = \\begin{cases} 1 & \\text{si } z > 0 \\\\ 0 & \\text{sinon} \\end{cases}$\n",
    "4. Que se passe-t-il si $z < 0$ ? (gradient disparaÃ®t !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 6.5 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : ReLU'(z) = 1 si z > 0, sinon 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 6.6 : Computational Graph\n",
    "\n",
    "**Consigne :** Pour $L = (a + b)^2$ oÃ¹ $a = x^2$ et $b = 2x$ :\n",
    "\n",
    "1. Dessinez le graphe computationnel (avec papier/commentaires)\n",
    "2. Calculez $\\frac{dL}{dx}$ avec la chain rule\n",
    "3. VÃ©rifiez en dÃ©veloppant $L$ directement en fonction de $x$\n",
    "4. Ã‰valuez en $x = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 6.6 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : \n",
    "# dL/dx = dL/da * da/dx + dL/db * db/dx\n",
    "# ou dÃ©veloppez L = (xÂ² + 2x)Â² puis dÃ©rivez\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”ï¸ Section 7 : Optimisation (Ex 7.1-7.5)\n",
    "\n",
    "Trouver minima et maxima de fonctions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.1 : Points Critiques 1D\n",
    "\n",
    "**Consigne :** Pour $f(x) = x^3 - 6x^2 + 9x + 1$ :\n",
    "\n",
    "1. Trouvez les points critiques : rÃ©solvez $f'(x) = 0$\n",
    "2. Calculez $f''(x)$\n",
    "3. Pour chaque point critique, dÃ©terminez s'il s'agit d'un minimum ou maximum\n",
    "4. Tracez la fonction et marquez les points critiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 7.1 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : sp.solve(f_prime, x) pour trouver points critiques\n",
    "# Si f''(x) > 0 â†’ minimum, si f''(x) < 0 â†’ maximum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.2 : Optimisation 2D - Points Critiques\n",
    "\n",
    "**Consigne :** Pour $f(x, y) = x^2 - 4x + y^2 + 6y + 5$ :\n",
    "\n",
    "1. Trouvez le point critique : rÃ©solvez $\\nabla f = \\mathbf{0}$\n",
    "2. Calculez la Hessienne $H$\n",
    "3. DÃ©terminez la nature du point (min/max/selle) via les valeurs propres\n",
    "4. Tracez une surface 3D avec le point critique marquÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 7.2 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : RÃ©solvez le systÃ¨me d'Ã©quations âˆ‚f/âˆ‚x = 0 et âˆ‚f/âˆ‚y = 0\n",
    "# sp.hessian(f, (x, y)) pour la matrice Hessienne\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.3 : Point-Selle\n",
    "\n",
    "**Consigne :** Pour $f(x, y) = x^2 - y^2$ (fonction selle) :\n",
    "\n",
    "1. Trouvez le point critique\n",
    "2. Calculez la Hessienne\n",
    "3. DÃ©terminez les valeurs propres (une positive, une nÃ©gative â†’ point-selle)\n",
    "4. Tracez la surface 3D et observez la forme de \"selle de cheval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 7.3 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Un point-selle a des valeurs propres de signes opposÃ©s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.4 : Optimisation Contrainte Simple\n",
    "\n",
    "**Consigne :** Minimisez $f(x, y) = x^2 + y^2$ sous la contrainte $x + y = 4$.\n",
    "\n",
    "MÃ©thode de substitution :\n",
    "1. Exprimez $y = 4 - x$\n",
    "2. Substituez dans $f$ : $g(x) = x^2 + (4-x)^2$\n",
    "3. Minimisez $g(x)$ (dÃ©rivÃ©e = 0)\n",
    "4. Trouvez $x$ puis $y$\n",
    "5. Tracez la contrainte et le point optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 7.4 : Ã‰crivez votre code ici\n",
    "\n",
    "# INDICE : Substituez y = 4 - x dans f\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7.5 : Comparaison des MÃ©thodes d'Optimisation\n",
    "\n",
    "**Consigne :** Pour $f(x, y) = (x - 2)^2 + (y + 1)^2$, comparez :\n",
    "\n",
    "**MÃ©thode 1 : Analytique**\n",
    "- RÃ©solvez $\\nabla f = 0$ directement\n",
    "\n",
    "**MÃ©thode 2 : Gradient Descent**\n",
    "- ItÃ©rations avec $\\alpha = 0.1$\n",
    "\n",
    "**MÃ©thode 3 : scipy.optimize.minimize**\n",
    "- Utilisez l'optimiseur intÃ©grÃ©\n",
    "\n",
    "Comparez les rÃ©sultats et le nombre d'itÃ©rations/Ã©valuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 7.5 : Ã‰crivez votre code ici\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# INDICE : \n",
    "# MÃ©thode 1 : sp.solve([df_dx, df_dy], [x, y])\n",
    "# MÃ©thode 2 : Boucle Gradient Descent\n",
    "# MÃ©thode 3 : minimize(lambda xy: f(xy[0], xy[1]), [0, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ‰ FÃ©licitations !\n",
    "\n",
    "Vous avez terminÃ© tous les exercices de calcul diffÃ©rentiel ! ðŸš€\n",
    "\n",
    "### ðŸ“Š Ce que vous avez accompli :\n",
    "\n",
    "âœ… **Section 1** : MaÃ®trise des limites et continuitÃ©  \n",
    "âœ… **Section 2** : Calcul de dÃ©rivÃ©es basiques et Gradient Descent  \n",
    "âœ… **Section 3** : Application des rÃ¨gles de dÃ©rivation  \n",
    "âœ… **Section 4** : Calcul de dÃ©rivÃ©es partielles  \n",
    "âœ… **Section 5** : ComprÃ©hension et utilisation du gradient  \n",
    "âœ… **Section 6** : MaÃ®trise de la rÃ¨gle de la chaÃ®ne et backpropagation  \n",
    "âœ… **Section 7** : Optimisation et recherche de minima/maxima  \n",
    "\n",
    "### ðŸš€ Prochaines Ã‰tapes :\n",
    "\n",
    "1. **VÃ©rifiez vos rÃ©ponses** : [solutions_05_calcul_differentiel.ipynb](./solutions_05_calcul_differentiel.ipynb)\n",
    "2. **Projet pratique** : [projet_05_visualisation_gradients.ipynb](./projet_05_visualisation_gradients.ipynb)\n",
    "3. **Revoyez les exercices** difficiles pour consolider\n",
    "\n",
    "### ðŸ’¡ Conseil Final :\n",
    "\n",
    "Vous comprenez maintenant **comment fonctionnent les rÃ©seaux de neurones** mathÃ©matiquement ! Gradient Descent et Backpropagation n'ont plus de secrets pour vous.\n",
    "\n",
    "**Continuez Ã  pratiquer - vous Ãªtes sur la voie de l'expertise en ML ! ðŸ’ª**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
