{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcca Chapitre 07 : Probabilit\u00e9s Fondamentales\n",
    "\n",
    "## \ud83c\udfaf Objectifs du chapitre\n",
    "\n",
    "Les probabilit\u00e9s sont **ABSOLUMENT ESSENTIELLES** pour le Machine Learning et la finance quantitative :\n",
    "\n",
    "- \ud83e\udd16 **Machine Learning** : Bayesian ML, Naive Bayes, \u00e9chantillonnage, incertitude\n",
    "- \ud83d\udcc8 **Finance Quantitative** : Mod\u00e9lisation des rendements, Value at Risk, options pricing\n",
    "- \ud83e\udde0 **Deep Learning** : Dropout, variational inference, uncertainty quantification\n",
    "- \ud83d\udcca **Statistics** : Fondation de tous les tests statistiques et mod\u00e8les\n",
    "\n",
    "### Ce que vous allez ma\u00eetriser :\n",
    "\n",
    "1. \u2705 Espaces de probabilit\u00e9 et \u00e9v\u00e9nements\n",
    "2. \u2705 Probabilit\u00e9s conditionnelles et ind\u00e9pendance\n",
    "3. \u2705 **Th\u00e9or\u00e8me de Bayes** (crucial en ML !)\n",
    "4. \u2705 Variables al\u00e9atoires discr\u00e8tes et continues\n",
    "5. \u2705 Distributions de probabilit\u00e9 (Bernoulli, Binomiale, Poisson, Normale)\n",
    "6. \u2705 Esp\u00e9rance, variance, moments\n",
    "7. \u2705 Covariance et corr\u00e9lation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Imports n\u00e9cessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.special import comb\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Seed pour reproductibilit\u00e9\n",
    "np.random.seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Espace de Probabilit\u00e9 et \u00c9v\u00e9nements\n",
    "\n",
    "### \ud83d\udcda Th\u00e9orie\n",
    "\n",
    "Un **espace de probabilit\u00e9** est un triplet $(\\Omega, \\mathcal{F}, P)$ o\u00f9 :\n",
    "\n",
    "- $\\Omega$ : **Espace \u00e9chantillonnal** (ensemble de tous les r\u00e9sultats possibles)\n",
    "- $\\mathcal{F}$ : **Tribu** ou \u03c3-alg\u00e8bre (ensemble des \u00e9v\u00e9nements mesurables)\n",
    "- $P$ : **Mesure de probabilit\u00e9** qui satisfait :\n",
    "  - $P(\\Omega) = 1$\n",
    "  - $P(A) \\geq 0$ pour tout $A \\in \\mathcal{F}$\n",
    "  - Si $A_1, A_2, ...$ sont disjoints : $P(\\bigcup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty} P(A_i)$\n",
    "\n",
    "### \ud83c\udfb2 Op\u00e9rations sur les \u00e9v\u00e9nements\n",
    "\n",
    "- **Union** : $A \\cup B$ (A ou B)\n",
    "- **Intersection** : $A \\cap B$ (A et B)\n",
    "- **Compl\u00e9mentaire** : $A^c$ (non A)\n",
    "- **Diff\u00e9rence** : $A \\setminus B$ = $A \\cap B^c$\n",
    "\n",
    "### \ud83d\udcd0 Axiomes de Kolmogorov\n",
    "\n",
    "$$P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$$\n",
    "$$P(A^c) = 1 - P(A)$$\n",
    "$$P(\\emptyset) = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exemple : Lancer de d\u00e9\n",
    "class EspaceProbabilite:\n",
    "    \"\"\"Repr\u00e9sentation d'un espace de probabilit\u00e9 discret\"\"\"\n",
    "    \n",
    "    def __init__(self, omega: List, probas: List[float]):\n",
    "        \"\"\"\n",
    "        omega: Liste des r\u00e9sultats possibles\n",
    "        probas: Liste des probabilit\u00e9s correspondantes\n",
    "        \"\"\"\n",
    "        assert len(omega) == len(probas), \"Dimensions incompatibles\"\n",
    "        assert abs(sum(probas) - 1.0) < 1e-10, \"Les probabilit\u00e9s doivent sommer \u00e0 1\"\n",
    "        \n",
    "        self.omega = omega\n",
    "        self.probas = np.array(probas)\n",
    "        self.proba_dict = dict(zip(omega, probas))\n",
    "    \n",
    "    def P(self, evenement: List) -> float:\n",
    "        \"\"\"Calcule P(\u00e9v\u00e9nement)\"\"\"\n",
    "        return sum(self.proba_dict.get(e, 0) for e in evenement)\n",
    "    \n",
    "    def union(self, A: List, B: List) -> List:\n",
    "        \"\"\"A \u222a B\"\"\"\n",
    "        return list(set(A) | set(B))\n",
    "    \n",
    "    def intersection(self, A: List, B: List) -> List:\n",
    "        \"\"\"A \u2229 B\"\"\"\n",
    "        return list(set(A) & set(B))\n",
    "    \n",
    "    def complementaire(self, A: List) -> List:\n",
    "        \"\"\"A^c\"\"\"\n",
    "        return [e for e in self.omega if e not in A]\n",
    "\n",
    "# Exemple : D\u00e9 \u00e9quilibr\u00e9\n",
    "de = EspaceProbabilite(\n",
    "    omega=[1, 2, 3, 4, 5, 6],\n",
    "    probas=[1/6] * 6\n",
    ")\n",
    "\n",
    "# \u00c9v\u00e9nements\n",
    "A = [2, 4, 6]  # Nombre pair\n",
    "B = [1, 2, 3]  # Nombre \u2264 3\n",
    "\n",
    "print(\"\ud83c\udfb2 Espace de probabilit\u00e9 : Lancer de d\u00e9\\n\")\n",
    "print(f\"P(nombre pair) = P({A}) = {de.P(A):.4f}\")\n",
    "print(f\"P(nombre \u2264 3) = P({B}) = {de.P(B):.4f}\")\n",
    "print(f\"\\nP(A \u222a B) = {de.P(de.union(A, B)):.4f}\")\n",
    "print(f\"P(A \u2229 B) = {de.P(de.intersection(A, B)):.4f}\")\n",
    "print(f\"P(A^c) = {de.P(de.complementaire(A)):.4f}\")\n",
    "print(f\"\\nV\u00e9rification : P(A) + P(A \u2229 B) - P(A \u2229 B) = {de.P(A) + de.P(B) - de.P(de.intersection(A, B)):.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Probabilit\u00e9s Conditionnelles\n",
    "\n",
    "### \ud83d\udcda D\u00e9finition\n",
    "\n",
    "La probabilit\u00e9 de $A$ **sachant** $B$ (not\u00e9e $P(A|B)$) est :\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "si $P(B) > 0$.\n",
    "\n",
    "### \ud83d\udd17 Formule de multiplication\n",
    "\n",
    "$$P(A \\cap B) = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A)$$\n",
    "\n",
    "### \ud83c\udfaf Ind\u00e9pendance\n",
    "\n",
    "Deux \u00e9v\u00e9nements $A$ et $B$ sont **ind\u00e9pendants** si :\n",
    "\n",
    "$$P(A \\cap B) = P(A) \\cdot P(B)$$\n",
    "\n",
    "\u00c9quivalent \u00e0 : $P(A|B) = P(A)$ (l'information sur $B$ ne change pas $A$)\n",
    "\n",
    "### \ud83c\udf33 Formule des probabilit\u00e9s totales\n",
    "\n",
    "Si $(B_1, ..., B_n)$ forme une partition de $\\Omega$ :\n",
    "\n",
    "$$P(A) = \\sum_{i=1}^{n} P(A|B_i) \\cdot P(B_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exemple : Test m\u00e9dical\n",
    "def probabilite_conditionnelle(p_a_inter_b: float, p_b: float) -> float:\n",
    "    \"\"\"Calcule P(A|B) = P(A\u2229B) / P(B)\"\"\"\n",
    "    if p_b == 0:\n",
    "        raise ValueError(\"P(B) ne peut pas \u00eatre 0\")\n",
    "    return p_a_inter_b / p_b\n",
    "\n",
    "# Probl\u00e8me : Test de d\u00e9pistage\n",
    "# M = malade, T = test positif\n",
    "P_M = 0.01  # 1% de la population est malade\n",
    "P_T_sachant_M = 0.95  # Sensibilit\u00e9 : 95% de vrais positifs\n",
    "P_T_sachant_non_M = 0.05  # 5% de faux positifs\n",
    "\n",
    "# Calcul de P(T) par probabilit\u00e9s totales\n",
    "P_T = P_T_sachant_M * P_M + P_T_sachant_non_M * (1 - P_M)\n",
    "\n",
    "print(\"\ud83c\udfe5 Exemple : Test m\u00e9dical\\n\")\n",
    "print(f\"Pr\u00e9valence de la maladie : {P_M*100:.1f}%\")\n",
    "print(f\"Sensibilit\u00e9 du test : {P_T_sachant_M*100:.1f}%\")\n",
    "print(f\"Taux de faux positifs : {P_T_sachant_non_M*100:.1f}%\")\n",
    "print(f\"\\nP(Test positif) = {P_T:.4f}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Diagramme en barres\n",
    "categories = ['Malades\\nTest\u00e9s+', 'Sains\\nTest\u00e9s+', 'Malades\\nTest\u00e9s-', 'Sains\\nTest\u00e9s-']\n",
    "probas = [\n",
    "    P_T_sachant_M * P_M,\n",
    "    P_T_sachant_non_M * (1 - P_M),\n",
    "    (1 - P_T_sachant_M) * P_M,\n",
    "    (1 - P_T_sachant_non_M) * (1 - P_M)\n",
    "]\n",
    "colors = ['green', 'orange', 'red', 'blue']\n",
    "\n",
    "ax1.bar(categories, probas, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Probabilit\u00e9')\n",
    "ax1.set_title('Distribution des cas')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (cat, prob) in enumerate(zip(categories, probas)):\n",
    "    ax1.text(i, prob + 0.01, f'{prob:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Diagramme de Venn\n",
    "from matplotlib.patches import Circle\n",
    "ax2.set_xlim(0, 4)\n",
    "ax2.set_ylim(0, 4)\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "circle_M = Circle((1.5, 2), 1, color='red', alpha=0.3, label='Malades')\n",
    "circle_T = Circle((2.5, 2), 1, color='blue', alpha=0.3, label='Test+')\n",
    "ax2.add_patch(circle_M)\n",
    "ax2.add_patch(circle_T)\n",
    "\n",
    "ax2.text(1.2, 2, f'M\\n{P_M:.3f}', ha='center', va='center', fontsize=10)\n",
    "ax2.text(2.8, 2, f'T\\n{P_T:.3f}', ha='center', va='center', fontsize=10)\n",
    "ax2.text(2, 2, f'M\u2229T\\n{P_T_sachant_M * P_M:.4f}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "ax2.set_title('Diagramme de Venn')\n",
    "ax2.legend()\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Th\u00e9or\u00e8me de Bayes \ud83c\udf1f\n",
    "\n",
    "### \ud83c\udf93 LE th\u00e9or\u00e8me fondamental du ML !\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "Ou, en version compl\u00e8te avec probabilit\u00e9s totales :\n",
    "\n",
    "$$P(A_i|B) = \\frac{P(B|A_i) \\cdot P(A_i)}{\\sum_{j=1}^{n} P(B|A_j) \\cdot P(A_j)}$$\n",
    "\n",
    "### \ud83d\udcd6 Vocabulaire bay\u00e9sien\n",
    "\n",
    "- $P(A)$ : **Prior** (probabilit\u00e9 a priori)\n",
    "- $P(B|A)$ : **Likelihood** (vraisemblance)\n",
    "- $P(A|B)$ : **Posterior** (probabilit\u00e9 a posteriori)\n",
    "- $P(B)$ : **Evidence** (normalisation)\n",
    "\n",
    "### \ud83e\udd16 ESSENTIEL EN ML !\n",
    "\n",
    "Le th\u00e9or\u00e8me de Bayes est la base de :\n",
    "- **Naive Bayes classifier**\n",
    "- **Bayesian inference**\n",
    "- **Bayesian optimization**\n",
    "- **Probabilistic graphical models**\n",
    "- **Posterior inference in neural networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def theoreme_bayes(prior: float, likelihood: float, evidence: float) -> float:\n",
    "    \"\"\"Calcule le posterior avec le th\u00e9or\u00e8me de Bayes\"\"\"\n",
    "    return (likelihood * prior) / evidence\n",
    "\n",
    "# Retour sur le test m\u00e9dical avec Bayes\n",
    "print(\"\ud83e\uddec Th\u00e9or\u00e8me de Bayes : Test m\u00e9dical\\n\")\n",
    "\n",
    "# P(Malade | Test+) = ?\n",
    "prior = P_M  # P(Malade)\n",
    "likelihood = P_T_sachant_M  # P(Test+ | Malade)\n",
    "evidence = P_T  # P(Test+)\n",
    "\n",
    "posterior = theoreme_bayes(prior, likelihood, evidence)\n",
    "\n",
    "print(f\"Prior P(Malade) = {prior:.4f}\")\n",
    "print(f\"Likelihood P(Test+|Malade) = {likelihood:.4f}\")\n",
    "print(f\"Evidence P(Test+) = {evidence:.4f}\")\n",
    "print(f\"\\n\ud83c\udfaf Posterior P(Malade|Test+) = {posterior:.4f}\")\n",
    "print(f\"\\n\ud83d\udca1 Si test positif, seulement {posterior*100:.1f}% de chances d'\u00eatre malade!\")\n",
    "print(f\"   (Car la pr\u00e9valence est faible et il y a des faux positifs)\")\n",
    "\n",
    "# Exemple 2 : Classification bay\u00e9sienne\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udce7 Exemple : Filtre anti-spam bay\u00e9sien\\n\")\n",
    "\n",
    "# Donn\u00e9es\n",
    "P_spam = 0.3  # 30% des emails sont des spams\n",
    "P_mot_viagra_sachant_spam = 0.8  # 80% des spams contiennent \"viagra\"\n",
    "P_mot_viagra_sachant_ham = 0.05  # 5% des emails l\u00e9gitimes contiennent \"viagra\"\n",
    "\n",
    "# P(\"viagra\") par probabilit\u00e9s totales\n",
    "P_mot_viagra = (P_mot_viagra_sachant_spam * P_spam + \n",
    "                P_mot_viagra_sachant_ham * (1 - P_spam))\n",
    "\n",
    "# P(Spam | \"viagra\") par Bayes\n",
    "P_spam_sachant_viagra = theoreme_bayes(\n",
    "    prior=P_spam,\n",
    "    likelihood=P_mot_viagra_sachant_spam,\n",
    "    evidence=P_mot_viagra\n",
    ")\n",
    "\n",
    "print(f\"Prior P(Spam) = {P_spam:.2f}\")\n",
    "print(f\"P('viagra'|Spam) = {P_mot_viagra_sachant_spam:.2f}\")\n",
    "print(f\"P('viagra'|Ham) = {P_mot_viagra_sachant_ham:.2f}\")\n",
    "print(f\"\\n\ud83c\udfaf P(Spam|'viagra') = {P_spam_sachant_viagra:.4f}\")\n",
    "print(f\"\\n\ud83d\udca1 Si email contient 'viagra', {P_spam_sachant_viagra*100:.1f}% de chances que ce soit un spam!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualisation de l'update bay\u00e9sien\n",
    "def visualiser_update_bayesien(priors: np.ndarray, likelihoods: np.ndarray, \n",
    "                               labels: List[str], titre: str):\n",
    "    \"\"\"\n",
    "    Visualise comment le prior devient posterior apr\u00e8s observation\n",
    "    \"\"\"\n",
    "    # Calcul des posteriors\n",
    "    evidence = np.sum(priors * likelihoods)\n",
    "    posteriors = (likelihoods * priors) / evidence\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Prior\n",
    "    axes[0].bar(labels, priors, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_title('Prior P(H)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Probabilit\u00e9')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Likelihood\n",
    "    axes[1].bar(labels, likelihoods, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title('Likelihood P(D|H)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Probabilit\u00e9')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Posterior\n",
    "    axes[2].bar(labels, posteriors, color='green', alpha=0.7, edgecolor='black')\n",
    "    axes[2].set_title('Posterior P(H|D)', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_ylabel('Probabilit\u00e9')\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    axes[2].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Annotations\n",
    "    for ax, values in zip(axes, [priors, likelihoods, posteriors]):\n",
    "        for i, v in enumerate(values):\n",
    "            ax.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(titre, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return posteriors\n",
    "\n",
    "# Exemple : Diagnostic m\u00e9dical avec 3 maladies possibles\n",
    "labels = ['Grippe', 'Covid', 'Allergie']\n",
    "priors = np.array([0.5, 0.1, 0.4])  # Pr\u00e9valences\n",
    "likelihoods = np.array([0.3, 0.9, 0.1])  # P(fi\u00e8vre | maladie)\n",
    "\n",
    "posteriors = visualiser_update_bayesien(\n",
    "    priors, likelihoods, labels,\n",
    "    \"Update Bay\u00e9sien : Diagnostic avec sympt\u00f4me 'fi\u00e8vre'\"\n",
    ")\n",
    "\n",
    "print(\"\\n\ud83d\udd04 Mise \u00e0 jour bay\u00e9sienne :\")\n",
    "for label, prior, post in zip(labels, priors, posteriors):\n",
    "    print(f\"{label:10s} : {prior:.3f} \u2192 {post:.3f} (\u00d7{post/prior:.2f})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Variables Al\u00e9atoires\n",
    "\n",
    "### \ud83d\udcda D\u00e9finition\n",
    "\n",
    "Une **variable al\u00e9atoire** (VA) est une fonction $X: \\Omega \\to \\mathbb{R}$ qui associe un nombre r\u00e9el \u00e0 chaque r\u00e9sultat.\n",
    "\n",
    "### \ud83c\udfb2 Variables al\u00e9atoires discr\u00e8tes\n",
    "\n",
    "Une VA discr\u00e8te prend un nombre fini ou d\u00e9nombrable de valeurs.\n",
    "\n",
    "**Fonction de masse de probabilit\u00e9 (PMF)** :\n",
    "$$p_X(x) = P(X = x)$$\n",
    "\n",
    "**Propri\u00e9t\u00e9s** :\n",
    "- $p_X(x) \\geq 0$ pour tout $x$\n",
    "- $\\sum_{x} p_X(x) = 1$\n",
    "\n",
    "**Fonction de r\u00e9partition (CDF)** :\n",
    "$$F_X(x) = P(X \\leq x) = \\sum_{t \\leq x} p_X(t)$$\n",
    "\n",
    "### \ud83d\udcca Variables al\u00e9atoires continues\n",
    "\n",
    "Une VA continue peut prendre toute valeur dans un intervalle.\n",
    "\n",
    "**Fonction de densit\u00e9 de probabilit\u00e9 (PDF)** :\n",
    "$$P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx$$\n",
    "\n",
    "**Propri\u00e9t\u00e9s** :\n",
    "- $f_X(x) \\geq 0$ pour tout $x$\n",
    "- $\\int_{-\\infty}^{\\infty} f_X(x) dx = 1$\n",
    "- $P(X = x) = 0$ pour toute valeur $x$ (!)  \n",
    "\n",
    "**Fonction de r\u00e9partition (CDF)** :\n",
    "$$F_X(x) = P(X \\leq x) = \\int_{-\\infty}^x f_X(t) dt$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exemple de VA discr\u00e8te : Lancer de d\u00e9\n",
    "class VariableAleatoireDiscrete:\n",
    "    \"\"\"Repr\u00e9sentation d'une variable al\u00e9atoire discr\u00e8te\"\"\"\n",
    "    \n",
    "    def __init__(self, valeurs: np.ndarray, probas: np.ndarray):\n",
    "        assert len(valeurs) == len(probas)\n",
    "        assert abs(np.sum(probas) - 1.0) < 1e-10\n",
    "        \n",
    "        self.valeurs = valeurs\n",
    "        self.probas = probas\n",
    "    \n",
    "    def pmf(self, x: float) -> float:\n",
    "        \"\"\"Fonction de masse P(X=x)\"\"\"\n",
    "        idx = np.where(self.valeurs == x)[0]\n",
    "        return self.probas[idx[0]] if len(idx) > 0 else 0.0\n",
    "    \n",
    "    def cdf(self, x: float) -> float:\n",
    "        \"\"\"Fonction de r\u00e9partition P(X\u2264x)\"\"\"\n",
    "        return np.sum(self.probas[self.valeurs <= x])\n",
    "    \n",
    "    def esperance(self) -> float:\n",
    "        \"\"\"Esp\u00e9rance E[X]\"\"\"\n",
    "        return np.sum(self.valeurs * self.probas)\n",
    "    \n",
    "    def variance(self) -> float:\n",
    "        \"\"\"Variance Var(X)\"\"\"\n",
    "        E_X = self.esperance()\n",
    "        return np.sum((self.valeurs - E_X)**2 * self.probas)\n",
    "    \n",
    "    def simuler(self, n: int) -> np.ndarray:\n",
    "        \"\"\"G\u00e9n\u00e8re n \u00e9chantillons\"\"\"\n",
    "        return np.random.choice(self.valeurs, size=n, p=self.probas)\n",
    "\n",
    "# Exemple : D\u00e9 truqu\u00e9\n",
    "de_truque = VariableAleatoireDiscrete(\n",
    "    valeurs=np.array([1, 2, 3, 4, 5, 6]),\n",
    "    probas=np.array([0.1, 0.1, 0.1, 0.2, 0.2, 0.3])  # Favorise 6\n",
    ")\n",
    "\n",
    "# Visualisation PMF et CDF\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PMF\n",
    "ax1.bar(de_truque.valeurs, de_truque.probas, color='steelblue', \n",
    "        alpha=0.7, edgecolor='black', width=0.6)\n",
    "ax1.set_xlabel('Valeur x')\n",
    "ax1.set_ylabel('P(X = x)')\n",
    "ax1.set_title('PMF : Fonction de masse', fontweight='bold')\n",
    "ax1.set_xticks(de_truque.valeurs)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for x, p in zip(de_truque.valeurs, de_truque.probas):\n",
    "    ax1.text(x, p + 0.01, f'{p:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# CDF\n",
    "x_range = np.linspace(0, 7, 100)\n",
    "cdf_values = [de_truque.cdf(x) for x in x_range]\n",
    "\n",
    "ax2.plot(x_range, cdf_values, 'b-', linewidth=2)\n",
    "ax2.scatter(de_truque.valeurs, [de_truque.cdf(x) for x in de_truque.valeurs],\n",
    "           color='red', s=50, zorder=5)\n",
    "ax2.set_xlabel('Valeur x')\n",
    "ax2.set_ylabel('P(X \u2264 x)')\n",
    "ax2.set_title('CDF : Fonction de r\u00e9partition', fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_ylim(-0.05, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Caract\u00e9ristiques du d\u00e9 truqu\u00e9 :\")\n",
    "print(f\"E[X] = {de_truque.esperance():.4f}\")\n",
    "print(f\"Var(X) = {de_truque.variance():.4f}\")\n",
    "print(f\"\u03c3(X) = {np.sqrt(de_truque.variance()):.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\ufe0f\u20e3 Distributions de Probabilit\u00e9 Classiques\n",
    "\n",
    "### \ud83c\udfb2 Distributions Discr\u00e8tes\n",
    "\n",
    "#### 1. Loi de Bernoulli : $X \\sim \\text{Ber}(p)$\n",
    "\n",
    "Mod\u00e9lise une exp\u00e9rience \u00e0 2 r\u00e9sultats (succ\u00e8s/\u00e9chec).\n",
    "\n",
    "- $X \\in \\{0, 1\\}$\n",
    "- $P(X = 1) = p$, $P(X = 0) = 1-p$\n",
    "- $E[X] = p$\n",
    "- $\\text{Var}(X) = p(1-p)$\n",
    "\n",
    "**Utilisation** : Classification binaire, dropout en neural networks\n",
    "\n",
    "#### 2. Loi Binomiale : $X \\sim \\mathcal{B}(n, p)$\n",
    "\n",
    "Nombre de succ\u00e8s en $n$ essais ind\u00e9pendants de Bernoulli.\n",
    "\n",
    "- $X \\in \\{0, 1, ..., n\\}$\n",
    "- $P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}$\n",
    "- $E[X] = np$\n",
    "- $\\text{Var}(X) = np(1-p)$\n",
    "\n",
    "**Utilisation** : Taux de conversion, A/B testing\n",
    "\n",
    "#### 3. Loi de Poisson : $X \\sim \\text{Poisson}(\\lambda)$\n",
    "\n",
    "Nombre d'\u00e9v\u00e9nements rares en un temps fix\u00e9.\n",
    "\n",
    "- $X \\in \\{0, 1, 2, ...\\}$\n",
    "- $P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$\n",
    "- $E[X] = \\lambda$\n",
    "- $\\text{Var}(X) = \\lambda$\n",
    "\n",
    "**Utilisation** : Comptage d'\u00e9v\u00e9nements (clics, transactions, d\u00e9faillances)\n",
    "\n",
    "### \ud83d\udcca Distributions Continues\n",
    "\n",
    "#### 4. Loi Normale (Gaussienne) : $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "\n",
    "**LA** distribution la plus importante en statistiques et ML !\n",
    "\n",
    "- $f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$\n",
    "- $E[X] = \\mu$\n",
    "- $\\text{Var}(X) = \\sigma^2$\n",
    "\n",
    "**Propri\u00e9t\u00e9s remarquables** :\n",
    "- Sym\u00e9trique autour de $\\mu$\n",
    "- 68% des valeurs dans $[\\mu - \\sigma, \\mu + \\sigma]$\n",
    "- 95% dans $[\\mu - 2\\sigma, \\mu + 2\\sigma]$\n",
    "- 99.7% dans $[\\mu - 3\\sigma, \\mu + 3\\sigma]$\n",
    "\n",
    "**Utilisation** : Mod\u00e9lisation des erreurs, rendements financiers, processus gaussiens, initialisation des poids en DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualisation des distributions classiques\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Bernoulli\n",
    "p = 0.3\n",
    "x_bern = [0, 1]\n",
    "pmf_bern = [1-p, p]\n",
    "axes[0, 0].bar(x_bern, pmf_bern, color='steelblue', alpha=0.7, edgecolor='black', width=0.3)\n",
    "axes[0, 0].set_title(f'Bernoulli(p={p})', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('x')\n",
    "axes[0, 0].set_ylabel('P(X=x)')\n",
    "axes[0, 0].set_xticks([0, 1])\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Binomiale\n",
    "n, p = 20, 0.3\n",
    "x_binom = np.arange(0, n+1)\n",
    "pmf_binom = stats.binom.pmf(x_binom, n, p)\n",
    "axes[0, 1].bar(x_binom, pmf_binom, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title(f'Binomiale(n={n}, p={p})\\nE[X]={n*p:.1f}, \u03c3={np.sqrt(n*p*(1-p)):.2f}', \n",
    "                     fontweight='bold')\n",
    "axes[0, 1].set_xlabel('x')\n",
    "axes[0, 1].set_ylabel('P(X=x)')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Poisson\n",
    "lambda_param = 5\n",
    "x_poisson = np.arange(0, 20)\n",
    "pmf_poisson = stats.poisson.pmf(x_poisson, lambda_param)\n",
    "axes[0, 2].bar(x_poisson, pmf_poisson, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[0, 2].set_title(f'Poisson(\u03bb={lambda_param})\\nE[X]=Var(X)={lambda_param}', \n",
    "                     fontweight='bold')\n",
    "axes[0, 2].set_xlabel('x')\n",
    "axes[0, 2].set_ylabel('P(X=x)')\n",
    "axes[0, 2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Normale standard\n",
    "x_norm = np.linspace(-4, 4, 200)\n",
    "pdf_norm = stats.norm.pdf(x_norm, 0, 1)\n",
    "axes[1, 0].plot(x_norm, pdf_norm, 'b-', linewidth=2, label='PDF')\n",
    "axes[1, 0].fill_between(x_norm, pdf_norm, alpha=0.3)\n",
    "\n",
    "# R\u00e8gle 68-95-99.7\n",
    "axes[1, 0].axvline(-1, color='red', linestyle='--', alpha=0.5, label='\u03bc\u00b1\u03c3 (68%)')\n",
    "axes[1, 0].axvline(1, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].axvline(-2, color='orange', linestyle='--', alpha=0.5, label='\u03bc\u00b12\u03c3 (95%)')\n",
    "axes[1, 0].axvline(2, color='orange', linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[1, 0].set_title('Normale(\u03bc=0, \u03c3\u00b2=1)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('x')\n",
    "axes[1, 0].set_ylabel('f(x)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 5. Comparaison de normales\n",
    "params = [(0, 1), (0, 2), (2, 1)]\n",
    "colors = ['blue', 'red', 'green']\n",
    "for (mu, sigma), color in zip(params, colors):\n",
    "    pdf = stats.norm.pdf(x_norm, mu, sigma)\n",
    "    axes[1, 1].plot(x_norm, pdf, color=color, linewidth=2, \n",
    "                    label=f'\u03bc={mu}, \u03c3={sigma}')\n",
    "\n",
    "axes[1, 1].set_title('Effet de \u03bc et \u03c3', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('x')\n",
    "axes[1, 1].set_ylabel('f(x)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# 6. Q-Q plot (Normal)\n",
    "sample = np.random.normal(0, 1, 1000)\n",
    "stats.probplot(sample, dist=\"norm\", plot=axes[1, 2])\n",
    "axes[1, 2].set_title('Q-Q Plot (test de normalit\u00e9)', fontweight='bold')\n",
    "axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# \ud83e\udd16 APPLICATION ML : \u00c9chantillonnage et g\u00e9n\u00e9ration de donn\u00e9es\n",
    "print(\"\ud83e\udd16 ESSENTIEL EN ML : G\u00e9n\u00e9ration de donn\u00e9es synth\u00e9tiques\\n\")\n",
    "\n",
    "# 1. Classification binaire avec donn\u00e9es gaussiennes\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Classe 0 : N(\u03bc\u2080, \u03a3\u2080)\n",
    "mean_0 = np.array([0, 0])\n",
    "cov_0 = np.array([[1, 0.5], [0.5, 1]])\n",
    "X_0 = np.random.multivariate_normal(mean_0, cov_0, n_samples)\n",
    "\n",
    "# Classe 1 : N(\u03bc\u2081, \u03a3\u2081)\n",
    "mean_1 = np.array([3, 3])\n",
    "cov_1 = np.array([[1, -0.3], [-0.3, 1]])\n",
    "X_1 = np.random.multivariate_normal(mean_1, cov_1, n_samples)\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.scatter(X_0[:, 0], X_0[:, 1], alpha=0.6, label='Classe 0', s=30)\n",
    "ax1.scatter(X_1[:, 0], X_1[:, 1], alpha=0.6, label='Classe 1', s=30)\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.set_title('Dataset de classification g\u00e9n\u00e9r\u00e9', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.axis('equal')\n",
    "\n",
    "# 2. Histogrammes marginaux\n",
    "ax2.hist(X_0[:, 0], bins=30, alpha=0.5, label='Classe 0 - Feature 1', density=True)\n",
    "ax2.hist(X_1[:, 0], bins=30, alpha=0.5, label='Classe 1 - Feature 1', density=True)\n",
    "\n",
    "# Overlay des densit\u00e9s th\u00e9oriques\n",
    "x_range = np.linspace(-3, 6, 200)\n",
    "ax2.plot(x_range, stats.norm.pdf(x_range, mean_0[0], np.sqrt(cov_0[0, 0])), \n",
    "         'b-', linewidth=2, label='Th\u00e9orique Classe 0')\n",
    "ax2.plot(x_range, stats.norm.pdf(x_range, mean_1[0], np.sqrt(cov_1[0, 0])), \n",
    "         'r-', linewidth=2, label='Th\u00e9orique Classe 1')\n",
    "\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Densit\u00e9')\n",
    "ax2.set_title('Distributions marginales', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 Applications en ML :\")\n",
    "print(\"  \u2022 G\u00e9n\u00e9ration de datasets synth\u00e9tiques pour tests\")\n",
    "print(\"  \u2022 Augmentation de donn\u00e9es (data augmentation)\")\n",
    "print(\"  \u2022 GANs (Generative Adversarial Networks)\")\n",
    "print(\"  \u2022 VAEs (Variational Autoencoders)\")\n",
    "print(\"  \u2022 Simulation de Monte Carlo\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\ufe0f\u20e3 Esp\u00e9rance, Variance et Moments\n",
    "\n",
    "### \ud83d\udcd0 Esp\u00e9rance math\u00e9matique\n",
    "\n",
    "L'**esp\u00e9rance** $E[X]$ est la \"moyenne pond\u00e9r\u00e9e\" des valeurs.\n",
    "\n",
    "**Cas discret** :\n",
    "$$E[X] = \\sum_{x} x \\cdot P(X = x)$$\n",
    "\n",
    "**Cas continu** :\n",
    "$$E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) dx$$\n",
    "\n",
    "**Propri\u00e9t\u00e9s** :\n",
    "- Lin\u00e9arit\u00e9 : $E[aX + b] = aE[X] + b$\n",
    "- Additivit\u00e9 : $E[X + Y] = E[X] + E[Y]$ (toujours !)\n",
    "- Si $X, Y$ ind\u00e9pendantes : $E[XY] = E[X]E[Y]$\n",
    "\n",
    "### \ud83d\udcca Variance\n",
    "\n",
    "La **variance** mesure la dispersion autour de la moyenne.\n",
    "\n",
    "$$\\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$$\n",
    "\n",
    "**\u00c9cart-type** :\n",
    "$$\\sigma_X = \\sqrt{\\text{Var}(X)}$$\n",
    "\n",
    "**Propri\u00e9t\u00e9s** :\n",
    "- $\\text{Var}(aX + b) = a^2 \\text{Var}(X)$\n",
    "- Si $X, Y$ ind\u00e9pendantes : $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)$\n",
    "\n",
    "### \ud83d\udcc8 Moments d'ordre sup\u00e9rieur\n",
    "\n",
    "**Moment d'ordre $n$** :\n",
    "$$E[X^n]$$\n",
    "\n",
    "**Moment centr\u00e9 d'ordre $n$** :\n",
    "$$E[(X - E[X])^n]$$\n",
    "\n",
    "**Skewness (asym\u00e9trie)** :\n",
    "$$\\gamma_1 = \\frac{E[(X - \\mu)^3]}{\\sigma^3}$$\n",
    "\n",
    "**Kurtosis (aplatissement)** :\n",
    "$$\\gamma_2 = \\frac{E[(X - \\mu)^4]}{\\sigma^4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcul et visualisation des moments\n",
    "def calculer_moments(data: np.ndarray) -> dict:\n",
    "    \"\"\"Calcule les moments statistiques d'un \u00e9chantillon\"\"\"\n",
    "    return {\n",
    "        'mean': np.mean(data),\n",
    "        'variance': np.var(data, ddof=1),  # ddof=1 pour variance non biais\u00e9e\n",
    "        'std': np.std(data, ddof=1),\n",
    "        'skewness': stats.skew(data),\n",
    "        'kurtosis': stats.kurtosis(data)\n",
    "    }\n",
    "\n",
    "# G\u00e9n\u00e9ration de diff\u00e9rentes distributions\n",
    "np.random.seed(42)\n",
    "n = 10000\n",
    "\n",
    "distributions = {\n",
    "    'Normal(0,1)': np.random.normal(0, 1, n),\n",
    "    'Exponentielle(1)': np.random.exponential(1, n),\n",
    "    'Uniforme(-2,2)': np.random.uniform(-2, 2, n),\n",
    "    'Chi\u00b2(5)': np.random.chisquare(5, n)\n",
    "}\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, data) in enumerate(distributions.items()):\n",
    "    moments = calculer_moments(data)\n",
    "    \n",
    "    # Histogramme\n",
    "    axes[i].hist(data, bins=50, density=True, alpha=0.7, \n",
    "                edgecolor='black', color='steelblue')\n",
    "    \n",
    "    # Ligne verticale pour la moyenne\n",
    "    axes[i].axvline(moments['mean'], color='red', linestyle='--', \n",
    "                   linewidth=2, label=f\"\u03bc = {moments['mean']:.2f}\")\n",
    "    \n",
    "    # Zone \u03bc \u00b1 \u03c3\n",
    "    axes[i].axvspan(moments['mean'] - moments['std'], \n",
    "                   moments['mean'] + moments['std'], \n",
    "                   alpha=0.2, color='green', label=f\"\u03bc\u00b1\u03c3 ({moments['std']:.2f})\")\n",
    "    \n",
    "    # Annotations\n",
    "    textstr = f\"Skewness: {moments['skewness']:.2f}\\nKurtosis: {moments['kurtosis']:.2f}\"\n",
    "    axes[i].text(0.65, 0.95, textstr, transform=axes[i].transAxes,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', \n",
    "                facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    axes[i].set_title(name, fontweight='bold')\n",
    "    axes[i].set_xlabel('Valeur')\n",
    "    axes[i].set_ylabel('Densit\u00e9')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tableau r\u00e9capitulatif\n",
    "print(\"\\n\ud83d\udcca Tableau des moments :\\n\")\n",
    "df_moments = pd.DataFrame({\n",
    "    name: calculer_moments(data) \n",
    "    for name, data in distributions.items()\n",
    "}).T\n",
    "\n",
    "print(df_moments.round(3))\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Interpr\u00e9tation :\")\n",
    "print(\"  \u2022 Skewness > 0 : Queue \u00e0 droite (asym\u00e9trie positive)\")\n",
    "print(\"  \u2022 Skewness < 0 : Queue \u00e0 gauche (asym\u00e9trie n\u00e9gative)\")\n",
    "print(\"  \u2022 Kurtosis > 0 : Plus 'pointue' que normale (queues lourdes)\")\n",
    "print(\"  \u2022 Kurtosis < 0 : Plus 'plate' que normale (queues l\u00e9g\u00e8res)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\ufe0f\u20e3 Covariance et Corr\u00e9lation\n",
    "\n",
    "### \ud83d\udd17 Covariance\n",
    "\n",
    "La **covariance** mesure la relation lin\u00e9aire entre deux variables.\n",
    "\n",
    "$$\\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]$$\n",
    "\n",
    "**Propri\u00e9t\u00e9s** :\n",
    "- $\\text{Cov}(X, X) = \\text{Var}(X)$\n",
    "- $\\text{Cov}(X, Y) = \\text{Cov}(Y, X)$ (sym\u00e9trie)\n",
    "- $\\text{Cov}(aX + b, Y) = a\\text{Cov}(X, Y)$\n",
    "- Si $X, Y$ ind\u00e9pendantes : $\\text{Cov}(X, Y) = 0$ (r\u00e9ciproque fausse !)\n",
    "\n",
    "**Variance d'une somme** :\n",
    "$$\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X, Y)$$\n",
    "\n",
    "### \ud83d\udcd0 Corr\u00e9lation (de Pearson)\n",
    "\n",
    "La **corr\u00e9lation** est la covariance normalis\u00e9e :\n",
    "\n",
    "$$\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "**Propri\u00e9t\u00e9s** :\n",
    "- $-1 \\leq \\rho \\leq 1$\n",
    "- $\\rho = 1$ : corr\u00e9lation lin\u00e9aire parfaite positive\n",
    "- $\\rho = -1$ : corr\u00e9lation lin\u00e9aire parfaite n\u00e9gative\n",
    "- $\\rho = 0$ : pas de corr\u00e9lation lin\u00e9aire (\u26a0\ufe0f peut y avoir d\u00e9pendance non-lin\u00e9aire !)\n",
    "\n",
    "### \ud83d\udcca Matrice de covariance\n",
    "\n",
    "Pour un vecteur al\u00e9atoire $\\mathbf{X} = (X_1, ..., X_n)$ :\n",
    "\n",
    "$$\\Sigma = \\begin{pmatrix}\n",
    "\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_n) \\\\\n",
    "\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_n) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(X_n, X_1) & \\text{Cov}(X_n, X_2) & \\cdots & \\text{Var}(X_n)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "**ESSENTIEL EN ML** : PCA, Gaussian Processes, portfolio optimization !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# G\u00e9n\u00e9ration de donn\u00e9es corr\u00e9l\u00e9es\n",
    "def generer_donnees_correlees(n: int, rho: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    G\u00e9n\u00e8re deux variables corr\u00e9l\u00e9es avec coefficient \u03c1\n",
    "    \"\"\"\n",
    "    # Matrice de covariance\n",
    "    cov = np.array([[1, rho], [rho, 1]])\n",
    "    \n",
    "    # G\u00e9n\u00e9ration\n",
    "    data = np.random.multivariate_normal([0, 0], cov, n)\n",
    "    return data[:, 0], data[:, 1]\n",
    "\n",
    "# Visualisation pour diff\u00e9rentes corr\u00e9lations\n",
    "correlations = [-0.9, -0.5, 0, 0.5, 0.9]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "for i, rho in enumerate(correlations):\n",
    "    X, Y = generer_donnees_correlees(n, rho)\n",
    "    \n",
    "    # Calcul de la corr\u00e9lation empirique\n",
    "    rho_empirique = np.corrcoef(X, Y)[0, 1]\n",
    "    cov_empirique = np.cov(X, Y, ddof=1)[0, 1]\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[i].scatter(X, Y, alpha=0.5, s=20)\n",
    "    \n",
    "    # R\u00e9gression lin\u00e9aire\n",
    "    z = np.polyfit(X, Y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(X.min(), X.max(), 100)\n",
    "    axes[i].plot(x_line, p(x_line), 'r-', linewidth=2, alpha=0.8)\n",
    "    \n",
    "    axes[i].set_title(f'\u03c1 th\u00e9orique = {rho:.1f}\\n\u03c1 empirique = {rho_empirique:.3f}',\n",
    "                     fontweight='bold')\n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('Y')\n",
    "    axes[i].grid(alpha=0.3)\n",
    "    axes[i].axis('equal')\n",
    "    \n",
    "    # Annotation de la covariance\n",
    "    axes[i].text(0.05, 0.95, f'Cov(X,Y) = {cov_empirique:.3f}',\n",
    "                transform=axes[i].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Cas non-lin\u00e9aire (derni\u00e8re subplot)\n",
    "X = np.random.uniform(-3, 3, n)\n",
    "Y = X**2 + np.random.normal(0, 1, n)\n",
    "rho_nonlinear = np.corrcoef(X, Y)[0, 1]\n",
    "\n",
    "axes[5].scatter(X, Y, alpha=0.5, s=20, color='purple')\n",
    "axes[5].set_title(f'D\u00e9pendance NON-lin\u00e9aire\\n\u03c1 = {rho_nonlinear:.3f} \u2248 0 !',\n",
    "                 fontweight='bold', color='red')\n",
    "axes[5].set_xlabel('X')\n",
    "axes[5].set_ylabel('Y = X\u00b2 + bruit')\n",
    "axes[5].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f ATTENTION :\")\n",
    "print(\"  \u2022 Corr\u00e9lation = 0 \u2260 Ind\u00e9pendance !\")\n",
    "print(\"  \u2022 La corr\u00e9lation mesure UNIQUEMENT les relations lin\u00e9aires\")\n",
    "print(\"  \u2022 Pour d\u00e9pendances non-lin\u00e9aires : mutual information, rank correlation, etc.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# \ud83d\udcc8 APPLICATION FINANCE : Matrice de corr\u00e9lation d'un portefeuille\n",
    "print(\"\ud83d\udcc8 APPLICATION FINANCE : Analyse de corr\u00e9lation de portefeuille\\n\")\n",
    "\n",
    "# Simulation de rendements d'actifs\n",
    "np.random.seed(42)\n",
    "n_days = 252  # 1 an de trading\n",
    "n_assets = 5\n",
    "\n",
    "# Matrice de corr\u00e9lation r\u00e9aliste\n",
    "rho_matrix = np.array([\n",
    "    [1.0, 0.7, 0.5, 0.3, 0.1],   # Tech stock\n",
    "    [0.7, 1.0, 0.6, 0.4, 0.2],   # Tech stock 2\n",
    "    [0.5, 0.6, 1.0, 0.2, 0.0],   # Consumer goods\n",
    "    [0.3, 0.4, 0.2, 1.0, -0.3],  # Energy\n",
    "    [0.1, 0.2, 0.0, -0.3, 1.0]   # Gold (safe haven)\n",
    "])\n",
    "\n",
    "# Volatilit\u00e9s annuelles\n",
    "volatilities = np.array([0.25, 0.30, 0.20, 0.35, 0.15])  # 15-35% par an\n",
    "\n",
    "# Matrice de covariance (annualis\u00e9e)\n",
    "cov_matrix = np.outer(volatilities, volatilities) * rho_matrix\n",
    "\n",
    "# Simulation des rendements quotidiens\n",
    "mean_returns = np.array([0.10, 0.12, 0.08, 0.15, 0.05]) / 252  # Rendements annuels \u2192 quotidiens\n",
    "cov_daily = cov_matrix / 252  # Variance annuelle \u2192 quotidienne\n",
    "\n",
    "returns = np.random.multivariate_normal(mean_returns, cov_daily, n_days)\n",
    "\n",
    "asset_names = ['Tech A', 'Tech B', 'Consumer', 'Energy', 'Gold']\n",
    "\n",
    "# Visualisation de la matrice de corr\u00e9lation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap de corr\u00e9lation\n",
    "im = ax1.imshow(rho_matrix, cmap='RdYlGn', vmin=-1, vmax=1, aspect='auto')\n",
    "ax1.set_xticks(range(n_assets))\n",
    "ax1.set_yticks(range(n_assets))\n",
    "ax1.set_xticklabels(asset_names, rotation=45, ha='right')\n",
    "ax1.set_yticklabels(asset_names)\n",
    "ax1.set_title('Matrice de Corr\u00e9lation', fontweight='bold')\n",
    "\n",
    "# Annotations\n",
    "for i in range(n_assets):\n",
    "    for j in range(n_assets):\n",
    "        text = ax1.text(j, i, f'{rho_matrix[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax1, label='Corr\u00e9lation')\n",
    "\n",
    "# \u00c9volution des rendements cumul\u00e9s\n",
    "cumulative_returns = (1 + returns).cumprod(axis=0)\n",
    "for i, name in enumerate(asset_names):\n",
    "    ax2.plot(cumulative_returns[:, i], label=name, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Jours de trading')\n",
    "ax2.set_ylabel('Valeur du portefeuille (base 1.0)')\n",
    "ax2.set_title('\u00c9volution des actifs', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcul du risque de portefeuille\n",
    "weights = np.array([0.3, 0.2, 0.2, 0.2, 0.1])  # R\u00e9partition du portefeuille\n",
    "\n",
    "# Variance du portefeuille : w^T \u03a3 w\n",
    "portfolio_variance = weights @ cov_matrix @ weights\n",
    "portfolio_volatility = np.sqrt(portfolio_variance)\n",
    "\n",
    "# Rendement attendu du portefeuille\n",
    "annual_returns = np.array([0.10, 0.12, 0.08, 0.15, 0.05])\n",
    "portfolio_return = weights @ annual_returns\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Analyse du portefeuille :\")\n",
    "print(f\"\\nR\u00e9partition : {dict(zip(asset_names, weights))}\")\n",
    "print(f\"\\nRendement attendu : {portfolio_return*100:.2f}% par an\")\n",
    "print(f\"Volatilit\u00e9 (risque) : {portfolio_volatility*100:.2f}% par an\")\n",
    "print(f\"Ratio de Sharpe (simplifi\u00e9) : {portfolio_return/portfolio_volatility:.2f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Effet de diversification :\")\n",
    "print(f\"  \u2022 Gold a corr\u00e9lation n\u00e9gative avec Energy (-0.3)\")\n",
    "print(f\"  \u2022 \u21d2 R\u00e9duit le risque global du portefeuille\")\n",
    "print(f\"  \u2022 Tech A et Tech B tr\u00e8s corr\u00e9l\u00e9es (0.7) \u21d2 Moins de diversification\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u270f\ufe0f Pratique maintenant !\n",
    "\ud83d\udcc1 **Exercices** : `exercices_07_probabilites.ipynb` \u2192 Ex 1.1 \u00e0 1.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}