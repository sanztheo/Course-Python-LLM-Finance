{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Calcul Diff√©rentiel : Le Langage du Changement\n",
    "\n",
    "**Bienvenue dans le cours le plus important pour le Machine Learning !** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Pr√©requis\n",
    "\n",
    "Avant de commencer, assure-toi d'avoir :\n",
    "\n",
    "1. ‚úÖ Compris les fonctions (Maths_02_Fonctions.ipynb)\n",
    "2. ‚úÖ Ma√Ætris√© les bases (puissances, racines)\n",
    "3. ‚úÖ Activ√© l'environnement `llm`\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Ce que tu vas apprendre\n",
    "\n",
    "Le **calcul diff√©rentiel** √©tudie comment les choses **changent**. C'est LA fondation math√©matique du Machine Learning.\n",
    "\n",
    "**Contenu du cours :**\n",
    "1. üìä Limites et continuit√©\n",
    "2. üìà D√©riv√©es : taux de changement instantan√©\n",
    "3. ‚ö° R√®gles de d√©rivation\n",
    "4. üî¢ D√©riv√©es de fonctions courantes\n",
    "5. üßÆ D√©riv√©es partielles\n",
    "6. üéØ Gradient : direction de plus grande pente\n",
    "7. üîó R√®gle de la cha√Æne (chain rule)\n",
    "8. üìê Jacobienne et Hessienne\n",
    "9. üèîÔ∏è Optimisation : trouver minima/maxima\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Pourquoi c'est ESSENTIEL en ML ?\n",
    "\n",
    "Le calcul diff√©rentiel est au c≈ìur de l'apprentissage automatique :\n",
    "\n",
    "**1. Gradient Descent** üéØ\n",
    "- Algorithme fondamental pour entra√Æner les mod√®les\n",
    "- Utilise le gradient pour trouver les meilleurs param√®tres\n",
    "- Sans d√©riv√©es = pas d'optimisation = pas de ML !\n",
    "\n",
    "**2. Backpropagation** üîô\n",
    "- M√©thode pour entra√Æner les r√©seaux de neurones\n",
    "- Utilise la r√®gle de la cha√Æne pour calculer les gradients\n",
    "- C'est LA technique qui fait fonctionner les LLMs !\n",
    "\n",
    "**3. Loss Function Optimization** üìâ\n",
    "- Minimiser l'erreur du mod√®le\n",
    "- Trouver le minimum d'une fonction = utiliser les d√©riv√©es\n",
    "\n",
    "**4. Understanding Model Behavior** üß†\n",
    "- Sensibilit√© des pr√©dictions aux inputs\n",
    "- Explications et interpr√©tabilit√©\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Objectif :** √Ä la fin de ce cours, tu comprendras exactement comment fonctionne l'entra√Ænement d'un r√©seau de neurones, math√©matiquement parlant !\n",
    "\n",
    "Allons-y ! üí™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ Limites et Continuit√© : L'Intuition\n",
    "\n",
    "Avant de parler de d√©riv√©es, on doit comprendre les **limites**. C'est simple !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Qu'est-ce qu'une Limite ?\n",
    "\n",
    "**Intuition :** La limite, c'est la valeur vers laquelle une fonction **s'approche** quand $x$ s'approche d'une certaine valeur.\n",
    "\n",
    "**Notation :** $$\\lim_{x \\to a} f(x) = L$$\n",
    "\n",
    "Se lit : \"La limite de $f(x)$ quand $x$ tend vers $a$ est $L$\"\n",
    "\n",
    "**Exemple simple :**\n",
    "$$\\lim_{x \\to 2} (2x + 1) = 5$$\n",
    "\n",
    "Car quand $x$ s'approche de 2, $2x + 1$ s'approche de $2(2) + 1 = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "\n",
    "# Fonction : f(x) = 2x + 1\n",
    "def f(x):\n",
    "    return 2*x + 1\n",
    "\n",
    "# Approcher x = 2 par la gauche et la droite\n",
    "x_gauche = np.array([1.9, 1.99, 1.999, 1.9999])\n",
    "x_droite = np.array([2.1, 2.01, 2.001, 2.0001])\n",
    "\n",
    "print(\"Approche de x = 2 par la GAUCHE :\")\n",
    "for x in x_gauche:\n",
    "    print(f\"  x = {x:.4f} ‚Üí f(x) = {f(x):.4f}\")\n",
    "\n",
    "print(\"\\nApproche de x = 2 par la DROITE :\")\n",
    "for x in x_droite:\n",
    "    print(f\"  x = {x:.4f} ‚Üí f(x) = {f(x):.4f}\")\n",
    "\n",
    "print(f\"\\n‚Üí La limite quand x ‚Üí 2 est : {f(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Continuit√©\n",
    "\n",
    "Une fonction est **continue** en $x = a$ si :\n",
    "$$\\lim_{x \\to a} f(x) = f(a)$$\n",
    "\n",
    "**En clair :** Pas de saut, pas de trou, on peut tracer la courbe sans lever le crayon !\n",
    "\n",
    "**Exemples :**\n",
    "- $f(x) = x^2$ ‚Üí Continue partout ‚úÖ\n",
    "- $f(x) = \\frac{1}{x}$ ‚Üí Discontinue en $x = 0$ ‚ùå (division par z√©ro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation : fonction continue vs discontinue\n",
    "x = np.linspace(-3, 3, 400)\n",
    "\n",
    "# Fonction continue\n",
    "y_continue = x**2\n",
    "\n",
    "# Fonction discontinue (√©viter x=0)\n",
    "x_disc = np.linspace(-3, 3, 400)\n",
    "x_disc = x_disc[x_disc != 0]  # Enlever 0\n",
    "y_discontinue = 1 / x_disc\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Continue\n",
    "ax1.plot(x, y_continue, 'b-', linewidth=2)\n",
    "ax1.set_title('Fonction Continue : f(x) = x¬≤', fontsize=14, fontweight='bold')\n",
    "ax1.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax1.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(-1, 9)\n",
    "ax1.text(0, 8, 'Pas de saut, pas de trou ‚úì', fontsize=11, ha='center')\n",
    "\n",
    "# Discontinue\n",
    "ax2.plot(x_disc[x_disc < 0], y_discontinue[x_disc < 0], 'r-', linewidth=2)\n",
    "ax2.plot(x_disc[x_disc > 0], y_discontinue[x_disc > 0], 'r-', linewidth=2)\n",
    "ax2.axvline(x=0, color='orange', linewidth=3, linestyle='--', label='Discontinuit√© en x=0')\n",
    "ax2.set_title('Fonction Discontinue : f(x) = 1/x', fontsize=14, fontweight='bold')\n",
    "ax2.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(-5, 5)\n",
    "ax2.legend()\n",
    "ax2.text(0, 4.5, 'Division par z√©ro ! ‚úó', fontsize=11, ha='center', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚úèÔ∏è Exercices correspondants :** [Exercices 1.1 √† 1.5 - Limites](../../envs/phase_1_math/exercices_05_calcul_differentiel.ipynb#Section-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ D√©riv√©es : Le Taux de Changement Instantan√©\n",
    "\n",
    "La **d√©riv√©e** mesure √† quelle vitesse une fonction change. C'est LE concept fondamental du ML !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà D√©finition G√©om√©trique : La Pente\n",
    "\n",
    "**Question :** Quelle est la pente de la courbe $y = f(x)$ en un point ?\n",
    "\n",
    "**R√©ponse :** C'est la pente de la **tangente** √† la courbe en ce point !\n",
    "\n",
    "**Intuition :**\n",
    "- Pente positive ‚Üí fonction croissante üìà\n",
    "- Pente n√©gative ‚Üí fonction d√©croissante üìâ\n",
    "- Pente nulle ‚Üí point plat (peut-√™tre un minimum ou maximum !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation : Tangente √† une courbe\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def tangente(x, a):\n",
    "    # Tangente √† f(x) = x¬≤ au point a\n",
    "    # f'(x) = 2x, donc pente en a = 2a\n",
    "    pente = 2 * a\n",
    "    return pente * (x - a) + f(a)\n",
    "\n",
    "x = np.linspace(-1, 3, 300)\n",
    "y = f(x)\n",
    "\n",
    "# Points o√π on trace les tangentes\n",
    "points = [0.5, 1, 2]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(x, y, 'k-', linewidth=3, label='f(x) = x¬≤')\n",
    "\n",
    "for pt, color in zip(points, colors):\n",
    "    # Point sur la courbe\n",
    "    plt.plot(pt, f(pt), 'o', color=color, markersize=10)\n",
    "    \n",
    "    # Tangente\n",
    "    x_tang = np.linspace(pt - 0.8, pt + 0.8, 100)\n",
    "    y_tang = tangente(x_tang, pt)\n",
    "    pente = 2 * pt\n",
    "    plt.plot(x_tang, y_tang, '--', color=color, linewidth=2, \n",
    "             label=f'Tangente en x={pt} (pente={pente:.1f})')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('D√©riv√©e = Pente de la Tangente', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Remarque : Plus x augmente, plus la pente (d√©riv√©e) augmente !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßÆ D√©finition Analytique\n",
    "\n",
    "**D√©finition formelle :** La d√©riv√©e de $f$ en $x$ est :\n",
    "\n",
    "$$f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n",
    "\n",
    "**Notations √©quivalentes :**\n",
    "- $f'(x)$ (notation de Lagrange)\n",
    "- $\\frac{df}{dx}$ (notation de Leibniz)\n",
    "- $\\frac{d}{dx}f(x)$ (op√©rateur de d√©rivation)\n",
    "\n",
    "**Interpr√©tation :**\n",
    "- $h$ = petit changement en $x$\n",
    "- $f(x + h) - f(x)$ = changement correspondant en $f$\n",
    "- $\\frac{f(x + h) - f(x)}{h}$ = taux de changement moyen\n",
    "- $\\lim_{h \\to 0}$ = taux de changement **instantan√©**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la d√©riv√©e num√©riquement avec la d√©finition\n",
    "def derivee_numerique(f, x, h=1e-5):\n",
    "    \"\"\"Approximation num√©rique de f'(x)\"\"\"\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "# Fonction : f(x) = x¬≤\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# D√©riv√©e analytique : f'(x) = 2x\n",
    "def f_prime_analytique(x):\n",
    "    return 2*x\n",
    "\n",
    "# Comparer pour diff√©rentes valeurs de x\n",
    "x_vals = [0, 1, 2, 3, 5]\n",
    "\n",
    "print(\"Comparaison : D√©riv√©e Num√©rique vs Analytique\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'x':<8} {'Num√©rique':<15} {'Analytique':<15} {'Diff√©rence'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for x in x_vals:\n",
    "    num = derivee_numerique(f, x)\n",
    "    ana = f_prime_analytique(x)\n",
    "    diff = abs(num - ana)\n",
    "    print(f\"{x:<8} {num:<15.10f} {ana:<15.10f} {diff:.2e}\")\n",
    "\n",
    "print(\"\\nRemarque : Les deux m√©thodes donnent (presque) le m√™me r√©sultat !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Pourquoi c'est CRUCIAL en ML : Gradient Descent\n",
    "\n",
    "**Probl√®me :** Minimiser une fonction de co√ªt (loss function) $L(\\theta)$\n",
    "\n",
    "**Solution : Gradient Descent**\n",
    "1. Calculer la d√©riv√©e : $\\frac{dL}{d\\theta}$\n",
    "2. Mettre √† jour les param√®tres : $\\theta_{new} = \\theta_{old} - \\alpha \\frac{dL}{d\\theta}$\n",
    "3. R√©p√©ter jusqu'√† convergence\n",
    "\n",
    "**Sans d√©riv√©es, pas de Gradient Descent = pas de ML moderne !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple : Minimiser f(x) = (x - 3)¬≤ avec Gradient Descent\n",
    "def loss(x):\n",
    "    return (x - 3)**2\n",
    "\n",
    "def gradient(x):\n",
    "    # D√©riv√©e de (x - 3)¬≤ = 2(x - 3)\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "# Gradient Descent\n",
    "x = 0  # Point de d√©part\n",
    "alpha = 0.1  # Learning rate\n",
    "iterations = 20\n",
    "history = [x]\n",
    "\n",
    "print(\"Gradient Descent en action :\")\n",
    "print(f\"{'Iter':<6} {'x':<12} {'Loss':<12} {'Gradient'}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(iterations):\n",
    "    grad = gradient(x)\n",
    "    x = x - alpha * grad  # Mise √† jour\n",
    "    history.append(x)\n",
    "    \n",
    "    if i % 2 == 0:  # Afficher toutes les 2 it√©rations\n",
    "        print(f\"{i:<6} {x:<12.6f} {loss(x):<12.6f} {grad:<12.6f}\")\n",
    "\n",
    "print(f\"\\nMinimum trouv√© en x = {x:.6f} (vrai minimum: x = 3)\")\n",
    "\n",
    "# Visualisation\n",
    "x_plot = np.linspace(-1, 5, 300)\n",
    "y_plot = loss(x_plot)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='Loss: f(x) = (x-3)¬≤')\n",
    "plt.plot(history, [loss(x) for x in history], 'ro-', markersize=6, linewidth=1.5, \n",
    "         label='Gradient Descent')\n",
    "plt.plot(history[0], loss(history[0]), 'go', markersize=12, label='D√©part')\n",
    "plt.plot(history[-1], loss(history[-1]), 'r*', markersize=20, label='Arriv√©e')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Gradient Descent : Descendre vers le Minimum', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚úèÔ∏è Exercices correspondants :** [Exercices 2.1 √† 2.6 - D√©riv√©es Basiques](../../envs/phase_1_math/exercices_05_calcul_differentiel.ipynb#Section-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ R√®gles de D√©rivation\n",
    "\n",
    "Calculer la d√©riv√©e avec la d√©finition est fastidieux. Heureusement, il y a des **r√®gles** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ûï R√®gle 1 : Somme/Diff√©rence\n",
    "\n",
    "$$\\frac{d}{dx}[f(x) + g(x)] = f'(x) + g'(x)$$\n",
    "\n",
    "$$\\frac{d}{dx}[f(x) - g(x)] = f'(x) - g'(x)$$\n",
    "\n",
    "**En clair :** La d√©riv√©e d'une somme = somme des d√©riv√©es !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úñÔ∏è R√®gle 2 : Produit (Product Rule)\n",
    "\n",
    "$$\\frac{d}{dx}[f(x) \\cdot g(x)] = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)$$\n",
    "\n",
    "**Exemple :** $\\frac{d}{dx}[x^2 \\cdot e^x] = 2x \\cdot e^x + x^2 \\cdot e^x = (2x + x^2)e^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ûó R√®gle 3 : Quotient (Quotient Rule)\n",
    "\n",
    "$$\\frac{d}{dx}\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{[g(x)]^2}$$\n",
    "\n",
    "**Moyen mn√©motechnique :** \"Lo D Hi minus Hi D Lo over Lo Lo\"\n",
    "- Lo = fonction du bas (Low)\n",
    "- Hi = fonction du haut (High)\n",
    "- D = d√©riv√©e\n",
    "\n",
    "**Exemple :** $\\frac{d}{dx}\\left[\\frac{x^2}{x+1}\\right] = \\frac{2x(x+1) - x^2(1)}{(x+1)^2} = \\frac{x^2 + 2x}{(x+1)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¢ R√®gle 4 : Puissance (Power Rule)\n",
    "\n",
    "$$\\frac{d}{dx}[x^n] = n \\cdot x^{n-1}$$\n",
    "\n",
    "**Exemples :**\n",
    "- $\\frac{d}{dx}[x^2] = 2x$\n",
    "- $\\frac{d}{dx}[x^3] = 3x^2$\n",
    "- $\\frac{d}{dx}[x^{10}] = 10x^9$\n",
    "- $\\frac{d}{dx}[\\sqrt{x}] = \\frac{d}{dx}[x^{1/2}] = \\frac{1}{2}x^{-1/2} = \\frac{1}{2\\sqrt{x}}$\n",
    "\n",
    "**C'est la r√®gle la plus utilis√©e !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification avec SymPy (calcul symbolique)\n",
    "import sympy as sp\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "# Exemples de r√®gles\n",
    "fonctions = [\n",
    "    x**2 + x**3,           # Somme\n",
    "    x**2 * sp.exp(x),      # Produit\n",
    "    x**2 / (x + 1),        # Quotient\n",
    "    x**5,                  # Puissance\n",
    "    sp.sqrt(x)             # Racine\n",
    "]\n",
    "\n",
    "print(\"D√©riv√©es avec SymPy (calcul symbolique) :\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for f in fonctions:\n",
    "    f_prime = sp.diff(f, x)\n",
    "    print(f\"f(x)  = {f}\")\n",
    "    print(f\"f'(x) = {f_prime}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚úèÔ∏è Exercices correspondants :** [Exercices 3.1 √† 3.6 - R√®gles de D√©rivation](../../envs/phase_1_math/exercices_05_calcul_differentiel.ipynb#Section-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ D√©riv√©es de Fonctions Courantes\n",
    "\n",
    "Voici les d√©riv√©es des fonctions que tu rencontreras constamment en ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Table des D√©riv√©es Essentielles\n",
    "\n",
    "| Fonction $f(x)$ | D√©riv√©e $f'(x)$ | Notes |\n",
    "|----------------|----------------|-------|\n",
    "| $c$ (constante) | $0$ | Constante ‚Üí pas de changement |\n",
    "| $x^n$ | $nx^{n-1}$ | Power rule |\n",
    "| $e^x$ | $e^x$ | Se d√©rive en elle-m√™me ! |\n",
    "| $\\ln(x)$ | $\\frac{1}{x}$ | Logarithme naturel |\n",
    "| $\\sin(x)$ | $\\cos(x)$ | |\n",
    "| $\\cos(x)$ | $-\\sin(x)$ | Attention au signe ! |\n",
    "| $\\tan(x)$ | $\\sec^2(x) = \\frac{1}{\\cos^2(x)}$ | |\n",
    "| $a^x$ | $a^x \\ln(a)$ | Exponentielle de base $a$ |\n",
    "| $\\log_a(x)$ | $\\frac{1}{x \\ln(a)}$ | Logarithme de base $a$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des fonctions et leurs d√©riv√©es\n",
    "x = np.linspace(-3, 3, 400)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Polyn√¥me : x¬≤\n",
    "ax = axes[0, 0]\n",
    "ax.plot(x, x**2, 'b-', linewidth=2, label='f(x) = x¬≤')\n",
    "ax.plot(x, 2*x, 'r--', linewidth=2, label=\"f'(x) = 2x\")\n",
    "ax.set_title('Polyn√¥me', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(0, color='k', linewidth=0.5)\n",
    "ax.axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "# 2. Exponentielle : e^x\n",
    "ax = axes[0, 1]\n",
    "x_pos = x[x >= -2]\n",
    "ax.plot(x_pos, np.exp(x_pos), 'b-', linewidth=2, label='f(x) = eÀ£')\n",
    "ax.plot(x_pos, np.exp(x_pos), 'r--', linewidth=2, label=\"f'(x) = eÀ£\")\n",
    "ax.set_title('Exponentielle (se d√©rive en elle-m√™me !)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "# 3. Logarithme : ln(x)\n",
    "ax = axes[1, 0]\n",
    "x_log = np.linspace(0.1, 3, 300)\n",
    "ax.plot(x_log, np.log(x_log), 'b-', linewidth=2, label='f(x) = ln(x)')\n",
    "ax.plot(x_log, 1/x_log, 'r--', linewidth=2, label=\"f'(x) = 1/x\")\n",
    "ax.set_title('Logarithme Naturel', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(0, color='k', linewidth=0.5)\n",
    "\n",
    "# 4. Sinus et Cosinus\n",
    "ax = axes[1, 1]\n",
    "ax.plot(x, np.sin(x), 'b-', linewidth=2, label='f(x) = sin(x)')\n",
    "ax.plot(x, np.cos(x), 'r--', linewidth=2, label=\"f'(x) = cos(x)\")\n",
    "ax.set_title('Trigonom√©trie', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(0, color='k', linewidth=0.5)\n",
    "ax.axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Remarque : La d√©riv√©e nous dit comment la fonction √©volue !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Fonctions d'Activation en ML\n",
    "\n",
    "Les fonctions d'activation sont CRUCIALES dans les r√©seaux de neurones. Voici leurs d√©riv√©es :\n",
    "\n",
    "**1. Sigmoid** (classification binaire)\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "**2. ReLU** (Rectified Linear Unit - la plus populaire !)\n",
    "$$\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} x & \\text{si } x > 0 \\\\ 0 & \\text{sinon} \\end{cases}$$\n",
    "$$\\text{ReLU}'(x) = \\begin{cases} 1 & \\text{si } x > 0 \\\\ 0 & \\text{sinon} \\end{cases}$$\n",
    "\n",
    "**3. Tanh** (Tangente hyperbolique)\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "$$\\tanh'(x) = 1 - \\tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions d'activation et leurs d√©riv√©es\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "x = np.linspace(-5, 5, 400)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0, 0].plot(x, sigmoid(x), 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Sigmoid: œÉ(x)', fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(0, color='k', linewidth=0.5)\n",
    "axes[0, 0].axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "axes[0, 1].plot(x, sigmoid_prime(x), 'r-', linewidth=2)\n",
    "axes[0, 1].set_title(\"œÉ'(x) = œÉ(x)(1-œÉ(x))\", fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[0, 1].axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "# ReLU\n",
    "axes[1, 0].plot(x, relu(x), 'b-', linewidth=2)\n",
    "axes[1, 0].set_title('ReLU: max(0, x)', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(0, color='k', linewidth=0.5)\n",
    "axes[1, 0].axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "axes[1, 1].plot(x, relu_prime(x), 'r-', linewidth=2)\n",
    "axes[1, 1].set_title(\"ReLU'(x) = 0 ou 1\", fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[1, 1].axvline(0, color='k', linewidth=0.5)\n",
    "axes[1, 1].set_ylim(-0.2, 1.2)\n",
    "\n",
    "# Tanh\n",
    "axes[2, 0].plot(x, tanh(x), 'b-', linewidth=2)\n",
    "axes[2, 0].set_title('Tanh: tanh(x)', fontweight='bold')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "axes[2, 0].axhline(0, color='k', linewidth=0.5)\n",
    "axes[2, 0].axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "axes[2, 1].plot(x, tanh_prime(x), 'r-', linewidth=2)\n",
    "axes[2, 1].set_title(\"tanh'(x) = 1 - tanh¬≤(x)\", fontweight='bold')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "axes[2, 1].axhline(0, color='k', linewidth=0.5)\n",
    "axes[2, 1].axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Ces d√©riv√©es sont utilis√©es dans la backpropagation !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ D√©riv√©es Partielles : Fonctions √† Plusieurs Variables\n",
    "\n",
    "En ML, on travaille avec des fonctions qui d√©pendent de PLUSIEURS variables (poids, biais, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßÆ Qu'est-ce qu'une D√©riv√©e Partielle ?\n",
    "\n",
    "Pour une fonction $f(x, y)$ de **deux variables**, on peut calculer :\n",
    "\n",
    "**D√©riv√©e partielle par rapport √† $x$** (on traite $y$ comme une constante) :\n",
    "$$\\frac{\\partial f}{\\partial x}$$\n",
    "\n",
    "**D√©riv√©e partielle par rapport √† $y$** (on traite $x$ comme une constante) :\n",
    "$$\\frac{\\partial f}{\\partial y}$$\n",
    "\n",
    "**Notation :** Le symbole $\\partial$ (curly d) remplace $d$ pour les d√©riv√©es partielles.\n",
    "\n",
    "**Exemple :** $f(x, y) = x^2y + 3xy^2$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = 2xy + 3y^2 \\quad \\text{(on d√©rive par rapport √† } x \\text{, } y \\text{ est constant)}$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial y} = x^2 + 6xy \\quad \\text{(on d√©rive par rapport √† } y \\text{, } x \\text{ est constant)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©riv√©es partielles avec SymPy\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Fonction : f(x, y) = x¬≤y + 3xy¬≤\n",
    "f = x**2 * y + 3*x*y**2\n",
    "\n",
    "# D√©riv√©es partielles\n",
    "df_dx = sp.diff(f, x)  # ‚àÇf/‚àÇx\n",
    "df_dy = sp.diff(f, y)  # ‚àÇf/‚àÇy\n",
    "\n",
    "print(\"Fonction : f(x, y) =\", f)\n",
    "print(\"\\nD√©riv√©e partielle par rapport √† x:\")\n",
    "print(\"‚àÇf/‚àÇx =\", df_dx)\n",
    "print(\"\\nD√©riv√©e partielle par rapport √† y:\")\n",
    "print(\"‚àÇf/‚àÇy =\", df_dy)\n",
    "\n",
    "# √âvaluation num√©rique en (x=2, y=3)\n",
    "val_df_dx = df_dx.subs([(x, 2), (y, 3)])\n",
    "val_df_dy = df_dy.subs([(x, 2), (y, 3)])\n",
    "\n",
    "print(\"\\n√âvaluation en (x=2, y=3):\")\n",
    "print(f\"‚àÇf/‚àÇx(2, 3) = {val_df_dx}\")\n",
    "print(f\"‚àÇf/‚àÇy(2, 3) = {val_df_dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualisation : Surface 3D et D√©riv√©es Partielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Fonction simple : f(x, y) = x¬≤ + y¬≤\n",
    "def f_2d(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Cr√©er une grille\n",
    "x_range = np.linspace(-3, 3, 50)\n",
    "y_range = np.linspace(-3, 3, 50)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f_2d(X, Y)\n",
    "\n",
    "# Plot 3D\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Surface 3D\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x, y)')\n",
    "ax1.set_title('Surface : f(x, y) = x¬≤ + y¬≤', fontweight='bold')\n",
    "\n",
    "# Contour plot avec gradients\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Ajouter quelques vecteurs de gradient\n",
    "points = [(1, 1), (-1, 1), (1, -1), (-1, -1), (2, 0), (0, 2)]\n",
    "for px, py in points:\n",
    "    # Gradient = (‚àÇf/‚àÇx, ‚àÇf/‚àÇy) = (2x, 2y)\n",
    "    grad_x = 2 * px\n",
    "    grad_y = 2 * py\n",
    "    ax2.arrow(px, py, -grad_x*0.2, -grad_y*0.2, \n",
    "              head_width=0.2, head_length=0.2, fc='red', ec='red')\n",
    "\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contours et Gradients (fl√®ches rouges)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(0, color='k', linewidth=0.5)\n",
    "ax2.axvline(0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Les fl√®ches rouges pointent vers la DESCENTE la plus rapide (gradient n√©gatif)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚úèÔ∏è Exercices correspondants :** [Exercices 4.1 √† 4.5 - D√©riv√©es Partielles](../../envs/phase_1_math/exercices_05_calcul_differentiel.ipynb#Section-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Le Gradient : Direction de Plus Grande Pente\n",
    "\n",
    "Le **gradient** est LE concept central du Machine Learning moderne !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ D√©finition du Gradient\n",
    "\n",
    "Pour une fonction $f(x_1, x_2, ..., x_n)$ de $n$ variables, le **gradient** est le vecteur de toutes les d√©riv√©es partielles :\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "**Notation :** $\\nabla$ (nabla) est le symbole du gradient.\n",
    "\n",
    "**Interpr√©tation g√©om√©trique :**\n",
    "- Le gradient **pointe** dans la direction de la **plus grande mont√©e**\n",
    "- Sa **norme** (longueur) indique la **pente** (√† quel point √ßa monte)\n",
    "- Le **gradient n√©gatif** $-\\nabla f$ pointe vers la **plus grande descente**\n",
    "\n",
    "**Exemple :** $f(x, y) = x^2 + y^2$\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix}$$\n",
    "\n",
    "En $(x=1, y=2)$ : $\\nabla f(1, 2) = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer et visualiser le gradient\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradient_f(x, y):\n",
    "    # Gradient = (‚àÇf/‚àÇx, ‚àÇf/‚àÇy) = (2x, 2y)\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "# Quelques points\n",
    "points = [(1, 2), (-2, 1), (2, -1), (-1, -2)]\n",
    "\n",
    "print(\"Calcul du gradient en diff√©rents points :\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Point (x, y)':<20} {'Gradient ‚àáf':<25} {'Norme ||‚àáf||'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for px, py in points:\n",
    "    grad = gradient_f(px, py)\n",
    "    norme = np.linalg.norm(grad)\n",
    "    print(f\"({px:3}, {py:3}){' '*12} [{grad[0]:4.1f}, {grad[1]:4.1f}]{' '*12} {norme:6.2f}\")\n",
    "\n",
    "print(\"\\nRemarque : Plus on est loin de l'origine, plus le gradient est grand !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Gradient Descent Multivari√©\n",
    "\n",
    "**Algorithme de Gradient Descent** (version g√©n√©rale) :\n",
    "\n",
    "$$\\mathbf{x}_{new} = \\mathbf{x}_{old} - \\alpha \\nabla f(\\mathbf{x}_{old})$$\n",
    "\n",
    "O√π :\n",
    "- $\\mathbf{x}$ = vecteur de param√®tres\n",
    "- $\\alpha$ = learning rate (taux d'apprentissage)\n",
    "- $\\nabla f$ = gradient de la fonction de co√ªt\n",
    "\n",
    "**C'est EXACTEMENT ce qui se passe dans l'entra√Ænement d'un r√©seau de neurones !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent 2D : Minimiser f(x, y) = x¬≤ + y¬≤\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradient_f(x, y):\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "# Point de d√©part\n",
    "x = np.array([3.0, 2.0])\n",
    "alpha = 0.1\n",
    "iterations = 20\n",
    "history = [x.copy()]\n",
    "\n",
    "print(\"Gradient Descent 2D :\")\n",
    "print(f\"{'Iter':<6} {'x':<12} {'y':<12} {'f(x,y)'}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(iterations):\n",
    "    grad = gradient_f(x[0], x[1])\n",
    "    x = x - alpha * grad\n",
    "    history.append(x.copy())\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        print(f\"{i:<6} {x[0]:<12.6f} {x[1]:<12.6f} {f(x[0], x[1]):<12.6f}\")\n",
    "\n",
    "print(f\"\\nMinimum trouv√© en (x, y) = ({x[0]:.6f}, {x[1]:.6f})\")\n",
    "print(f\"Vrai minimum : (0, 0)\")\n",
    "\n",
    "# Visualisation\n",
    "history = np.array(history)\n",
    "\n",
    "# Cr√©er grille de contours\n",
    "x_range = np.linspace(-4, 4, 100)\n",
    "y_range = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f(X, Y)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "contour = plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "plt.plot(history[:, 0], history[:, 1], 'ro-', linewidth=2, markersize=6, label='Gradient Descent')\n",
    "plt.plot(history[0, 0], history[0, 1], 'go', markersize=12, label='D√©part')\n",
    "plt.plot(history[-1, 0], history[-1, 1], 'r*', markersize=20, label='Arriv√©e')\n",
    "plt.plot(0, 0, 'ks', markersize=10, label='Vrai minimum')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Gradient Descent 2D : Descente vers le Minimum', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚úèÔ∏è Exercices correspondants :** [Exercices 5.1 √† 5.6 - Gradient](../../envs/phase_1_math/exercices_05_calcul_differentiel.ipynb#Section-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ La R√®gle de la Cha√Æne : Cl√© de la Backpropagation\n",
    "\n",
    "La **r√®gle de la cha√Æne** (chain rule) est LA technique qui permet d'entra√Æner les r√©seaux de neurones !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó R√®gle de la Cha√Æne (Version Simple)\n",
    "\n",
    "**Pour une fonction compos√©e** $h(x) = f(g(x))$ :\n",
    "\n",
    "$$\\frac{dh}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "**En clair :** Pour d√©river une fonction compos√©e, on multiplie les d√©riv√©es !\n",
    "\n",
    "**Exemple :** $h(x) = (x^2 + 1)^3$\n",
    "\n",
    "Pose : $g(x) = x^2 + 1$ et $f(g) = g^3$\n",
    "\n",
    "Alors :\n",
    "- $\\frac{dg}{dx} = 2x$\n",
    "- $\\frac{df}{dg} = 3g^2 = 3(x^2 + 1)^2$\n",
    "\n",
    "$$\\frac{dh}{dx} = 3(x^2 + 1)^2 \\cdot 2x = 6x(x^2 + 1)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification avec SymPy\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "# Fonction compos√©e : h(x) = (x¬≤ + 1)¬≥\n",
    "h = (x**2 + 1)**3\n",
    "\n",
    "# D√©riv√©e avec SymPy\n",
    "dh_dx = sp.diff(h, x)\n",
    "\n",
    "print(\"Fonction compos√©e : h(x) = (x¬≤ + 1)¬≥\")\n",
    "print(\"\\nD√©riv√©e avec la r√®gle de la cha√Æne :\")\n",
    "print(\"h'(x) =\", dh_dx)\n",
    "print(\"\\nFormulaire d√©velopp√© :\")\n",
    "print(\"h'(x) =\", sp.expand(dh_dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó R√®gle de la Cha√Æne (Version Multivari√©e)\n",
    "\n",
    "**Pour une fonction compos√©e** $z = f(x, y)$ o√π $x = g(t)$ et $y = h(t)$ :\n",
    "\n",
    "$$\\frac{dz}{dt} = \\frac{\\partial f}{\\partial x} \\cdot \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\cdot \\frac{dy}{dt}$$\n",
    "\n",
    "**Version g√©n√©rale (pour r√©seaux de neurones) :**\n",
    "\n",
    "Si $z$ d√©pend de $y$ qui d√©pend de $x$ :\n",
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Application : Backpropagation dans un R√©seau de Neurones\n",
    "\n",
    "**Architecture simple** (2 couches) :\n",
    "$$x \\xrightarrow{w_1} z_1 = w_1 x \\xrightarrow{\\sigma} a_1 = \\sigma(z_1) \\xrightarrow{w_2} z_2 = w_2 a_1 \\xrightarrow{\\sigma} a_2 = \\sigma(z_2) \\rightarrow L$$\n",
    "\n",
    "O√π $L$ est la loss (erreur).\n",
    "\n",
    "**Question :** Comment calculer $\\frac{\\partial L}{\\partial w_1}$ pour mettre √† jour $w_1$ ?\n",
    "\n",
    "**R√©ponse : Backpropagation avec la r√®gle de la cha√Æne !**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1}$$\n",
    "\n",
    "On calcule de la **droite vers la gauche** (d'o√π \"back\" propagation) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple de backpropagation\n",
    "# Architecture : x -> w1*x -> sigmoid -> w2*a -> sigmoid -> loss\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Forward pass\n",
    "x = 2.0\n",
    "y_true = 0.8  # Valeur cible\n",
    "\n",
    "w1 = 0.5\n",
    "w2 = 0.3\n",
    "\n",
    "# Couche 1\n",
    "z1 = w1 * x\n",
    "a1 = sigmoid(z1)\n",
    "\n",
    "# Couche 2  \n",
    "z2 = w2 * a1\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "# Loss (erreur quadratique)\n",
    "loss = (a2 - y_true)**2\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FORWARD PASS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input x = {x}\")\n",
    "print(f\"Weights: w1 = {w1}, w2 = {w2}\")\n",
    "print(f\"\\nCouche 1: z1 = w1*x = {z1:.4f}, a1 = œÉ(z1) = {a1:.4f}\")\n",
    "print(f\"Couche 2: z2 = w2*a1 = {z2:.4f}, a2 = œÉ(z2) = {a2:.4f}\")\n",
    "print(f\"\\nPr√©diction: {a2:.4f}, Cible: {y_true}\")\n",
    "print(f\"Loss: {loss:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BACKWARD PASS (Backpropagation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Backward pass (r√®gle de la cha√Æne)\n",
    "# ‚àÇL/‚àÇa2\n",
    "dL_da2 = 2 * (a2 - y_true)\n",
    "print(f\"‚àÇL/‚àÇa2 = {dL_da2:.6f}\")\n",
    "\n",
    "# ‚àÇL/‚àÇz2 = ‚àÇL/‚àÇa2 * ‚àÇa2/‚àÇz2\n",
    "da2_dz2 = sigmoid_derivative(z2)\n",
    "dL_dz2 = dL_da2 * da2_dz2\n",
    "print(f\"‚àÇL/‚àÇz2 = ‚àÇL/‚àÇa2 √ó ‚àÇa2/‚àÇz2 = {dL_da2:.6f} √ó {da2_dz2:.6f} = {dL_dz2:.6f}\")\n",
    "\n",
    "# ‚àÇL/‚àÇw2 = ‚àÇL/‚àÇz2 * ‚àÇz2/‚àÇw2\n",
    "dz2_dw2 = a1\n",
    "dL_dw2 = dL_dz2 * dz2_dw2\n",
    "print(f\"‚àÇL/‚àÇw2 = ‚àÇL/‚àÇz2 √ó ‚àÇz2/‚àÇw2 = {dL_dz2:.6f} √ó {dz2_dw2:.6f} = {dL_dw2:.6f}\")\n",
    "\n",
    "# ‚àÇL/‚àÇa1 = ‚àÇL/‚àÇz2 * ‚àÇz2/‚àÇa1\n",
    "dz2_da1 = w2\n",
    "dL_da1 = dL_dz2 * dz2_da1\n",
    "print(f\"‚àÇL/‚àÇa1 = ‚àÇL/‚àÇz2 √ó ‚àÇz2/‚àÇa1 = {dL_dz2:.6f} √ó {dz2_da1:.6f} = {dL_da1:.6f}\")\n",
    "\n",
    "# ‚àÇL/‚àÇz1 = ‚àÇL/‚àÇa1 * ‚àÇa1/‚àÇz1\n",
    "da1_dz1 = sigmoid_derivative(z1)\n",
    "dL_dz1 = dL_da1 * da1_dz1\n",
    "print(f\"‚àÇL/‚àÇz1 = ‚àÇL/‚àÇa1 √ó ‚àÇa1/‚àÇz1 = {dL_da1:.6f} √ó {da1_dz1:.6f} = {dL_dz1:.6f}\")\n",
    "\n",
    "# ‚àÇL/‚àÇw1 = ‚àÇL/‚àÇz1 * ‚àÇz1/‚àÇw1\n",
    "dz1_dw1 = x\n",
    "dL_dw1 = dL_dz1 * dz1_dw1\n",
    "print(f\"‚àÇL/‚àÇw1 = ‚àÇL/‚àÇz1 √ó ‚àÇz1/‚àÇw1 = {dL_dz1:.6f} √ó {dz1_dw1:.6f} = {dL_dw1:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISE √Ä JOUR DES POIDS (Gradient Descent)\")\n",
    "print(\"=\"*60)\n",
    "alpha = 0.5  # Learning rate\n",
    "w1_new = w1 - alpha * dL_dw1\n",
    "w2_new = w2 - alpha * dL_dw2\n",
    "\n",
    "print(f\"Learning rate Œ± = {alpha}\")\n",
    "print(f\"\\nw1: {w1:.4f} ‚Üí {w1_new:.4f} (changement: {w1_new - w1:.4f})\")\n",
    "print(f\"w2: {w2:.4f} ‚Üí {w2_new:.4f} (changement: {w2_new - w2:.4f})\")\n",
    "print(\"\\nüéâ Voil√† comment fonctionne l'entra√Ænement d'un r√©seau de neurones !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚úèÔ∏è Exercices correspondants :** [Exercices 6.1 √† 6.6 - Chain Rule](../../envs/phase_1_math/exercices_05_calcul_differentiel.ipynb#Section-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ Jacobienne et Hessienne : Concepts Avanc√©s\n",
    "\n",
    "Deux matrices importantes pour l'optimisation avanc√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê Matrice Jacobienne\n",
    "\n",
    "Pour une fonction vectorielle $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m$ :\n",
    "\n",
    "$$\\mathbf{f}(\\mathbf{x}) = \\begin{bmatrix} f_1(x_1, ..., x_n) \\\\ f_2(x_1, ..., x_n) \\\\ \\vdots \\\\ f_m(x_1, ..., x_n) \\end{bmatrix}$$\n",
    "\n",
    "La **Jacobienne** $J$ est la matrice de toutes les d√©riv√©es partielles :\n",
    "\n",
    "$$J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "**Utilit√© en ML :** Backpropagation vectorielle, calcul de gradients dans des couches complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Jacobienne d'une transformation\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Fonction vectorielle : f(x, y) = [x¬≤+y, xy, y¬≤]\n",
    "f1 = x**2 + y\n",
    "f2 = x * y\n",
    "f3 = y**2\n",
    "\n",
    "# Calcul de la Jacobienne\n",
    "J = sp.Matrix([[sp.diff(f1, x), sp.diff(f1, y)],\n",
    "               [sp.diff(f2, x), sp.diff(f2, y)],\n",
    "               [sp.diff(f3, x), sp.diff(f3, y)]])\n",
    "\n",
    "print(\"Fonction vectorielle :\")\n",
    "print(f\"f‚ÇÅ(x, y) = {f1}\")\n",
    "print(f\"f‚ÇÇ(x, y) = {f2}\")\n",
    "print(f\"f‚ÇÉ(x, y) = {f3}\")\n",
    "print(\"\\nMatrice Jacobienne J :\")\n",
    "print(J)\n",
    "\n",
    "# √âvaluation num√©rique en (x=2, y=3)\n",
    "J_num = J.subs([(x, 2), (y, 3)])\n",
    "print(\"\\nJ(2, 3) =\")\n",
    "print(J_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê Matrice Hessienne\n",
    "\n",
    "Pour une fonction scalaire $f: \\mathbb{R}^n \\to \\mathbb{R}$, la **Hessienne** $H$ est la matrice des d√©riv√©es secondes :\n",
    "\n",
    "$$H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots \\\\ \\vdots & \\vdots & \\ddots \\end{bmatrix}$$\n",
    "\n",
    "**Utilit√© en ML :**\n",
    "- Optimisation de second ordre (Newton's method)\n",
    "- Analyse de la courbure de la loss function\n",
    "- D√©tection de minima/maxima/points-selles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Hessienne\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Fonction : f(x, y) = x¬≤y + xy¬≤\n",
    "f = x**2 * y + x * y**2\n",
    "\n",
    "# Calcul de la Hessienne\n",
    "H = sp.hessian(f, (x, y))\n",
    "\n",
    "print(\"Fonction : f(x, y) =\", f)\n",
    "print(\"\\nMatrice Hessienne H :\")\n",
    "print(H)\n",
    "\n",
    "# √âvaluation en (x=1, y=1)\n",
    "H_num = H.subs([(x, 1), (y, 1)])\n",
    "print(\"\\nH(1, 1) =\")\n",
    "print(H_num)\n",
    "\n",
    "# Interpr√©tation : valeurs propres\n",
    "eigenvals = list(H_num.eigenvals().keys())\n",
    "print(\"\\nValeurs propres :\", eigenvals)\n",
    "if all(ev > 0 for ev in eigenvals):\n",
    "    print(\"‚Üí Minimum local (toutes positives)\")\n",
    "elif all(ev < 0 for ev in eigenvals):\n",
    "    print(\"‚Üí Maximum local (toutes n√©gatives)\")\n",
    "else:\n",
    "    print(\"‚Üí Point-selle (signes mixtes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9Ô∏è‚É£ Optimisation : Trouver Minima et Maxima\n",
    "\n",
    "L'objectif final : **minimiser** la fonction de co√ªt (ou **maximiser** la performance) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Conditions d'Optimalit√©\n",
    "\n",
    "**Pour une fonction $f(x)$ d'une variable :**\n",
    "\n",
    "1. **Condition n√©cessaire** (point critique) : $f'(x) = 0$\n",
    "2. **Test de la d√©riv√©e seconde** :\n",
    "   - Si $f''(x) > 0$ ‚Üí **minimum local**\n",
    "   - Si $f''(x) < 0$ ‚Üí **maximum local**\n",
    "   - Si $f''(x) = 0$ ‚Üí test inconclusif\n",
    "\n",
    "**Pour une fonction $f(\\mathbf{x})$ de plusieurs variables :**\n",
    "\n",
    "1. **Condition n√©cessaire** : $\\nabla f = \\mathbf{0}$ (gradient nul)\n",
    "2. **Test de la Hessienne** :\n",
    "   - Hessienne d√©finie positive ‚Üí **minimum local**\n",
    "   - Hessienne d√©finie n√©gative ‚Üí **maximum local**\n",
    "   - Hessienne ind√©finie ‚Üí **point-selle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Trouver le minimum de f(x) = x¬≤ - 4x + 5\n",
    "x = sp.Symbol('x')\n",
    "f = x**2 - 4*x + 5\n",
    "\n",
    "# D√©riv√©e premi√®re\n",
    "f_prime = sp.diff(f, x)\n",
    "print(\"Fonction : f(x) =\", f)\n",
    "print(\"D√©riv√©e : f'(x) =\", f_prime)\n",
    "\n",
    "# Trouver les points critiques : f'(x) = 0\n",
    "points_critiques = sp.solve(f_prime, x)\n",
    "print(\"\\nPoints critiques (f'(x) = 0) :\", points_critiques)\n",
    "\n",
    "# D√©riv√©e seconde\n",
    "f_double_prime = sp.diff(f_prime, x)\n",
    "print(\"D√©riv√©e seconde : f''(x) =\", f_double_prime)\n",
    "\n",
    "# Test pour chaque point critique\n",
    "for xc in points_critiques:\n",
    "    f2_val = f_double_prime.subs(x, xc)\n",
    "    f_val = f.subs(x, xc)\n",
    "    print(f\"\\nEn x = {xc}:\")\n",
    "    print(f\"  f({xc}) = {f_val}\")\n",
    "    print(f\"  f''({xc}) = {f2_val}\")\n",
    "    if f2_val > 0:\n",
    "        print(f\"  ‚Üí MINIMUM LOCAL\")\n",
    "    elif f2_val < 0:\n",
    "        print(f\"  ‚Üí MAXIMUM LOCAL\")\n",
    "    else:\n",
    "        print(f\"  ‚Üí Test inconclusif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation : Points critiques\n",
    "x_plot = np.linspace(-1, 5, 300)\n",
    "f_plot = x_plot**2 - 4*x_plot + 5\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_plot, f_plot, 'b-', linewidth=2, label='f(x) = x¬≤ - 4x + 5')\n",
    "plt.plot(2, 1, 'r*', markersize=20, label='Minimum en x=2, f(2)=1')\n",
    "plt.axvline(x=2, color='r', linestyle='--', alpha=0.5)\n",
    "plt.axhline(y=1, color='r', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Trouver le Minimum avec f\\'(x) = 0', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèîÔ∏è Paysage d'Optimisation : Minimum Global vs Local\n",
    "\n",
    "**Attention :** En ML, la fonction de co√ªt peut avoir :\n",
    "- **Plusieurs minima locaux** (pi√®ges !)\n",
    "- **Un minimum global** (le meilleur)\n",
    "- **Des points-selles** (ni minimum ni maximum)\n",
    "\n",
    "**Strat√©gies :**\n",
    "- **Random restarts** : Red√©marrer Gradient Descent de diff√©rents points\n",
    "- **Momentum** : Ajouter de l'inertie pour √©chapper aux minima locaux\n",
    "- **Learning rate adaptatif** : Ajuster $\\alpha$ dynamiquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : Fonction avec plusieurs minima locaux\n",
    "def f_complex(x):\n",
    "    return x**4 - 4*x**3 + 4*x**2 + 2\n",
    "\n",
    "def f_complex_prime(x):\n",
    "    return 4*x**3 - 12*x**2 + 8*x\n",
    "\n",
    "x_plot = np.linspace(-1, 4, 500)\n",
    "y_plot = f_complex(x_plot)\n",
    "\n",
    "# Trouver les points critiques num√©riquement\n",
    "from scipy.optimize import fsolve\n",
    "x0_guesses = [0, 1, 2]\n",
    "critical_points = []\n",
    "for x0 in x0_guesses:\n",
    "    sol = fsolve(f_complex_prime, x0)[0]\n",
    "    if -1 <= sol <= 4 and not any(abs(sol - cp) < 0.01 for cp in critical_points):\n",
    "        critical_points.append(sol)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = x‚Å¥ - 4x¬≥ + 4x¬≤ + 2')\n",
    "\n",
    "for xc in critical_points:\n",
    "    yc = f_complex(xc)\n",
    "    plt.plot(xc, yc, 'ro', markersize=10)\n",
    "    plt.text(xc, yc + 0.5, f'x={xc:.2f}', ha='center', fontsize=10)\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Fonction avec Plusieurs Points Critiques', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Points critiques trouv√©s :\")\n",
    "for xc in critical_points:\n",
    "    print(f\"  x = {xc:.4f}, f(x) = {f_complex(xc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**‚úèÔ∏è Exercices correspondants :** [Exercices 7.1 √† 7.5 - Optimisation](../../envs/phase_1_math/exercices_05_calcul_differentiel.ipynb#Section-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ F√©licitations !\n",
    "\n",
    "Tu as termin√© le cours sur le **Calcul Diff√©rentiel** ! üöÄ\n",
    "\n",
    "### üìä Ce que tu as appris :\n",
    "\n",
    "‚úÖ **Limites et continuit√©** : Fondations des d√©riv√©es  \n",
    "‚úÖ **D√©riv√©es** : Taux de changement instantan√©, pente de la tangente  \n",
    "‚úÖ **R√®gles de d√©rivation** : Somme, produit, quotient, puissance  \n",
    "‚úÖ **D√©riv√©es de fonctions courantes** : Polyn√¥mes, exp, log, trig, activations  \n",
    "‚úÖ **D√©riv√©es partielles** : Fonctions √† plusieurs variables  \n",
    "‚úÖ **Gradient** : Direction de plus grande pente, Gradient Descent  \n",
    "‚úÖ **R√®gle de la cha√Æne** : Backpropagation dans les r√©seaux de neurones  \n",
    "‚úÖ **Jacobienne et Hessienne** : Matrices de d√©riv√©es  \n",
    "‚úÖ **Optimisation** : Trouver minima/maxima, conditions d'optimalit√©  \n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ Pourquoi c'est CRUCIAL pour le ML ?\n",
    "\n",
    "**Tu comprends maintenant :**\n",
    "- Comment fonctionne **Gradient Descent** (minimiser la loss)\n",
    "- Comment fonctionne **Backpropagation** (entra√Æner les r√©seaux de neurones)\n",
    "- Pourquoi les **fonctions d'activation** ont ces formes particuli√®res\n",
    "- Comment optimiser les **hyperparam√®tres** (learning rate, etc.)\n",
    "\n",
    "**Sans calcul diff√©rentiel = Pas de Deep Learning = Pas de LLMs !**\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Prochaines √âtapes :\n",
    "\n",
    "1. **Exercices** : [exercices_05_calcul_differentiel.ipynb](../../envs/phase_1_math/exercices_05_calcul_differentiel.ipynb)\n",
    "2. **Solutions** : [solutions_05_calcul_differentiel.ipynb](../../envs/phase_1_math/solutions_05_calcul_differentiel.ipynb)\n",
    "3. **Projet** : [projet_05_visualisation_gradients.ipynb](../../envs/phase_1_math/projet_05_visualisation_gradients.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Continue ton apprentissage avec les prochains cours :**\n",
    "- Alg√®bre Lin√©aire (vecteurs, matrices, transformations)\n",
    "- Probabilit√©s et Statistiques\n",
    "- Optimisation Avanc√©e\n",
    "\n",
    "**Excellent travail ! Tu es sur la bonne voie pour devenir un expert en ML ! üí™**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
