{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìê Chapitre 04 : Alg√®bre Lin√©aire Avanc√©e\n",
    "\n",
    "Bienvenue dans le chapitre sur l'alg√®bre lin√©aire avanc√©e ! üöÄ\n",
    "\n",
    "## üéØ Objectifs du Chapitre\n",
    "\n",
    "√Ä la fin de ce chapitre, tu seras capable de :\n",
    "\n",
    "- ‚úÖ Comprendre les **espaces vectoriels** et **sous-espaces**\n",
    "- ‚úÖ Analyser l'**ind√©pendance lin√©aire** et les **bases**\n",
    "- ‚úÖ Calculer le **rang** d'une matrice\n",
    "- ‚úÖ Ma√Ætriser les **valeurs propres** et **vecteurs propres** (eigenvalues/eigenvectors)\n",
    "- ‚úÖ Appliquer la **d√©composition SVD** (Singular Value Decomposition)\n",
    "- ‚úÖ Utiliser **PCA** (Principal Component Analysis) pour la r√©duction de dimension\n",
    "\n",
    "## üî• Pourquoi c'est CRUCIAL en ML ?\n",
    "\n",
    "L'alg√®bre lin√©aire avanc√©e est **le c≈ìur battant** du Machine Learning :\n",
    "\n",
    "- üß† **Deep Learning** : Les r√©seaux de neurones sont des transformations lin√©aires compos√©es\n",
    "- üìä **PCA** : R√©duction de dimension pour visualiser et acc√©l√©rer les mod√®les\n",
    "- üé® **SVD** : Compression d'images, syst√®mes de recommandation (Netflix !)\n",
    "- üîç **Eigenvalues** : Analyse de stabilit√©, PageRank de Google\n",
    "- üí∞ **Finance Quantitative** : Analyse de corr√©lations, gestion de portefeuille\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table des Mati√®res\n",
    "\n",
    "1. [Espaces Vectoriels et Sous-Espaces](#1-espaces-vectoriels)\n",
    "2. [Ind√©pendance Lin√©aire et Base](#2-independance-lineaire)\n",
    "3. [Rang d'une Matrice](#3-rang-matrice)\n",
    "4. [Valeurs Propres et Vecteurs Propres](#4-valeurs-propres)\n",
    "5. [D√©composition SVD](#5-svd)\n",
    "6. [PCA - Principal Component Analysis](#6-pca)\n",
    "7. [Applications en Finance Quantitative](#7-applications-finance)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports n√©cessaires\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import linalg\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration pour de beaux graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis !\")\n",
    "print(f\"üìå NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Espaces Vectoriels et Sous-Espaces <a id=\"1-espaces-vectoriels\"></a>\n",
    "\n",
    "### üìñ Th√©orie\n",
    "\n",
    "#### Espace Vectoriel\n",
    "\n",
    "Un **espace vectoriel** $V$ est un ensemble de vecteurs o√π on peut :\n",
    "- ‚ûï **Additionner** deux vecteurs : $\\vec{u} + \\vec{v} \\in V$\n",
    "- ‚úñÔ∏è **Multiplier** par un scalaire : $\\alpha \\vec{v} \\in V$\n",
    "\n",
    "**Exemples** :\n",
    "- $\\mathbb{R}^2$ : tous les vecteurs 2D\n",
    "- $\\mathbb{R}^3$ : tous les vecteurs 3D\n",
    "- $\\mathbb{R}^n$ : tous les vecteurs n-dimensionnels\n",
    "\n",
    "#### Sous-Espace Vectoriel\n",
    "\n",
    "Un **sous-espace** $W \\subseteq V$ est un sous-ensemble de $V$ qui est lui-m√™me un espace vectoriel.\n",
    "\n",
    "**Conditions** :\n",
    "1. Contient le vecteur nul : $\\vec{0} \\in W$\n",
    "2. Ferm√© par addition : $\\vec{u}, \\vec{v} \\in W \\Rightarrow \\vec{u} + \\vec{v} \\in W$\n",
    "3. Ferm√© par multiplication scalaire : $\\vec{v} \\in W, \\alpha \\in \\mathbb{R} \\Rightarrow \\alpha\\vec{v} \\in W$\n",
    "\n",
    "**Exemples en $\\mathbb{R}^3$** :\n",
    "- Une **ligne** passant par l'origine (sous-espace 1D)\n",
    "- Un **plan** passant par l'origine (sous-espace 2D)\n",
    "- L'espace entier $\\mathbb{R}^3$ (sous-espace 3D)\n",
    "\n",
    "### üî• CRUCIAL en ML\n",
    "\n",
    "- **Feature spaces** : Les donn√©es vivent dans des espaces vectoriels\n",
    "- **Subspaces** : PCA trouve les sous-espaces les plus informatifs\n",
    "- **Null space** : Solutions √† $A\\vec{x} = \\vec{0}$ forment un sous-espace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualisation : Sous-espaces dans R¬≥\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Sous-espace 1D : une ligne\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "t = np.linspace(-2, 2, 100)\n",
    "direction = np.array([1, 2, 1])\n",
    "line = np.outer(t, direction)\n",
    "\n",
    "ax1.plot(line[:, 0], line[:, 1], line[:, 2], 'b-', linewidth=2, label='Ligne (1D)')\n",
    "ax1.scatter([0], [0], [0], color='red', s=100, label='Origine')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "ax1.set_title('Sous-espace 1D\\nLigne passant par l\\'origine')\n",
    "ax1.legend()\n",
    "ax1.set_xlim([-2, 2])\n",
    "ax1.set_ylim([-2, 2])\n",
    "ax1.set_zlim([-2, 2])\n",
    "\n",
    "# Sous-espace 2D : un plan\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "x = np.linspace(-2, 2, 10)\n",
    "y = np.linspace(-2, 2, 10)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = 0.5 * X + 0.3 * Y  # Plan d√©fini par deux vecteurs\n",
    "\n",
    "ax2.plot_surface(X, Y, Z, alpha=0.5, cmap='viridis')\n",
    "ax2.scatter([0], [0], [0], color='red', s=100, label='Origine')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('Y')\n",
    "ax2.set_zlabel('Z')\n",
    "ax2.set_title('Sous-espace 2D\\nPlan passant par l\\'origine')\n",
    "ax2.legend()\n",
    "\n",
    "# Espace complet R¬≥\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "# Quelques vecteurs dans R¬≥\n",
    "vectors = np.random.randn(20, 3) * 2\n",
    "for v in vectors:\n",
    "    ax3.quiver(0, 0, 0, v[0], v[1], v[2], arrow_length_ratio=0.1, alpha=0.6)\n",
    "ax3.scatter([0], [0], [0], color='red', s=100, label='Origine')\n",
    "ax3.set_xlabel('X')\n",
    "ax3.set_ylabel('Y')\n",
    "ax3.set_zlabel('Z')\n",
    "ax3.set_title('Espace complet R¬≥\\nTous les vecteurs 3D')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Dimension du sous-espace = nombre de directions ind√©pendantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Ind√©pendance Lin√©aire et Base <a id=\"2-independance-lineaire\"></a>\n",
    "\n",
    "### üìñ Th√©orie\n",
    "\n",
    "#### Ind√©pendance Lin√©aire\n",
    "\n",
    "Des vecteurs $\\vec{v_1}, \\vec{v_2}, ..., \\vec{v_n}$ sont **lin√©airement ind√©pendants** si :\n",
    "\n",
    "$$\\alpha_1 \\vec{v_1} + \\alpha_2 \\vec{v_2} + ... + \\alpha_n \\vec{v_n} = \\vec{0} \\quad \\Rightarrow \\quad \\alpha_1 = \\alpha_2 = ... = \\alpha_n = 0$$\n",
    "\n",
    "En d'autres termes : **aucun vecteur ne peut s'√©crire comme combinaison des autres**.\n",
    "\n",
    "**Exemples** :\n",
    "- ‚úÖ $\\vec{v_1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\vec{v_2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ sont ind√©pendants\n",
    "- ‚ùå $\\vec{v_1} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $\\vec{v_2} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$ sont d√©pendants ($\\vec{v_2} = 2\\vec{v_1}$)\n",
    "\n",
    "#### Base d'un Espace Vectoriel\n",
    "\n",
    "Une **base** de $V$ est un ensemble de vecteurs qui sont :\n",
    "1. **Lin√©airement ind√©pendants**\n",
    "2. **Engendrent tout l'espace** : tout vecteur de $V$ peut s'√©crire comme combinaison lin√©aire des vecteurs de la base\n",
    "\n",
    "**Base standard de $\\mathbb{R}^3$** :\n",
    "$$\\vec{e_1} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\vec{e_2} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\vec{e_3} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "#### Dimension\n",
    "\n",
    "La **dimension** d'un espace vectoriel = **nombre de vecteurs dans une base**.\n",
    "\n",
    "- $\\dim(\\mathbb{R}^2) = 2$\n",
    "- $\\dim(\\mathbb{R}^3) = 3$\n",
    "- $\\dim(\\mathbb{R}^n) = n$\n",
    "\n",
    "### üî• CRUCIAL en ML\n",
    "\n",
    "- **Feature selection** : Enlever les features d√©pendantes (redondantes)\n",
    "- **PCA** : Trouve une nouvelle base plus informative\n",
    "- **Dimensionality** : Nombre de param√®tres ind√©pendants dans un mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Test d'ind√©pendance lin√©aire avec NumPy\n",
    "\n",
    "def test_linear_independence(vectors):\n",
    "    \"\"\"\n",
    "    Teste si des vecteurs sont lin√©airement ind√©pendants.\n",
    "    \n",
    "    M√©thode : on forme une matrice et on regarde son rang.\n",
    "    Si rang = nombre de vecteurs ‚Üí ind√©pendants\n",
    "    Sinon ‚Üí d√©pendants\n",
    "    \"\"\"\n",
    "    A = np.column_stack(vectors)\n",
    "    rank = np.linalg.matrix_rank(A)\n",
    "    n_vectors = len(vectors)\n",
    "    \n",
    "    print(f\"Matrice form√©e :\")\n",
    "    print(A)\n",
    "    print(f\"\\nRang : {rank}\")\n",
    "    print(f\"Nombre de vecteurs : {n_vectors}\")\n",
    "    \n",
    "    if rank == n_vectors:\n",
    "        print(\"‚úÖ Les vecteurs sont LIN√âAIREMENT IND√âPENDANTS\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Les vecteurs sont LIN√âAIREMENT D√âPENDANTS\")\n",
    "        print(f\"   ‚Üí Seulement {rank} vecteurs ind√©pendants parmi {n_vectors}\")\n",
    "        return False\n",
    "\n",
    "# Exemple 1 : Vecteurs ind√©pendants\n",
    "print(\"=\"*50)\n",
    "print(\"Exemple 1 : Base standard de R¬≤\")\n",
    "print(\"=\"*50)\n",
    "v1 = np.array([1, 0])\n",
    "v2 = np.array([0, 1])\n",
    "test_linear_independence([v1, v2])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Exemple 2 : Vecteurs d√©pendants\")\n",
    "print(\"=\"*50)\n",
    "v1 = np.array([1, 2])\n",
    "v2 = np.array([2, 4])  # v2 = 2*v1\n",
    "test_linear_independence([v1, v2])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Exemple 3 : Trois vecteurs dans R¬≥\")\n",
    "print(\"=\"*50)\n",
    "v1 = np.array([1, 0, 0])\n",
    "v2 = np.array([0, 1, 0])\n",
    "v3 = np.array([0, 0, 1])\n",
    "test_linear_independence([v1, v2, v3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualisation : Ind√©pendance vs D√©pendance\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Cas 1 : Vecteurs ind√©pendants\n",
    "ax = axes[0]\n",
    "v1 = np.array([2, 1])\n",
    "v2 = np.array([1, 3])\n",
    "\n",
    "ax.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='blue', width=0.01, label='v‚ÇÅ')\n",
    "ax.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='red', width=0.01, label='v‚ÇÇ')\n",
    "\n",
    "# Montrer qu'on peut atteindre n'importe quel point\n",
    "for a in np.linspace(-1, 1, 5):\n",
    "    for b in np.linspace(-1, 1, 5):\n",
    "        point = a * v1 + b * v2\n",
    "        ax.scatter(point[0], point[1], alpha=0.3, s=20, color='green')\n",
    "\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title('‚úÖ Vecteurs IND√âPENDANTS\\nPeuvent engendrer tout le plan', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "# Cas 2 : Vecteurs d√©pendants\n",
    "ax = axes[1]\n",
    "v1 = np.array([2, 1])\n",
    "v2 = np.array([4, 2])  # v2 = 2*v1\n",
    "\n",
    "ax.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='blue', width=0.01, label='v‚ÇÅ')\n",
    "ax.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, \n",
    "          color='red', width=0.01, label='v‚ÇÇ = 2v‚ÇÅ')\n",
    "\n",
    "# Ne peuvent engendrer qu'une ligne\n",
    "t = np.linspace(-2, 2, 100)\n",
    "line = np.outer(t, v1)\n",
    "ax.plot(line[:, 0], line[:, 1], 'g--', alpha=0.5, linewidth=2, label='Sous-espace engendr√©')\n",
    "\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title('‚ùå Vecteurs D√âPENDANTS\\nN\\'engendrent qu\\'une ligne', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Rang d'une Matrice <a id=\"3-rang-matrice\"></a>\n",
    "\n",
    "### üìñ Th√©orie\n",
    "\n",
    "Le **rang** d'une matrice $A$ est le **nombre maximal de colonnes (ou lignes) lin√©airement ind√©pendantes**.\n",
    "\n",
    "$$\\text{rang}(A) = \\dim(\\text{espace des colonnes}) = \\dim(\\text{espace des lignes})$$\n",
    "\n",
    "**Propri√©t√©s** :\n",
    "- $\\text{rang}(A) \\leq \\min(m, n)$ pour une matrice $m \\times n$\n",
    "- $\\text{rang}(A) = \\text{rang}(A^T)$\n",
    "- Si $\\text{rang}(A) = n$ (matrice $n \\times n$), alors $A$ est **inversible** (rang plein)\n",
    "- Si $\\text{rang}(A) < n$, alors $A$ est **singuli√®re** (non inversible)\n",
    "\n",
    "**Interpr√©tation g√©om√©trique** :\n",
    "- Rang 1 : Tous les vecteurs colonnes sont colin√©aires ‚Üí g√©n√®rent une ligne\n",
    "- Rang 2 : Les colonnes g√©n√®rent un plan\n",
    "- Rang 3 : Les colonnes g√©n√®rent l'espace 3D complet\n",
    "\n",
    "### üî• CRUCIAL en ML\n",
    "\n",
    "- **Multicollin√©arit√©** : D√©tecter les features redondantes\n",
    "- **SVD** : Approximation de rang faible pour compression\n",
    "- **Matrix completion** : Syst√®mes de recommandation (Netflix)\n",
    "- **Regularization** : √âviter les matrices singuli√®res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Calcul du rang avec NumPy\n",
    "\n",
    "def analyze_rank(A, name=\"Matrice\"):\n",
    "    \"\"\"Analyse compl√®te du rang d'une matrice.\"\"\"\n",
    "    m, n = A.shape\n",
    "    rank = np.linalg.matrix_rank(A)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(A)\n",
    "    print(f\"\\nDimension : {m} √ó {n}\")\n",
    "    print(f\"Rang : {rank}\")\n",
    "    print(f\"Rang maximum possible : {min(m, n)}\")\n",
    "    \n",
    "    if rank == min(m, n):\n",
    "        print(\"‚úÖ Rang PLEIN (full rank)\")\n",
    "        if m == n:\n",
    "            print(\"   ‚Üí Matrice INVERSIBLE\")\n",
    "    else:\n",
    "        print(f\"‚ùå Rang D√âFICIENT (rank deficient)\")\n",
    "        print(f\"   ‚Üí Manque {min(m, n) - rank} dimension(s)\")\n",
    "        if m == n:\n",
    "            print(\"   ‚Üí Matrice NON INVERSIBLE (singuli√®re)\")\n",
    "    \n",
    "    return rank\n",
    "\n",
    "# Exemple 1 : Matrice de rang plein\n",
    "A1 = np.array([[1, 2],\n",
    "               [3, 4]])\n",
    "analyze_rank(A1, \"Exemple 1 : Matrice 2√ó2 inversible\")\n",
    "\n",
    "# Exemple 2 : Matrice singuli√®re (rang d√©ficient)\n",
    "A2 = np.array([[1, 2],\n",
    "               [2, 4]])  # Ligne 2 = 2 √ó Ligne 1\n",
    "analyze_rank(A2, \"Exemple 2 : Matrice 2√ó2 singuli√®re\")\n",
    "\n",
    "# Exemple 3 : Matrice rectangulaire\n",
    "A3 = np.array([[1, 2, 3],\n",
    "               [4, 5, 6]])\n",
    "analyze_rank(A3, \"Exemple 3 : Matrice 2√ó3\")\n",
    "\n",
    "# Exemple 4 : Matrice de rang 1\n",
    "u = np.array([[1], [2], [3]])\n",
    "v = np.array([[1, 2, 4]])\n",
    "A4 = u @ v  # Produit ext√©rieur ‚Üí rang 1\n",
    "analyze_rank(A4, \"Exemple 4 : Matrice de rang 1 (produit ext√©rieur)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualisation : Impact du rang sur la transformation\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Grille de points dans R¬≤\n",
    "x = np.linspace(-2, 2, 20)\n",
    "y = np.linspace(-2, 2, 20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "points = np.vstack([X.ravel(), Y.ravel()])\n",
    "\n",
    "# Matrice rang 2 (inversible)\n",
    "A_rank2 = np.array([[2, 1],\n",
    "                     [1, 2]])\n",
    "transformed_rank2 = A_rank2 @ points\n",
    "\n",
    "axes[0].scatter(points[0], points[1], alpha=0.3, s=10, label='Original')\n",
    "axes[0].scatter(transformed_rank2[0], transformed_rank2[1], alpha=0.3, s=10, \n",
    "                color='red', label='Transform√©')\n",
    "axes[0].set_title(f'Rang 2 (plein)\\nL\\'espace est pr√©serv√©', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(-10, 10)\n",
    "axes[0].set_ylim(-10, 10)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Matrice rang 1 (singuli√®re)\n",
    "A_rank1 = np.array([[1, 2],\n",
    "                     [2, 4]])  # Rang 1\n",
    "transformed_rank1 = A_rank1 @ points\n",
    "\n",
    "axes[1].scatter(points[0], points[1], alpha=0.3, s=10, label='Original')\n",
    "axes[1].scatter(transformed_rank1[0], transformed_rank1[1], alpha=0.3, s=10, \n",
    "                color='red', label='Transform√©')\n",
    "axes[1].set_title(f'Rang 1\\nCollapse sur une ligne !', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(-10, 10)\n",
    "axes[1].set_ylim(-10, 10)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# Matrice rang 0 (matrice nulle)\n",
    "A_rank0 = np.zeros((2, 2))\n",
    "transformed_rank0 = A_rank0 @ points\n",
    "\n",
    "axes[2].scatter(points[0], points[1], alpha=0.3, s=10, label='Original')\n",
    "axes[2].scatter(transformed_rank0[0], transformed_rank0[1], alpha=0.3, s=100, \n",
    "                color='red', label='Transform√©', marker='x')\n",
    "axes[2].set_title(f'Rang 0\\nCollapse √† l\\'origine !', fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xlim(-3, 3)\n",
    "axes[2].set_ylim(-3, 3)\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Intuition :\")\n",
    "print(\"   Rang = Dimension de l'image de la transformation\")\n",
    "print(\"   Rang faible = Perte d'information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Valeurs Propres et Vecteurs Propres <a id=\"4-valeurs-propres\"></a>\n",
    "\n",
    "### üìñ Th√©orie\n",
    "\n",
    "#### D√©finition\n",
    "\n",
    "Pour une matrice carr√©e $A$, un **vecteur propre** $\\vec{v}$ et sa **valeur propre** $\\lambda$ v√©rifient :\n",
    "\n",
    "$$A\\vec{v} = \\lambda \\vec{v}$$\n",
    "\n",
    "**Interpr√©tation** : \n",
    "- Le vecteur $\\vec{v}$ ne change PAS de direction quand on applique $A$\n",
    "- Il est seulement **√©tir√©** (ou r√©tr√©ci) par un facteur $\\lambda$\n",
    "\n",
    "#### √âquation Caract√©ristique\n",
    "\n",
    "Pour trouver les valeurs propres, on r√©sout :\n",
    "\n",
    "$$\\det(A - \\lambda I) = 0$$\n",
    "\n",
    "C'est un **polyn√¥me** de degr√© $n$ qui a $n$ racines (√©ventuellement complexes ou r√©p√©t√©es).\n",
    "\n",
    "#### Propri√©t√©s Importantes\n",
    "\n",
    "1. **Trace** : $\\sum \\lambda_i = \\text{tr}(A)$\n",
    "2. **D√©terminant** : $\\prod \\lambda_i = \\det(A)$\n",
    "3. **Diagonalisation** : Si $A$ a $n$ vecteurs propres ind√©pendants :\n",
    "   $$A = PDP^{-1}$$\n",
    "   o√π $D$ est diagonale (valeurs propres) et $P$ contient les vecteurs propres\n",
    "\n",
    "### üî• CRUCIAL en ML\n",
    "\n",
    "- **PCA** : Les vecteurs propres de la matrice de covariance sont les directions principales\n",
    "- **PageRank** : Le vecteur propre dominant de la matrice de liens web\n",
    "- **Stabilit√©** : Valeurs propres < 1 ‚Üí syst√®me stable\n",
    "- **Optimisation** : Analyse de la Hessienne pour trouver minima/maxima\n",
    "- **Spectral Clustering** : Utilise les vecteurs propres du Laplacien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Calcul des valeurs et vecteurs propres\n",
    "\n",
    "def analyze_eigenvalues(A, name=\"Matrice\"):\n",
    "    \"\"\"Analyse compl√®te des valeurs/vecteurs propres.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(A)\n",
    "    \n",
    "    # Calcul\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "    \n",
    "    print(f\"\\nüìä Valeurs propres (Œª) :\")\n",
    "    for i, Œª in enumerate(eigenvalues, 1):\n",
    "        print(f\"   Œª{i} = {Œª:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüß≠ Vecteurs propres (normalis√©s) :\")\n",
    "    for i in range(len(eigenvalues)):\n",
    "        v = eigenvectors[:, i]\n",
    "        print(f\"   v{i+1} = {v}\")\n",
    "    \n",
    "    # V√©rification\n",
    "    print(f\"\\n‚úÖ V√©rification A*v = Œª*v :\")\n",
    "    for i in range(len(eigenvalues)):\n",
    "        v = eigenvectors[:, i]\n",
    "        Œª = eigenvalues[i]\n",
    "        Av = A @ v\n",
    "        Œªv = Œª * v\n",
    "        is_close = np.allclose(Av, Œªv)\n",
    "        symbol = \"‚úì\" if is_close else \"‚úó\"\n",
    "        print(f\"   {symbol} Vecteur propre {i+1} : {is_close}\")\n",
    "    \n",
    "    # Propri√©t√©s\n",
    "    print(f\"\\nüìå Propri√©t√©s :\")\n",
    "    print(f\"   Trace de A = {np.trace(A):.4f}\")\n",
    "    print(f\"   Somme des Œª = {np.sum(eigenvalues):.4f}\")\n",
    "    print(f\"   D√©terminant de A = {np.linalg.det(A):.4f}\")\n",
    "    print(f\"   Produit des Œª = {np.prod(eigenvalues):.4f}\")\n",
    "    \n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "# Exemple 1 : Matrice sym√©trique (valeurs propres r√©elles)\n",
    "A1 = np.array([[4, 2],\n",
    "               [2, 3]])\n",
    "Œª1, v1 = analyze_eigenvalues(A1, \"Matrice Sym√©trique\")\n",
    "\n",
    "# Exemple 2 : Matrice diagonale (simple !)\n",
    "A2 = np.array([[5, 0],\n",
    "               [0, 3]])\n",
    "Œª2, v2 = analyze_eigenvalues(A2, \"Matrice Diagonale\")\n",
    "\n",
    "# Exemple 3 : Matrice de rotation (valeurs propres complexes)\n",
    "Œ∏ = np.pi / 4  # 45 degr√©s\n",
    "A3 = np.array([[np.cos(Œ∏), -np.sin(Œ∏)],\n",
    "               [np.sin(Œ∏), np.cos(Œ∏)]])\n",
    "Œª3, v3 = analyze_eigenvalues(A3, \"Matrice de Rotation (45¬∞)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualisation : Vecteurs propres comme directions invariantes\n",
    "\n",
    "# Matrice d'exemple\n",
    "A = np.array([[3, 1],\n",
    "              [1, 3]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Avant transformation\n",
    "ax = axes[0]\n",
    "# Grille de vecteurs\n",
    "for i in range(-2, 3):\n",
    "    for j in range(-2, 3):\n",
    "        if i != 0 or j != 0:\n",
    "            v = np.array([i, j])\n",
    "            ax.arrow(0, 0, v[0], v[1], head_width=0.1, head_length=0.1, \n",
    "                    fc='lightblue', ec='blue', alpha=0.3)\n",
    "\n",
    "# Vecteurs propres en rouge\n",
    "for i in range(2):\n",
    "    v = eigenvectors[:, i] * 2\n",
    "    ax.arrow(0, 0, v[0], v[1], head_width=0.15, head_length=0.15, \n",
    "            fc='red', ec='darkred', linewidth=3, alpha=0.8,\n",
    "            label=f'v{i+1} (Œª={eigenvalues[i]:.2f})')\n",
    "\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title('Avant Transformation\\nVecteurs propres en rouge', fontweight='bold')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Apr√®s transformation\n",
    "ax = axes[1]\n",
    "for i in range(-2, 3):\n",
    "    for j in range(-2, 3):\n",
    "        if i != 0 or j != 0:\n",
    "            v = np.array([i, j])\n",
    "            transformed = A @ v\n",
    "            ax.arrow(0, 0, transformed[0], transformed[1], \n",
    "                    head_width=0.2, head_length=0.2, \n",
    "                    fc='lightgreen', ec='green', alpha=0.3)\n",
    "\n",
    "# Vecteurs propres transform√©s (toujours dans la m√™me direction !)\n",
    "for i in range(2):\n",
    "    v = eigenvectors[:, i] * 2\n",
    "    transformed = A @ v\n",
    "    ax.arrow(0, 0, transformed[0], transformed[1], \n",
    "            head_width=0.3, head_length=0.3, \n",
    "            fc='red', ec='darkred', linewidth=3, alpha=0.8,\n",
    "            label=f'A*v{i+1} (√©tir√© par {eigenvalues[i]:.2f})')\n",
    "\n",
    "ax.set_xlim(-8, 8)\n",
    "ax.set_ylim(-8, 8)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_title('Apr√®s Transformation A\\nVecteurs propres gardent leur direction !', \n",
    "             fontweight='bold')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Intuition :\")\n",
    "print(\"   Les vecteurs propres sont les AXES NATURELS de la transformation !\")\n",
    "print(\"   Dans leur direction, la matrice agit comme une simple multiplication.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Application : Puissance Matricielle avec Diagonalisation\n",
    "\n",
    "Si $A = PDP^{-1}$, alors :\n",
    "$$A^n = PD^nP^{-1}$$\n",
    "\n",
    "C'est **beaucoup plus rapide** car $D^n$ est trivial (√©lever chaque valeur propre √† la puissance $n$) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° Calcul rapide de A^100 par diagonalisation\n",
    "\n",
    "import time\n",
    "\n",
    "A = np.array([[3, 1],\n",
    "              [1, 3]])\n",
    "\n",
    "n = 100\n",
    "\n",
    "# M√©thode 1 : Multiplication na√Øve\n",
    "start = time.time()\n",
    "result_naive = np.linalg.matrix_power(A, n)\n",
    "time_naive = time.time() - start\n",
    "\n",
    "# M√©thode 2 : Diagonalisation\n",
    "start = time.time()\n",
    "Œª, P = np.linalg.eig(A)\n",
    "D = np.diag(Œª)\n",
    "D_n = np.diag(Œª**n)  # Tr√®s rapide !\n",
    "result_diag = P @ D_n @ np.linalg.inv(P)\n",
    "time_diag = time.time() - start\n",
    "\n",
    "print(f\"Calcul de A^{n} :\")\n",
    "print(f\"\\n‚è±Ô∏è  M√©thode na√Øve : {time_naive*1000:.4f} ms\")\n",
    "print(f\"‚ö° Diagonalisation : {time_diag*1000:.4f} ms\")\n",
    "print(f\"\\nüöÄ Acc√©l√©ration : {time_naive/time_diag:.2f}x\")\n",
    "print(f\"\\n‚úÖ R√©sultats identiques : {np.allclose(result_naive, result_diag)}\")\n",
    "\n",
    "print(f\"\\nR√©sultat :\")\n",
    "print(result_diag.real)  # .real pour enlever les parties imaginaires num√©riques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ D√©composition SVD (Singular Value Decomposition) <a id=\"5-svd\"></a>\n",
    "\n",
    "### üìñ Th√©orie\n",
    "\n",
    "La **SVD** est la d√©composition la plus importante en alg√®bre lin√©aire !\n",
    "\n",
    "#### Th√©or√®me SVD\n",
    "\n",
    "Toute matrice $A$ (m√™me non carr√©e !) peut s'√©crire :\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "O√π :\n",
    "- $U$ : matrice orthogonale $m \\times m$ (vecteurs singuliers gauches)\n",
    "- $\\Sigma$ : matrice diagonale $m \\times n$ (valeurs singuli√®res)\n",
    "- $V^T$ : matrice orthogonale $n \\times n$ (vecteurs singuliers droits)\n",
    "\n",
    "#### Valeurs Singuli√®res\n",
    "\n",
    "Les **valeurs singuli√®res** $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_r > 0$ sont :\n",
    "- Les racines carr√©es des valeurs propres de $A^T A$ (ou $AA^T$)\n",
    "- Toujours **positives** et **ordonn√©es**\n",
    "- Le nombre de valeurs non nulles = rang de $A$\n",
    "\n",
    "#### Approximation de Rang Faible\n",
    "\n",
    "On peut approximer $A$ en gardant seulement les $k$ plus grandes valeurs singuli√®res :\n",
    "\n",
    "$$A_k = \\sum_{i=1}^{k} \\sigma_i \\vec{u_i} \\vec{v_i}^T$$\n",
    "\n",
    "C'est la **meilleure approximation** de rang $k$ au sens de la norme de Frobenius !\n",
    "\n",
    "### üî• CRUCIAL en ML\n",
    "\n",
    "- **Compression d'images** : Stocker seulement les composantes principales\n",
    "- **Recommandation** : Matrix factorization (Netflix, Amazon)\n",
    "- **NLP** : Latent Semantic Analysis (LSA)\n",
    "- **R√©duction de dimension** : Alternative √† PCA\n",
    "- **Denoising** : Enlever le bruit en coupant les petites valeurs singuli√®res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç D√©composition SVD d'une matrice\n",
    "\n",
    "def svd_analysis(A, name=\"Matrice\"):\n",
    "    \"\"\"Analyse compl√®te de la SVD.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"Matrice A :\")\n",
    "    print(A)\n",
    "    print(f\"Shape : {A.shape}\")\n",
    "    \n",
    "    # SVD\n",
    "    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    print(f\"\\nüìä Valeurs singuli√®res :\")\n",
    "    for i, œÉ in enumerate(s, 1):\n",
    "        print(f\"   œÉ{i} = {œÉ:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìå Propri√©t√©s :\")\n",
    "    print(f\"   Rang de A = {np.sum(s > 1e-10)}\")\n",
    "    print(f\"   Nombre de valeurs singuli√®res > 0 : {np.sum(s > 1e-10)}\")\n",
    "    print(f\"   Plus grande valeur singuli√®re : {s[0]:.4f}\")\n",
    "    print(f\"   Plus petite valeur singuli√®re : {s[-1]:.4f}\")\n",
    "    print(f\"   Condition number : {s[0]/s[-1]:.4f}\")\n",
    "    \n",
    "    # Reconstruction\n",
    "    Sigma = np.zeros((U.shape[1], Vt.shape[0]))\n",
    "    Sigma[:len(s), :len(s)] = np.diag(s)\n",
    "    A_reconstructed = U @ Sigma @ Vt\n",
    "    \n",
    "    print(f\"\\n‚úÖ V√©rification de la reconstruction :\")\n",
    "    reconstruction_error = np.linalg.norm(A - A_reconstructed)\n",
    "    print(f\"   Erreur de reconstruction : {reconstruction_error:.2e}\")\n",
    "    print(f\"   Reconstruction parfaite : {np.allclose(A, A_reconstructed)}\")\n",
    "    \n",
    "    return U, s, Vt\n",
    "\n",
    "# Exemple 1 : Matrice simple\n",
    "A1 = np.array([[3, 1, 1],\n",
    "               [1, 3, 1]])\n",
    "U1, s1, Vt1 = svd_analysis(A1, \"Matrice 2√ó3\")\n",
    "\n",
    "# Exemple 2 : Matrice de rang faible\n",
    "A2 = np.array([[1, 2],\n",
    "               [2, 4],\n",
    "               [3, 6]])  # Toutes les lignes sont proportionnelles\n",
    "U2, s2, Vt2 = svd_analysis(A2, \"Matrice 3√ó2 de rang 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualisation : Approximation de rang faible\n",
    "\n",
    "# Cr√©er une matrice 10√ó10 avec structure\n",
    "np.random.seed(42)\n",
    "# Matrice de rang faible + bruit\n",
    "u = np.random.randn(10, 1)\n",
    "v = np.random.randn(1, 10)\n",
    "A = u @ v + 0.1 * np.random.randn(10, 10)\n",
    "\n",
    "# SVD\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "\n",
    "# Approximations avec diff√©rents rangs\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Matrice originale\n",
    "im = axes[0, 0].imshow(A, cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "axes[0, 0].set_title('Matrice Originale', fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[0, 0])\n",
    "\n",
    "# Approximations\n",
    "ranks = [1, 2, 3, 5, 10]\n",
    "for idx, k in enumerate(ranks):\n",
    "    ax_idx = (idx + 1) // 3, (idx + 1) % 3\n",
    "    \n",
    "    # Approximation de rang k\n",
    "    Sigma_k = np.zeros_like(A)\n",
    "    Sigma_k[:k, :k] = np.diag(s[:k])\n",
    "    A_k = U @ Sigma_k @ Vt\n",
    "    \n",
    "    # Erreur\n",
    "    error = np.linalg.norm(A - A_k, 'fro')\n",
    "    \n",
    "    im = axes[ax_idx].imshow(A_k, cmap='RdBu_r', vmin=-2, vmax=2)\n",
    "    axes[ax_idx].set_title(f'Rang {k}\\nErreur: {error:.3f}', fontweight='bold')\n",
    "    plt.colorbar(im, ax=axes[ax_idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Graphique des valeurs singuli√®res\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(s) + 1), s, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Index', fontsize=12)\n",
    "plt.ylabel('Valeur Singuli√®re', fontsize=12)\n",
    "plt.title('Spectre des Valeurs Singuli√®res', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.axhline(y=0.1, color='r', linestyle='--', label='Seuil de bruit')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation :\")\n",
    "print(\"   Les premi√®res valeurs singuli√®res capturent la structure principale.\")\n",
    "print(\"   Les petites valeurs = bruit qu'on peut ignorer !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ PCA - Principal Component Analysis <a id=\"6-pca\"></a>\n",
    "\n",
    "### üìñ Th√©orie\n",
    "\n",
    "**PCA** (Analyse en Composantes Principales) est une technique de **r√©duction de dimension**.\n",
    "\n",
    "#### Objectif\n",
    "\n",
    "Trouver les **directions de variance maximale** dans les donn√©es.\n",
    "\n",
    "#### Algorithme PCA\n",
    "\n",
    "1. **Centrer** les donn√©es : $X_{centered} = X - \\bar{X}$\n",
    "2. **Calculer** la matrice de covariance : $C = \\frac{1}{n-1} X_{centered}^T X_{centered}$\n",
    "3. **Trouver** les vecteurs propres et valeurs propres de $C$\n",
    "4. **Trier** par valeurs propres d√©croissantes\n",
    "5. **Projeter** sur les $k$ premiers vecteurs propres\n",
    "\n",
    "#### Variance Expliqu√©e\n",
    "\n",
    "La **variance expliqu√©e** par la $i$-√®me composante :\n",
    "\n",
    "$$\\text{Variance expliqu√©e}_i = \\frac{\\lambda_i}{\\sum_j \\lambda_j}$$\n",
    "\n",
    "#### Lien avec SVD\n",
    "\n",
    "PCA peut aussi se calculer via SVD de $X_{centered}$ :\n",
    "- Les **composantes principales** = colonnes de $V$\n",
    "- Les **valeurs propres** = $\\sigma_i^2 / (n-1)$\n",
    "\n",
    "### üî• CRUCIAL en ML\n",
    "\n",
    "- **Visualisation** : R√©duire √† 2D ou 3D pour explorer les donn√©es\n",
    "- **Feature reduction** : Acc√©l√©rer l'entra√Ænement en gardant l'essentiel\n",
    "- **Denoising** : Enlever les dimensions de bruit\n",
    "- **Compression** : Stocker moins de features\n",
    "- **D√©tection d'anomalies** : Points loin des composantes principales = outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Impl√©mentation de PCA from scratch\n",
    "\n",
    "class PCA_FromScratch:\n",
    "    \"\"\"PCA impl√©ment√© √† la main pour comprendre.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        self.eigenvalues = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Ajuster PCA sur les donn√©es X.\"\"\"\n",
    "        # √âtape 1 : Centrer les donn√©es\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "        \n",
    "        # √âtape 2 : Matrice de covariance\n",
    "        cov_matrix = np.cov(X_centered.T)\n",
    "        \n",
    "        # √âtape 3 : Valeurs propres et vecteurs propres\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "        \n",
    "        # √âtape 4 : Trier par valeurs propres d√©croissantes\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # √âtape 5 : Garder les k premi√®res composantes\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "        self.eigenvalues = eigenvalues[:self.n_components]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Projeter les donn√©es sur les composantes principales.\"\"\"\n",
    "        X_centered = X - self.mean\n",
    "        return X_centered @ self.components\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Ajuster et transformer en une seule √©tape.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def explained_variance_ratio(self):\n",
    "        \"\"\"Proportion de variance expliqu√©e par chaque composante.\"\"\"\n",
    "        return self.eigenvalues / np.sum(self.eigenvalues)\n",
    "\n",
    "# Test avec des donn√©es synth√©tiques\n",
    "np.random.seed(42)\n",
    "# Donn√©es corr√©l√©es en 3D\n",
    "n_samples = 200\n",
    "X = np.random.randn(n_samples, 3)\n",
    "X[:, 1] = X[:, 0] + 0.5 * np.random.randn(n_samples)  # Corr√©lation\n",
    "X[:, 2] = 0.1 * np.random.randn(n_samples)  # Peu de variance\n",
    "\n",
    "# Appliquer PCA\n",
    "pca = PCA_FromScratch(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"üìä R√©sultats PCA\")\n",
    "print(f\"\\nShape originale : {X.shape}\")\n",
    "print(f\"Shape apr√®s PCA : {X_pca.shape}\")\n",
    "print(f\"\\nComposantes principales (vecteurs propres) :\")\n",
    "print(pca.components.T)\n",
    "print(f\"\\nVariance expliqu√©e par chaque composante :\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio(), 1):\n",
    "    print(f\"   PC{i} : {ratio*100:.2f}%\")\n",
    "print(f\"\\nVariance totale expliqu√©e : {np.sum(pca.explained_variance_ratio())*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualisation : PCA en action\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# 1. Donn√©es 3D originales\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c='blue', alpha=0.6, s=20)\n",
    "ax1.set_xlabel('X‚ÇÅ')\n",
    "ax1.set_ylabel('X‚ÇÇ')\n",
    "ax1.set_zlabel('X‚ÇÉ')\n",
    "ax1.set_title('Donn√©es Originales (3D)', fontweight='bold', fontsize=12)\n",
    "\n",
    "# 2. Projection sur PC1-PC2\n",
    "ax2 = fig.add_subplot(132)\n",
    "scatter = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c='red', alpha=0.6, s=20)\n",
    "ax2.set_xlabel('PC1 (Composante Principale 1)')\n",
    "ax2.set_ylabel('PC2 (Composante Principale 2)')\n",
    "ax2.set_title('Apr√®s PCA (2D)\\nGarde l\\'essentiel de l\\'information', \n",
    "              fontweight='bold', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax2.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "# 3. Variance expliqu√©e\n",
    "ax3 = fig.add_subplot(133)\n",
    "# Calculer pour toutes les composantes\n",
    "pca_full = PCA_FromScratch(n_components=3)\n",
    "pca_full.fit(X)\n",
    "variance_ratios = pca_full.explained_variance_ratio()\n",
    "cumulative_variance = np.cumsum(variance_ratios)\n",
    "\n",
    "x_pos = np.arange(1, len(variance_ratios) + 1)\n",
    "ax3.bar(x_pos, variance_ratios * 100, alpha=0.6, label='Variance individuelle')\n",
    "ax3.plot(x_pos, cumulative_variance * 100, 'ro-', linewidth=2, \n",
    "         markersize=8, label='Variance cumul√©e')\n",
    "ax3.set_xlabel('Composante Principale')\n",
    "ax3.set_ylabel('Variance Expliqu√©e (%)')\n",
    "ax3.set_title('Scree Plot\\nCombien de composantes garder ?', \n",
    "              fontweight='bold', fontsize=12)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=95, color='green', linestyle='--', label='Seuil 95%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpr√©tation :\")\n",
    "print(\"   - Les 2 premi√®res composantes capturent ~{:.1f}% de la variance\".format(\n",
    "    cumulative_variance[1]*100))\n",
    "print(\"   - La 3√®me composante n'apporte que {:.1f}% d'information\".format(\n",
    "    variance_ratios[2]*100))\n",
    "print(\"   ‚Üí On peut donc r√©duire de 3D √† 2D sans perte majeure !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exemple R√©aliste : PCA sur Dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üå∏ PCA sur le dataset Iris (4D ‚Üí 2D)\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Charger les donn√©es\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"üìä Dataset Iris\")\n",
    "print(f\"Shape : {X_iris.shape} (150 √©chantillons, 4 features)\")\n",
    "print(f\"Features : {feature_names}\")\n",
    "print(f\"Classes : {target_names}\")\n",
    "\n",
    "# Standardiser (important pour PCA !)\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "# PCA\n",
    "pca_iris = PCA_FromScratch(n_components=2)\n",
    "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
    "\n",
    "print(f\"\\n‚úÖ PCA effectu√©e : 4D ‚Üí 2D\")\n",
    "print(f\"Variance expliqu√©e :\")\n",
    "for i, ratio in enumerate(pca_iris.explained_variance_ratio(), 1):\n",
    "    print(f\"   PC{i} : {ratio*100:.2f}%\")\n",
    "print(f\"Total : {np.sum(pca_iris.explained_variance_ratio())*100:.2f}%\")\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(X_iris_pca[y_iris == i, 0], \n",
    "                X_iris_pca[y_iris == i, 1],\n",
    "                c=colors[i], label=target_name, alpha=0.7, s=50)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_iris.explained_variance_ratio()[0]*100:.1f}% variance)', \n",
    "           fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_iris.explained_variance_ratio()[1]*100:.1f}% variance)', \n",
    "           fontsize=12)\n",
    "plt.title('Dataset Iris projet√© sur les 2 premi√®res composantes principales', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation :\")\n",
    "print(\"   Les 3 esp√®ces d'iris sont bien s√©par√©es en 2D !\")\n",
    "print(\"   PCA a r√©duit la dimension tout en pr√©servant la structure des donn√©es.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Applications en Finance Quantitative <a id=\"7-applications-finance\"></a>\n",
    "\n",
    "### üî• Pourquoi c'est crucial pour la Finance ?\n",
    "\n",
    "L'alg√®bre lin√©aire avanc√©e est au c≈ìur de la finance quantitative moderne.\n",
    "\n",
    "#### 1. Analyse de Portefeuille avec PCA\n",
    "\n",
    "**Objectif** : Identifier les **facteurs de risque** principaux dans un portefeuille.\n",
    "\n",
    "- Les composantes principales = facteurs de risque ind√©pendants\n",
    "- Permet de comprendre les corr√©lations cach√©es entre actifs\n",
    "- R√©duction de dimension pour grandes matrices de corr√©lation\n",
    "\n",
    "#### 2. Optimisation de Portefeuille\n",
    "\n",
    "Le probl√®me de **Markowitz** :\n",
    "$$\\min_{w} w^T \\Sigma w \\quad \\text{sujet √†} \\quad w^T \\mu \\geq r_{\\text{target}}$$\n",
    "\n",
    "N√©cessite :\n",
    "- Inversion de matrice de covariance $\\Sigma$\n",
    "- Valeurs propres pour v√©rifier la d√©finition positive\n",
    "- SVD pour matrices mal conditionn√©es\n",
    "\n",
    "#### 3. Pricing d'Options\n",
    "\n",
    "- R√©solution de syst√®mes lin√©aires pour grilles de diff√©rences finies\n",
    "- D√©composition de Cholesky pour simulation Monte Carlo\n",
    "- Vecteurs propres pour mod√®les multi-factoriels\n",
    "\n",
    "#### 4. Gestion de Risque\n",
    "\n",
    "- **VaR** (Value at Risk) : Utilise la matrice de covariance\n",
    "- **Stress testing** : Analyse en composantes principales des sc√©narios\n",
    "- **Hedging** : R√©solution de syst√®mes pour delta-hedging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí∞ Exemple : Analyse de Portefeuille avec PCA\n",
    "\n",
    "# Simuler des rendements d'actifs corr√©l√©s\n",
    "np.random.seed(42)\n",
    "n_days = 252  # 1 an de trading\n",
    "n_assets = 10\n",
    "\n",
    "# Cr√©er des rendements avec structure de corr√©lation\n",
    "# 3 facteurs cach√©s\n",
    "factors = np.random.randn(n_days, 3)\n",
    "loadings = np.random.randn(n_assets, 3)\n",
    "noise = 0.3 * np.random.randn(n_days, n_assets)\n",
    "returns = (factors @ loadings.T + noise) * 0.01  # Rendements en %\n",
    "\n",
    "# Ajouter des noms d'actifs\n",
    "asset_names = [f'Action_{i+1}' for i in range(n_assets)]\n",
    "\n",
    "print(\"üìà Simulation de Portefeuille\")\n",
    "print(f\"Nombre d'actifs : {n_assets}\")\n",
    "print(f\"Nombre de jours : {n_days}\")\n",
    "print(f\"\\nRendements moyens (% par jour) :\")\n",
    "for i, name in enumerate(asset_names):\n",
    "    print(f\"   {name} : {np.mean(returns[:, i])*100:.3f}%\")\n",
    "\n",
    "# Matrice de corr√©lation\n",
    "correlation_matrix = np.corrcoef(returns.T)\n",
    "\n",
    "# PCA sur les rendements\n",
    "pca_portfolio = PCA_FromScratch(n_components=5)\n",
    "pca_portfolio.fit(returns)\n",
    "\n",
    "print(f\"\\nüîç Analyse PCA du Portefeuille\")\n",
    "print(f\"\\nVariance expliqu√©e par chaque facteur :\")\n",
    "variance_ratios = pca_portfolio.explained_variance_ratio()\n",
    "cumulative = 0\n",
    "for i, ratio in enumerate(variance_ratios, 1):\n",
    "    cumulative += ratio\n",
    "    print(f\"   Facteur {i} : {ratio*100:.2f}% (cumul√©: {cumulative*100:.2f}%)\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Matrice de corr√©lation\n",
    "im = axes[0].imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[0].set_xticks(range(n_assets))\n",
    "axes[0].set_yticks(range(n_assets))\n",
    "axes[0].set_xticklabels(asset_names, rotation=45, ha='right')\n",
    "axes[0].set_yticklabels(asset_names)\n",
    "axes[0].set_title('Matrice de Corr√©lation des Actifs', fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Variance expliqu√©e\n",
    "x_pos = np.arange(1, len(variance_ratios) + 1)\n",
    "axes[1].bar(x_pos, variance_ratios * 100, alpha=0.7, color='steelblue')\n",
    "axes[1].plot(x_pos, np.cumsum(variance_ratios) * 100, 'ro-', \n",
    "             linewidth=2, markersize=8, label='Cumul√©e')\n",
    "axes[1].set_xlabel('Facteur de Risque')\n",
    "axes[1].set_ylabel('Variance Expliqu√©e (%)')\n",
    "axes[1].set_title('Facteurs de Risque Principaux\\n(Scree Plot)', fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=80, color='green', linestyle='--', alpha=0.5, \n",
    "                label='Seuil 80%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insights Financiers :\")\n",
    "n_factors_80 = np.argmax(np.cumsum(variance_ratios) >= 0.8) + 1\n",
    "print(f\"   - {n_factors_80} facteurs expliquent 80% du risque du portefeuille\")\n",
    "print(f\"   - Au lieu de g√©rer {n_assets} actifs, on peut se concentrer sur {n_factors_80} facteurs\")\n",
    "print(f\"   - Simplifie √©norm√©ment la gestion de risque !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù R√©sum√© du Chapitre\n",
    "\n",
    "### üéØ Concepts Cl√©s Ma√Ætris√©s\n",
    "\n",
    "1. **Espaces Vectoriels** :\n",
    "   - Sous-espaces et dimensions\n",
    "   - Span et combinaisons lin√©aires\n",
    "\n",
    "2. **Ind√©pendance Lin√©aire** :\n",
    "   - Test avec le rang\n",
    "   - Bases et dimensions\n",
    "\n",
    "3. **Rang** :\n",
    "   - Interpr√©tation g√©om√©trique\n",
    "   - Rang plein vs d√©ficient\n",
    "   - Impact sur les transformations\n",
    "\n",
    "4. **Valeurs/Vecteurs Propres** :\n",
    "   - Directions invariantes\n",
    "   - Diagonalisation\n",
    "   - Applications en stabilit√© et puissance\n",
    "\n",
    "5. **SVD** :\n",
    "   - D√©composition universelle\n",
    "   - Approximation de rang faible\n",
    "   - Compression et denoising\n",
    "\n",
    "6. **PCA** :\n",
    "   - R√©duction de dimension\n",
    "   - Variance expliqu√©e\n",
    "   - Visualisation et compression\n",
    "\n",
    "### üöÄ Applications en ML et Finance\n",
    "\n",
    "- ‚úÖ **Deep Learning** : Comprendre les transformations dans les r√©seaux\n",
    "- ‚úÖ **Dimensionality Reduction** : PCA pour features engineering\n",
    "- ‚úÖ **Recommandation** : SVD pour matrix factorization\n",
    "- ‚úÖ **Portfolio Optimization** : Analyse de risque avec PCA\n",
    "- ‚úÖ **Compression** : Images, audio, donn√©es financi√®res\n",
    "\n",
    "### üìö Pour Aller Plus Loin\n",
    "\n",
    "**Exercices** : `exercices_04_algebre_avancee.ipynb` (30+ exercices)\n",
    "\n",
    "**Solutions** : `solutions_04_algebre_avancee.ipynb`\n",
    "\n",
    "**Projet** : `projet_04_compression_svd.ipynb` - Compression d'images avec SVD\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ F√©licitations !\n",
    "\n",
    "Tu ma√Ætrises maintenant l'alg√®bre lin√©aire avanc√©e, un outil **indispensable** pour :\n",
    "- üß† Comprendre le Deep Learning en profondeur\n",
    "- üí∞ Faire de la finance quantitative de haut niveau\n",
    "- üìä Analyser et r√©duire la dimension des donn√©es\n",
    "- üöÄ Optimiser les algorithmes de ML\n",
    "\n",
    "**Continue avec le Chapitre 05 : Calcul Diff√©rentiel** pour compl√©ter ta bo√Æte √† outils math√©matique ! üí™üî•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
